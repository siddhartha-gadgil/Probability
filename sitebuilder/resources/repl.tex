\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[preprint,  11pt]{amsart}

\usepackage{mathrsfs}
\usepackage{fullpage}
\usepackage{natbib}

\linespread{1.28}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsbsy}
%\usepackage{natbib}
\usepackage{amssymb} % Without this \qed does not get filled square
\usepackage{verbatim}
\usepackage{bm} %for some types of bold letters
\usepackage{paralist} %Need to use \inparaenum
%\RequirePackage[dvips]{hyperref}
%\usepackage{lineno}
 \usepackage{color}
 \usepackage{mathrsfs}
 \usepackage{graphicx}
\usepackage{hyperref}
%\hypersetup{
%  colorlinks   = true, %Colours links instead of ugly boxes
%  urlcolor     = blue, %Colour for external hyperlinks
%  linkcolor    = blue, %Colour of internal links
%  citecolor   = red %Colour of citations
%}



% PROBABILITY RELATED
\newcommand{\E}{\mathbf{E}}
\def\P{\mathbf{P}}
\def\Filt{\mathcal F_{\bullet}} %filtration
\def\Var{\mbox{Var}}
\def\sd{\mbox{s.d.}}
\def\convd{\stackrel{d}{\rightarrow}}
\def\convp{\stackrel{P}{\rightarrow}}
\def\convas{\stackrel{a.s.}{\rightarrow}}
\def\convlp{\stackrel{L^{p}}{\rightarrow}}
\def\eqd{\stackrel{d}{=}}
\def\given{\left.\vphantom{\hbox{\Large (}}\right|}
%\def\Given{\left.\vphantom{\hbox{\large (}}\right|}
\def\Given{\ \pmb{\big|} \ }
\newcommand{\as}{$a.s.$}
\newcommand{\dist}{\mbox{\rm dist}}
\newcommand{\Poi}{\mbox{\rm Poi}}
\newcommand{\EE}[1]{\E\l[#1\r]}
\def\CN{CN}
\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
%\renewcommand{\Pr}[1]{\,\mathbb P\,\(\,#1\,\)\,}
\newcommand\gauss{\ensuremath {e^{-x^{2}/2}}}
\DeclareMathOperator\erf{erf}
\newcommand\CRplus{\ensuremath{C[0,\infty)}}
\newcommand\CI{\ensuremath{C[0,1]}}


% COMMON MATHEMATICAL OBJECTS
\newcommand{\mat}[4]{\l[\begin{array}{cc}  #1 & #2 \\ #3 & #4  \end{array} \r]}
\newcommand{\vect}[2]{\l[\begin{array}{cc}  #1 \\ #2  \end{array} \r]}
\def\half{\frac{1}{2}}
\newcommand{\slfrac}[2]{\left.#1\middle/#2\right.}
\def\ccinf{C_c^{\infty}}
\newcommand{\one}{{\mathbf 1}}
\def\I#1{\mathbf{1}_{#1}}
\def\ind{{\mathbf 1}}
\newcommand\der[2]{\frac{d^{#1}}{d  #2^{#1}}}
%\renewcommand\matx[4]{\l[\begin{array}{cc}  #1 & #2 \\ #3 & #4\end{array} \r]}

% COMMON OPERATIONS
\def\summ{\sum\limits}
\def\intt{\int\limits}
\newcommand{\intg}[2]{\intt_{#1}^{#2}}
\def\doublint{\int\!\!\int}
\def\doublintfull{\intt_{-\infty}^{\infty}\!\!\intt_{-\infty}^{\infty}}
\def\prodd{\prod\limits}
\def\limm{\lim\limits}
\def\to{\rightarrow}
\def\tends{\rightarrow}
\def\fr{\frac}
\newcommand{\ontop}{\genfrac{}{}{0cm}{2}}
\def\d{\partial}
\def\dz{\frac{\partial }{ \partial z}}
\def\dzbar{\frac{\partial }{ \partial {\bar z}}}
\def\dw{\frac{\partial }{ \partial w}}
\def\dwbar{\frac{\partial }{ \partial {\bar zw}}}
\def\grad{\nabla}
\def\lap{\Delta}
\def\defn{\stackrel{\tiny def}{=}}
\def\di{\mbox{d}}
\newcommand{\diff}[1]{\frac{\partial}{\partial #1}}

% IMPORTANT MATH FUNCTIONS
\def\sgn{{\mb{sgn}}}
\newcommand\Tr{{\mbox{Tr}}}
\newcommand\tr{{\mbox{tr}}}
\def\per{\mb{per}}
\def\haf{\mb{haf}}
%\def\bar{\overline}
\newcommand{\Wi}[1]{:#1:}


% SPACINGS
\def\hsp{\hspace}
\def\mb{\mbox}
\newcommand{\head}[1]{{\noindent{\bfseries #1:}}}
\newcommand{\para}[1]{\vspace{4mm}\noindent{\bfseries #1:}}
\newcommand{\parau}[1]{\vspace{4mm}\noindent{\bfseries \underline{#1}:}}
\newcommand{\parag}[1]{\vspace{4mm}\noindent{\bfseries #1}}
\newcommand{\margin}[1]{\marginpar{\small \color{blue}{#1}}}

% BRACKETS AND BRACES
\def\Mid{\left\vert \right.}
\def\l{\left}
\def\r{\right}
\def\<{\langle}
\def\>{\rangle}

%\def\given{\left.\vphantom{\hbox{\Large (}}\right|}


% SHORTCUTS FOR ENVIRONMENTS
%\newcommand{\align}[1]{\begin{align*}#1\end{align*}}
\newcommand{\ba}{\[\begin{aligned}}
\newcommand{\ea}{\end{aligned}\]}
\newcommand\mnote[1]{} %off
\newcommand{\beq}[1]{\begin{equation}\label{#1}}
\newcommand\eeq{\end{equation}}
%\newcommand\be{\begin{align*}}
%\newcommand\ee{\end{align*}}
\newcommand\ben{\begin{equation}}
\newcommand\een{\end{equation}}
\newcommand\bes{\begin{eqnarray*}}
\newcommand\ees{\end{eqnarray*}}
\newcommand\besn{\begin{eqnarray}}
\newcommand\eesn{\end{eqnarray}}


\def\bthm{\begin{theorem}}
\def\ethm{\end{theorem}}
\def\bdefn{\begin{definition}}
\def\edefn{\end{definition}}
\newcommand{\benu}{\begin{enumerate}\setlength\itemsep{6pt}}
\newcommand{\beit}{\begin{itemize}\setlength\itemsep{3pt}}
\def\eenu{\end{enumerate}}
\def\eeit{\end{itemize}}
\def\beds{\begin{description}}
\def\eeds{\end{description}}
\def\bepr{\begin{problem}}
\def\eepr{\end{problem}}


\def\bprf{\begin{proof}}
\def\eprf{\end{proof}}
\def\berk{\begin{remark}}
\def\eerk{\end{remark}}
\def\bex{\begin{exercise}}
\def\eex{\end{exercise}}
\def\beg{\begin{example}}
\def\eeg{\end{example}}

% COMMON PHRASES
\newcommand\bllt{$\blacktriangleright$ \;\;}
\def\suchthat{{\; : \;}}
\renewcommand{\qed}{\hfill\text{$\blacksquare$}}
\newcommand*{\Cdot}{{\scalebox{2}{$\cdot$}}}

% CALLIGRAPHIC FONTS FOR GENERAL PURPOSE
\def\AA{{\mathcal A}}
\def\BB{{\mathcal B}}
\def \CC{{\mathcal C}}
\def\DD{{\mathcal D}}
\def\FF{{\mathcal F}}
\def\GG{{\mathcal G}}
\def\HH{{\mathcal H}}
\def\II{{\mathcal I}}
\def\JJ{{\mathcal J}}
\def \KK{{\mathcal K}}
\def\LL{{\mathbb L}}
\def\MM{{\mathcal M}}
\def\PP{{\mathcal P}}
\def\QQ{{\mathcal Q}}
\def\TT{{\mathcal T}}
\def\UU{{\mathcal U}}
\def\XX{{\mathcal X}}
\def\YY{{\mathcal Y}}
\def\ZZ{{\mathcal Z}}

% SETS OF REALS, INTEGERS, ETC.
\def\C{\mathbb{C}}
\def\D{\mathbb{D}} % unit disk
%\def\H{\mathbb H} % hyperbolic plane
%\renewcommand\H{{\varmathbb{H}}}
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\Q{\mathbb{Q}}
\def\S{\mathbb{S}} % for sphere
\def\Z{\mathbb{Z}}
\def\Sym{{\mathcal S}} % symmetric group
\def\Uni{{\mathcal U}} % Unitary group


% SPECIALIZED SHORTCUTS
\newcommand{\intc}{\int_0^{2\pi}}
\newcommand{\supp}{\mbox{\textrm supp}}
\newcommand{\Arg}{\mbox{\textrm Arg}}
\newcommand{\Vol}{\mbox{\textrm Vol}}
\newcommand{\Cov}{\mbox{\textrm Cov}}
\newcommand{\sm}{{\raise0.3ex\hbox{$\scriptstyle \setminus$}}}
\newcommand{\agau}{a complex Gaussian analytic function}
\newcommand{\gi}{\,|\,}
\newcommand{\area}{\operatorname{area}}
\newcommand{\wA}{\widetilde{A}}
\newcommand{\zb}{\overline{z}}
\newcommand{\const}{\operatorname{const}}
\newcommand{\ip}[2]{\langle#1,#2\rangle}


% SPECIALIZED FONT USAGE
%\newcommand{\phi}{\varphi}


%  GREEK LETTERS
\def\alp{\alpha}
\def\bet{\beta}
\def\gam{\gamma}
\def\del{\delta}
\def\eps{\epsilon}
\renewcommand\phi{\varphi}
\def\kap{\kappa}
\def\lam{\lambda}
\def\ome{\omega}
\def\sig{{\sigma}}
\def\Sig{{\Sigma}}
\def\ups{\upsilon}
\def\zet{\zeta}
\def\Gam{\Gamma}
\def\Del{\Delta}
\def\Lam{\Lambda}
\def\Ome{\Omega}
%\newcommand\u{\mathbf{u}}
\renewcommand\u{\mathbf{u}}
\newcommand\x{\mathbf{x}}
\newcommand\y{\mathbf{y}}
\renewcommand\v{\mathbf{v}}
\newcommand\w{\mathbf{w}}




% THEOREM ENVIRONMENTS - DON'T REALLY UNDERSTAND ALL DETAILS BELOW
\theoremstyle{plain} %documentation says there are only three styles
                  %{plain},{definition},{remark}
%\theoremstyle{theorem}
    \newtheorem{theorem}{Theorem}
    \newtheorem{lemma}[theorem]{Lemma}
    \newtheorem{proposition}[theorem]{Proposition}
    \newtheorem{corollary}[theorem]{Corollary}
    \newtheorem{claim}[theorem]{Claim}
    \newtheorem{conjecture}[theorem]{Conjecture}


%    \newtheorem{proof}[proof]{Proof}

\theoremstyle{definition} % For roman text in the body
    \newtheorem{definition}[theorem]{Definition}
    \newtheorem{fact}[theorem]{Fact}
    \newtheorem{result}[theorem]{Result}
    \newtheorem{question}[theorem]{Question}
    \newtheorem{algorithm}[theorem]{Algorithm}
    \newtheorem{assumption}[theorem]{Assumption}
    \newtheorem{exercise}[theorem]{Exercise}
    \newtheorem{problem}[theorem]{Problem}
        \newtheorem{remark}[theorem]{Remark}
    \newtheorem{example}[theorem]{Example}

\newtheorem{protoeg}[theorem]{Example}
\newtheorem{protoremark}[theorem]{Remark}
\newtheorem{protodefinition}[theorem]{Definition}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thetheorem}{\arabic{theorem}}


\def\newblock{\hskip .11em plus .33em minus .07em}


\def\t{{\bf t}}
\def\h{{\bf h}}
\def\omeg{\underline{\ome}}
\def\X{{\bf X}}
\def\Y{{\bf Y}}
%\newcommand{\para}[1]{\vspace{4mm}\noindent{\bf #1:}}
%\newcommand{\parag}[1]{\vspace{4mm}\noindent{\bf #1}}
\def\HH{\mathcal H}
\def\LL{\mathcal L}
\newcommand{\matrices}[4]{\l[\begin{array}{cc} #1 & #2 \\ #3 & #4  \end{array} \r]}
\def\doublintfull{\int\!\!\!\int}
\def\intg{\int\limits}
\def\half{\frac{1}{2}}
\def\sd{\mb{s.d.}}

\def\sig{{\sigma}}
\def\Sig{{\Sigma}}
\def\LL{{\mathbb L}}
\def\DD{{\mathcal D}}
\def\CC{{\mathcal C}}
\def\TT{{\mathcal T}}
\renewcommand\phi{{\varphi}}
\renewcommand{\benu}{\begin{enumerate}\setlength\itemsep{6pt}}
\renewcommand{\beit}{\begin{itemize}\setlength\itemsep{3pt}}
%\renewcommand\{Q}{\mathbf Q}

\renewcommand{\bex}{\indent\begin{exercise}}
\renewcommand\subset{\subseteq}
\def\mod{\left.\vphantom{\hbox{\Large (}}\right|}

\openup 0.4em

\usepackage{palatino}

\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=blue,  %choose some color if you want links to stand out
}
\setcounter{tocdepth}{1}

\begin{document}

\title{Probability and Statistics}
\author{Manjunath Krishnapur}

\maketitle

 \tableofcontents

\newpage

\maketitle

\setcounter{page}{4}



\newpage
\vspace*{\fill}
\begin{center}
\Huge {\bf Probability}
\end{center}
\vspace*{\fill}
\newpage

\section{What is statistics and what is probability?}
Sometimes statistics is described as {\em the art or science of decision making in the face of uncertainty}.  Here are some examples to illustrate what it means.
\begin{example} Recall the apocryphal story of two women who go to King Solomon with a child, each claiming that it is her own daughter. The solution according to the story uses human psychology and is not relevant to recall here. But is this a reasonable question that the king can decide? 

Daughters resemble mothers to varying degrees, and one cannot be absolutely sure of guessing correctly.  
 On the other hand, by comparing various features of the child with those of the two women, there is certainly a decent chance to guess correctly.
   
  If we could always get the right answer, or if we could never get it right, the question would not have been interesting. However, here we have uncertainty, but there is a decent chance of getting the right answer. That makes it interesting - for example, we can have a debate between {\em eyeists}  and {\em nosists} as to whether it is better to compare the eyes or the noses in arriving at a decision.
\end{example}

\begin{example} The IISc cricket team meets the Basavanagudi cricket club for a match. Unfortunately, the Basavanagudi team forgot to bring a coin to toss. The IISc captain helpfully offers his coin, but can he be trusted? What if he spent the previous night doctoring the coin so that it falls on one side with probability $3/4$ (or some other number)? 

Instead of cricket, they could spend their time on the more interesting question of checking if the coin is {\em fair} or {\em biased}. Here is one way. If the coin is fair, in a large number of tosses, common sense suggests that we should get about equal number of heads and tails. So they  toss the coin 100 times. If the number of heads is exactly 50, perhaps they will agree that it is fair. If the number of heads is 90, perhaps they will agree that it is biased. What if the number of heads is 60? Or 35? Where and on what basis to draw the line between fair and biased? Again we are faced with the question of making decision in the face of uncertainty.
\end{example}

\begin{example} A psychic claims to have divine visions unavailable to most of us. You are assigned the task of testing her claims. You take a standard deck of cards, shuffle it well and keep it face down on the table. The psychic writes down the list of cards in some order - whatever her vision tells her about how the deck is ordered. Then you count the number of correct guesses. If the number is 1 or 2, perhaps you can dismiss her claims. If it is 45, perhaps you ought to be take her seriously. Again, where to draw the line? 

The logic is this. Roughly one may say that {\em surprise} is just the name for our reaction to an event that we {\em \'{a} priori} thought to have low chance of occurring. Thus, we approach the experiment with the belief that the psychic is just guessing at random, and if the results are such that under that random-guess-hypothesis they have very small probability, then we are willing to be surprised, that is willing to discard our preconception and accept that she is a psychic. 

How low a probability is surprising? In the context of psychics, let us say, $1/10000$. Once we fix that, we must find a number $m\le 52$ such that by pure guessing, the probability to get more than $m$ correct guesses is less that $1/10000$. Then we tell the psychic that if she gets more than $m$ correct guesses, we accept her claim, and otherwise, reject her claim.  This raises the simple (and you can do it yourself) 
\begin{question} For a deck of $52$ cards, find the number $m$ such that
$$
\mathbf{P}(\mbox{by random guessing we get more than }m\mbox{ correct guesses})<\frac{1}{10000}.
$$
\end{question}
\end{example}


\noindent{\bf{Summary:}} There are many situations in real life where one is required to make decisions under uncertainty. A general template for the answer could be to fix a small number that we allow as the probability of error, and deduce thresholds based on it. This brings us to the question of computing probabilities in various situations.



\vspace{4mm}
\noindent{\bf Probability:} Probability theory is a branch of pure mathematics, and forms the theoretical basis of statistics. In itself, probability theory has some basic objects and their relations (like real numbers, addition etc for analysis) and it makes no pretense of saying anything about the real world. Axioms are given and theorems are then deduced about these objects, just as in any other part of mathematics. 

But a very important aspect of probability is that it is {\em applicable}. In other words, there are many real-world situations in which it is reasonable to take a model in probability and it turns out to reasonably replicate features of  the real-world situation.

In the example above, to compute the probability one must make the assumption that the deck of cards was completely shuffled. In other words, all possible 52! orders of the 52 cards are assumed to be equally likely. Whether this assumption is reasonable or not depends on how well the card was shuffled, whether the psychic was able to get a peek at the cards, whether some insider is informing the psychic of the cards etc. All these are non-mathematical questions, and must be decided on other basis.

\para{However...} Probability and statistics are very relevant in many situations that do not involve any uncertainty on the face of it. Here are some examples.
\begin{example} {\em Compression of data}. Large files in a computer can be compressed to a .zip format and uncompressed when necessary. How is it possible to compress data like this? To give a very simple analogy, consider a long English word like {\em invertebrate}. If we take a novel and replace every occurrence of this word with ``zqz'', then  it is certainly possible to recover the original novel (since ``zqz'' does not occur anywhere else). But the reduction in size by replacing the 12-letter word by the 3-letter word is not much, since the word {\em invertebrate} does not occur often. Instead, if we replace the 4-letter word ``then'' by ``zqz'', then the total reduction obtained may be much higher, as the word ``then'' occurs quite often. 

This suggests the following optimal way to represent words in English. The 26 most frequent words will be represented by single letters. The next $26\times 26$ most frequent words will be represented by two letter words, the next $26\times 26\times 26$ most frequent words by three-letter words, etc. Assuming there are no errors in transcription, this is a good way to reduce the size of any text document! Now, this involves knowing what the frequencies of occurrences of various words in actual texts are. Such statistics of usage of words are therefore clearly relevant (and they could be different for biology textbooks as compared to 19th century novels). 
\end{example}
\begin{example} Search algorithms such as Google, use many randomized procedures. This cannot be explained right now, but let us give a simple reason to say why introducing randomness is a good idea in many situations. In the game of {\em rock-paper-scissors}, two people simultaneously shout one of the three words, rock, paper or scissors. The rule is that scissors beats paper, paper beats rock and rock beats scissors (if they both call the same word, they must repeat). In a game like this, although there is complete symmetry in the three items, it would be silly to have a fixed strategy. In other words, if you decide to always say rock, thinking that it doesn't matter which you choose, then your opponent can use that knowledge to always choose paper and thus win! In many games where the opponent gets to know your strategy (but not your move), the best strategy would involve  randomly choosing your move.
\end{example}


%\newpage
\section{Discrete probability spaces}
\begin{definition} Let $\Omega$ be a finite or countable\footnote{For those unfamiliar with countable sets, it will be explained in some detail later.} set. Let $p:\Omega\rightarrow [0,1]$ be a function such that $\sum_{\omega\in \Omega}p_{\omega}=1$. Then $(\Omega,p)$ is called a {\em discrete probability space}. $\Omega$ is called the {\em sample space} and $p_{\omega}$ are called {\em elementary probabilities}. 
\begin{itemize}
\item Any subset $A\subseteq \Omega$ is called an {\em event}. For an event $A$ we define its {\em probability} as $\mathbf{P}(A)=\sum_{\omega\in A}p_{\omega}$.
\item Any function $X:\Omega\rightarrow \mathbb{R}$ is called a {\em random variable}.  For a random variable we define its {\em expected value} or {\em mean} as $\mathbf{E}[X]=\sum_{\omega \in \Omega}X(\omega)p_{\omega}$.
\end{itemize}
\end{definition}

{\em\underline{ All of probability in one line}}: Take an (interesting) probability space $(\Omega, p)$ and an (interesting) event $A\subseteq \Omega$. Find $\mathbf{P}(A)$. 



\vspace{2mm}
This is the mathematical side of the picture. It is easy to make up any number of probability spaces -  simply take a finite set and assign non-negative numbers to each element of the set so that the total is $1$.
\begin{example} $\Omega=\{0,1\}$ and $p_{0}=p_{1}=\frac{1}{2}$. There are only four events here, $\emptyset, \{0\}, \{1\}$ and $\{0,1\}$. Their probabilities are, $0$, $1/2$, $1/2$ and $1$, respectively.
\end{example}

\begin{example}\label{eg:onecointoss} $\Omega=\{0,1\}$. Fix a number $0\le p \le 1$ and let $p_{1}=p$ and $p_{0}=1-p$. The sample space is the same as before, but the probability space is different for each value of $p$. Again there are only four events, and their probabilities are $\mathbf{P}\{\emptyset\}=0$, $\mathbf{P}\{0\}=1-p$, $\mathbf{P}\{1\}=p$ and $\mathbf{P}\{0,1\}=1$. 
\end{example}

\begin{example}\label{eg:ncointosses} Fix a positive integer $n$. Let $$\Omega=\{0,1\}^{n}=\{\underline{\ome}{\; : \;} \underline{\ome}=(\omega_{1},\ldots ,\omega_{n})\mbox{ with }\omega_{i}=0\mbox{ or }1\mbox{ for each }i\le n\}.$$
Let $p_{\underline{\ome}}=2^{-n}$ for each $\underline{\ome}\in \Omega$. Since $\Omega$ has $2^{n}$ elements, it follows that this is a valid assignment of elementary probabilities. 

There are $2^{\#\Omega}=2^{2^{n}}$ events. One example is $A_{k}=\{\underline{\ome}{\; : \;} \underline{\ome}\in \Omega \mbox{ and } \omega_{1}+\ldots +\omega_{n}=k
\}$ where $k$ is some fixed integer. In words, $A_{k}$ consists of those $n$-tuples of zeros and ones that have a total of $k$ many ones. Since there are $\binom{n}{k}$ ways to choose where to place these ones, we see that $\#A_{k}=\binom{n}{k}$. Consequently,
$$
\mathbf{P}\{A_{k}\}=\sum_{\underline{\ome}\in A_{k}}p_{\underline{\ome}} = \frac{\# A_{k}}{2^{n}}=\begin{cases} \binom{n}{k}2^{-n} &\mbox{ if }0\le k\le n, \\ 0 & \mbox{ otherwise}. \end{cases}
$$
It will be convenient to adopt the notation that $\binom{a}{b}=0$ if $a,b$ are positive integers and if $b>a$ or if $b<0$. Then we can simply write $\mathbf{P}\{A_{k}\}=\binom{n}{k}2^{-n}$ without having to split the values of $k$ into cases.
\end{example}

\begin{example}\label{eg:rballsinmbins} Fix two positive integers $r$ and $m$. Let 
$$\Omega=\{\underline{\ome}{\; : \;} \underline{\ome}=(\omega_{1},\ldots ,\omega_{r}) \mbox{ with }1\le \omega_{i} \le m \mbox{ for each }i\le r\}.$$

The cardinality of $\Omega$ is $m^{r}$ (since each co-ordinate $\omega_{i}$ can take one of $m$ values). Hence, if we set $p_{\underline{\ome}}=m^{-r}$ for each $\underline{\ome}\in \Omega$, we get a valid probability space. 

Of course, there are $2^{m^{r}}$ many events, which is quite large even for small numbers like $m=3$ and $r=4$. Some interesting events are $A=\{\underline{\ome} {\; : \;} \omega_{r}=1\}$, $B=\{\underline{\ome}{\; : \;} \omega_{i}\not=1 \mbox{ for all }i\}$, $C=\{\underline{\ome}{\; : \;} \omega_{i}\not=\omega_{j} \mbox{ if } i\not= j\}$. The reason why these are interesting will be explained later. Because of equal elementary probabilities, the probability of an event $S$ is just $\#S/m^{r}$.
\begin{itemize}\setlength\itemsep{3pt}
\item Counting $A$: We have $m$ choices for each of  $\omega_{1},\ldots ,\omega_{r-1}$. There is only one choice for $\omega_{r}$. Hence $\#A=m^{r-1}$. Thus, $\mathbf{P}(A)=\frac{m^{r-1}}{m^{r}} = \frac{1}{m}$.
\item Counting $B$: We have $m-1$ choices for each $\omega_{i}$ (since $\omega_{i}$ cannot be $1$). Hence $\#B=(m-1)^{r}$ and thus $\mathbf{P}(B)=\frac{(m-1)^{r}}{m^{r}}=(1-\frac{1}{m})^{r}$.
\item Counting $C$: We must choose a distinct value for each $\omega_{1},\ldots ,\omega_{r}$. This is impossible if $m<r$. If $m\ge r$, then $\omega_{1}$ can be chosen as any of $m$ values. After $\omega_{1}$ is chosen, there are $(m-1)$ possible values for $\omega_{2}$, and then $(m-2)$ values for $\omega_{3}$ etc., all the way till $\omega_{r}$ which has $(m-r+1)$ choices. Thus, $\#C=m(m-1)\ldots (m-r+1)$. Note that we get the same answer if we choose $\omega_{i}$ in a different order (it would be strange if we did not!).

Thus, $\mathbf{P}(C)=\frac{m(m-1)\ldots (m-r+1)}{m^{r}}$. Note that this formula is also valid for $m<r$ since one of the factors on the right side is zero.
\end{itemize}

\end{example}


\subsection{Probability in the real world} In real life, there are often situations where there are several possible outcomes but which one will occur is unpredictable in some way. For example, when we toss a coin, we may get heads or tails. In such cases we use words such as {\em probability or chance}, {\em event or happening}, {\em randomness} etc.  What is the relationship between the intuitive and mathematical meanings of words such as probability or chance?

 In a given physical situation, we choose one out of all possible probability spaces that we think captures best the chance happenings in the situation. The chosen probability space is then called a {\em model} or a {\em probability model} for the given situation. Once the model has been chosen, calculation of probabilities of events therein is a mathematical problem. Whether the model really captures the given situation, or whether the model is inadequate and over-simplified is a non-mathematical question. Nevertheless that is an important question, and can be answered by observing the real life situation and comparing the outcomes with predictions made using the model\footnote{Roughly speaking we may divide the course into two parts according to these two issues. In the probability part of the course, we shall take many such models for granted and learn how to calculate or approximately calculate probabilities. In the statistics part of the course we shall see some methods by which we can arrive at such models, or test the validity of a proposed model.}.

 Now we describe several ``random experiments'' (a non-mathematical term to indicate a ``real-life'' phenomenon that is supposed to involve chance happenings) in which the previously given examples of probability spaces arise. Describing the probability space is the first step in any probability problem.
 
\begin{example} \para{Physical situation} Toss a coin. Randomness enters because we believe that the coin may turn up head or tail and that it is inherently unpredictable. 

\para{The corresponding probability model} Since there are two outcomes, the sample space $\Omega=\{0,1\}$ (where we use $1$ for heads and $2$ for tails) is a clear choice. What about elementary probabilities? Under the equal chance hypothesis, we may take $p_{0}=p_{1}=\frac{1}{2}$. Then we have a probability model for the coin toss.

 If the coin was not fair, we would change the model by keeping $\Omega=\{0,1\}$ as before but letting $p_{1}=p$ and $p_{0}=1-p$ where the parameter  $p\in [0,1]$ is fixed. 
 
Which model is correct? If the coin looks very symmetrical, then the two sides are equally likely to turn up, so the first model where $p_{1}=p_{0}=\frac{1}{2}$ is reasonable. However, if the coin looks irregular, then theoretical considerations are usually inadequate to arrive at the value of $p$. Experimenting with the coin (by tossing it a large number of times) is the only way. 
 
There is always an approximation in going from the real-world to a mathematical model. For example, the model above ignores the possibility that the coin can land on its side. If the coin is very thick, then it might be closer to a cylinder which can land in three ways and then we would have to modify the model... 
\end{example}

Thus we see that example~\ref{eg:onecointoss} is a good model for a physical coin toss. What physical situations are captured by the probability spaces in example~\ref{eg:ncointosses} and example~\ref{eg:rballsinmbins}?

\para{Example~\ref{eg:ncointosses}} This probability space can be a model for tossing $n$ fair coins. It is clear in what sense, so we omit details for you to fill in. 

The same probability space can also be a model for the tossing of the same coin $n$ times in succession. In this, we are implicitly assuming that the coin forgets the outcomes on the previous tosses. While that may seem obvious, it would be violated if our ``coin'' was a hollow lens filled with a semi-solid material like glue (then, depending on which way the coin fell on the first toss, the glue would settle more on the lower side and consequently the coin would be more likely to fall the same way again). This is a coin with memory!

\para{Example~\ref{eg:rballsinmbins}} There are several situations that can be captured by this probability space. We list some.
\begin{itemize}\setlength\itemsep{3pt}
\item There are $r$ labelled balls and $m$ labelled bins. One by one, we put the balls into bins ``at random''. Then, by letting $\omega_{i}$ be the bin-number into which the $i^{\mbox{\tiny th}}$ ball goes, we can capture the full configuration by the vector $\underline{\ome}=(\omega_{1},\ldots ,\omega_{n})$. If each ball is placed completely at random then the probabilities are $m^{-r}$ for each configuration $\underline{\ome}$. 

In that example, $A$ is the event that the last ball ends up in the first bin, $B$ is the event that the first bin is empty and $C$ is the event that no bin contains more than one ball. 
\item If $m=6$, then this may also be the model for throwing a fair die $r$ times. Then $\omega_{i}$ is the outcome on the $i^{\mbox{\tiny th}}$ throw. Of course, it also models throwing $r$ different (and distinguishable) fair dice.
\item If $m=2$ and $r=n$, this is same as Example~\ref{eg:ncointosses}, and thus models the tossing of $n$ fair coins (or a fair coin $n$ times).
\item Let $m=365$. Omitting the possibility of leap years, this is a model for choosing $r$ people at random and noting their birthdays (which can be in any of $365$ ``bins''). If we assume that all days are equally likely as a birthday (is this really true?), then the same probability space is a model for this physical situation. In this example, $C$ is the event that no two people have the same birthday.
\end{itemize}


The next example is more involved and interesting.
\begin{example} \para{Real-life situation} Imagine a man-woman pair. Their first child is random, for example, the sex of the child, or the height to which the child will ultimately grow, etc cannot be predicted with certainty. How to make a probability model that captures the situation?

\para{A  possible probability model} Let there be $n$ genes in each human, and each of the genes can take two possible values (Mendel's ``factors''), which we denote as $0$ or $1$. Then, let $\Omega=\{0,1\}^{n}=\{\x=(x_{1},\ldots ,x_{n}){\; : \;} x_{i}=0 \mbox{ or }1\}$. In this sense, each human being can be encoded as a vector in $\{0,1\}^{n}$. 

To assign probabilities, one must know the parents. Let the two parents have gene sequences ${\bf a}=(a_{1},\ldots,a_{n})$ and ${\bf b}=(b_{1},\ldots ,b_{n})$. Then the possible offsprings gene sequences are in the set $\Omega_{0}:=\{\x\in \{0,1\}^{n}{\; : \;} x_{i}=a_{i} \mbox{ or }b_{i}, \; \mbox{ for each }i\le n\}$. Let $L:=\#\{i{\; : \;} a_{i}\not=b_{i}\}$.

One possible assignment of probabilities is that each of these offsprings is equally likely. In that case we can capture the situation in the following probability models. 
\begin{enumerate}\setlength\itemsep{6pt}
\item Let $\Omega_{0}$ be the sample space and let $p_{\x}=2^{-L}$ for each $\x \in \Omega_{0}$.
\item Let $\Omega$ be the sample space and let $$p_{\x}=\begin{cases}2^{-L} & \mbox{ if }\x\in \Omega_{0}\\ 0 & \mbox{ if }\x \not\in \Omega_{0}. \end{cases}$$
\end{enumerate}
The second one has the advantage that if we change the parent pair, we don't have to change the sample space, only the elementary probabilities. What are some interesting events? Hypothetically, the susceptibility to a disease $X$ could be determined by the first ten genes, say the person is likely to get the disease if there are at-most four $1$s among the first ten. This would correspond to the event that $A=\{\x\in \Omega_{0}{\; : \;} x_{0}+\ldots+x_{10}\le 4\}$. (Caution: As far as I know, reading the genetic sequence to infer about the phenotype is still an impractical task in general). 


\parag{Reasonable model?} There are many simplifications involved here. Firstly, genes are somewhat ill-defined concepts, better defined are nucleotides in the DNA (and even then there are two copies of each gene). Secondly, there are many ``errors'' in real DNA, even the total number of genes can change, there can be big chunks missing, a whole extra chromosome etc. Thirdly, the assumption that all possible gene-sequences in $\Omega_{0}$ are equally likely is incorrect - if two genes are physically close to each other in a chromosome, then they are likely to both come from the father or both from the mother. Lastly, if our interest originally was to guess the eventual height of the child or its intelligence, then it is not clear that these are determined by the genes alone (environmental factors such as availability of food etc. also matter). Finally, in case of the problem that Solomon faced, the information about genes of the parents was not available, the model as written would be use. 
%Then, let $\Omega=to capture the situation is to take $\Omega=\{\mbox{Black}, \mbox{Brown}, \mbox{Blue}, \mbox{Green}\}$ (so we are assuming that these are all the possible colours). What about the assignment of elementary probabilities? The genes of the parents determine completely the possible combinations of genes that the child can get, and the chance of inheriting any particular combination of genes.
\end{example}

\begin{remark} We have discussed at length the reasonability of the model in this example to indicate the enormous effort needed to find a sufficiently  accurate but also reasonably simple probability model for a real-world situation. Henceforth, we shall omit such caveats and simply switch back-and-forth between a real-world situation and a reasonable-looking probability model as if there is no difference between the two. However, thinking about the appropriateness of the chosen models is much encouraged. 
\end{remark}

%\newpage
\section{Examples of discrete probability spaces}
\begin{example} \parag{Toss $n$ coins}. We saw this before, but assumed that the coins are fair. Now we do not. The sample space is
$$
\Omega=\{0,1\}^{n}=\{\underline{\ome}=(\omega_{1},\ldots ,\omega_{n}){\; : \;} \omega_{i}=0 \mbox{ or } 1 \mbox{ for each }i\le n\}.
$$
Further we assign $p_{\underline{\ome}}=\alpha_{\omega_{1}}^{(1)}\ldots \alpha_{\omega_{n}}^{(n)}$. Here $\alpha_{0}^{(j)}$ and $\alpha_{1}^{(j)}$ are supposed to indicate the probabilities that the $j^{\mbox{th}}$ coin falls tails up or heads up, respectively. Why did we take the product of $\alpha^{(j)}_{\cdot}$s and not some other combination? This is a non-mathematical question about what model is suited for the given real-life example. For now, the only justification is that empirically the above model seems to capture the real life situation accurately.

In particular, if the $n$ coins are identical, we may write $p=\alpha_{1}^{(j)}$ (for any $j$) and the elementary probabilities become  $p_{\underline{\ome}}=p^{\sum_{i}\omega_{i}}q^{n-\sum_{i}\omega_{i}}$ where $q=1-p$. 

Fix $0\le k\le n$ and let $B_{k}=\{\underline{\ome}{\; : \;} \sum_{i=1}^{n}\omega_{i}=k\}$ be the event that we see exactly $k$ heads out of $n$ tosses. Then $\mathbf{P}(B_{k})=\binom{n}{k}p^{k}q^{n-k}$. If $A_{k}$ is the event that there are at least $k$ heads, then
$\mathbf{P}(A_{k})=\sum\limits_{\ell=k}^{n}\binom{n}{\ell}p^{\ell}q^{n-\ell}$.
\end{example}

\begin{example} \parag{Toss a coin $n$ times}. Again 
\begin{align*}
\Omega=\{0,1\}^{n}&=\{\underline{\ome}=(\omega_{1},\ldots ,\omega_{n}){\; : \;} \omega_{i}=0 \mbox{ or } 1 \mbox{ for each }i\le n\}, \\
p_{\underline{\ome}}&=p^{\sum_{i}\omega_{i}}q^{n-\sum_{i}\omega_{i}}.
\end{align*}
This is the same probability space that we got for the tossing of $n$ identical looking coins. Implicit is the assumption that once a coin is tossed, for the next toss it is as good as a different coin but with the same $p$. It is possible to imagine a world where coins retain the memory of what happened before (or as explained before, we can make a ``coin'' that remembers previous tosses!), in which case this would not be a good model for the given situation. We don't believe that this is the case for coins in our world, and this can be verified empirically.
\end{example}

\begin{example} \parag{Shuffle a deck of 52 cards}. $\Omega=S_{52}$, the set of all permutations\footnote{We use the notation $[n]$ to denote the set $\{1,2,\ldots ,n\}$. A permutation of $[n]$ is a vector $(i_{1},i_{2},\ldots ,i_{n})$ where $i_{1},\ldots ,i_{n}$ are distinct elements of $[n]$, in other words, they are $1,2,\ldots ,n$ but in some order. Mathematically, we may define a permutation as a bijection $\pi:[n]\rightarrow [n]$. Indeed, for a bijection $\pi$, the numbers $\pi(1),\ldots ,\pi(n)$ are just $1,2,\ldots ,n$ in some order.} of $[52]$ and $p_{\pi}=\frac{1}{52!}$ for each $\pi\in S_{52}$.
\end{example}

\begin{example} \parag{``Psychic'' guesses a deck of cards}. The sample space is $\Omega=S_{52}\times S_{52}$ and $p_{(\pi,{\sigma})}=1/(52!)^{2}$ for each pair $(\pi,{\sigma})$ of permutations. In a pair $(\pi,{\sigma})$, the permutation $\pi$ denotes the actual order of  cards in the shuffled deck, and ${\sigma}$ denotes the order guessed by the psychic. If the guesses are purely random, then the probabilities are as we have written.

An interesting random variable is the number of correct guesses. This is the function $X:\Omega\rightarrow \mathbb{R}$ defined by $X(\pi,{\sigma})=\sum_{i=1}^{52}{\mathbf 1}_{\pi_{i}={\sigma}_{i}}$. Correspondingly we have the events $A_{k}=\{(\pi,{\sigma}){\; : \;} X(\pi,{\sigma})\ge k\}$. 
\end{example}

\begin{example} \parag{Toss a coin till a head turns up}. $\Omega=\{1,01,001,0001,\ldots \}\cup\{\overline{0}\}$. Let us write $0^{k}1=0\ldots01$ as a short form for $k$ zeros (tails) followed by $1$ and $\overline{0}$ stands for the sequence of all tails. Let $p\in [0,1]$. Then, we set $p_{0^{k}1}=q^{k}p$ for each $k\in \mathbb{N}$. We also set $p_{\overline{0}}=0$ if $p>0$ and $p_{\overline{0}}=1$ if $p=0$. This is forced on us by the requirement that elementary probabilities add to $1$.

Let $A=\{0^{k}1{\; : \;} k\ge n\}$ be the event that at least $n$ tails fall before a head turns up. Then $\mathbf{P}(A)=q^{n}p+q^{n+1}p+\ldots =q^{n}$.
\end{example}

\begin{example} \parag{Place $r$ distinguishable balls in $m$ distinguishable urns at random}. We saw this before (the words ``labelled'' and ``distinguishable'' mean the same thing here). The sample space is $\Omega=[m]^{r}=\{\underline{\ome}=(\omega_{1},\ldots ,\omega_{r}){\; : \;} 1\le \omega_{i}\le m\}$ and $p_{\underline{\ome}}=m^{-r}$ for every $\underline{\ome}\in \Omega$. Here $\omega_{i}$ indicates the urn number into which the $i^{\mbox{th}}$ ball goes.
\end{example}

\begin{example} \parag{Place $r$ indistinguishable balls in $m$ distinguishable urns at random}. 
Since the balls are indistinguishable, we can only count the number of balls in each urn. The sample space is 
$$
 \Omega=\{(\ell_{1},\ldots ,\ell_{m}){\; : \;} \ell_{i}\ge 0, \; \ell_{1}+\ldots +\ell_{m}=r\}.
$$
We give two proposals for the elementary probabilities. 
\begin{enumerate}\setlength\itemsep{6pt}
\item Let $p^{\mbox{\tiny MB}}_{(\ell_{1},\ldots ,\ell_{m})}=\frac{m!}{\ell_{1}!\ell_{2}!\ldots \ell_{m}!}\frac{1}{m^{r}}$. These are the probabilities that result if we place $r$ labelled balls in $m$ labelled urns, and then erase the labels on the balls.
%Let $\Omega_{\mbox{\tiny MB}}=\{(1,1),(1,2),(2,1),(2,2)\}$ and $p_{\omega}=1/4$ for each $\omega\in \Omega_{\mbox{\tiny MB}}$. Here $(1,1)$ indicates that the first ball is in the first urn and the second ball also is in the first urn etc. If the balls are labelled, then $\Omega_{\mbox{\tiny MB}}$ is the space of all distinguishable configurations. Elementary probabilities are chosen so that all distinguishable configurations are equally likely. 
\item Let $p^{\mbox{\tiny BE}}_{(\ell_{1},\ldots ,\ell_{m})}=\frac{1}{\binom{m+r-1}{r-1}}$ for each $(\ell_{1},\ldots ,\ell_{m})\in \Omega$. Elementary probabilities are chosen so that all distinguishable configurations are equally likely. 
\end{enumerate}
That these are legitimate probability spaces depend on two combinatorial facts.
\begin{exercise} 
\begin{enumerate}\setlength\itemsep{6pt}
\item Let $(\ell_{1},\ldots ,\ell_{m})\in \Omega$. Show that $\#\{\underline{\ome}\in [m]^{r} {\; : \;} \sum_{j=1}^{r}{\mathbf 1}_{\omega_{j}=i}=\ell_{i} \mbox{ for each }i\in [m]\} = \frac{n!}{\ell_{1}!\ell_{2}!\ldots \ell_{m}!}$. Hence or directly, show that $\sum\limits_{\omega\in \Omega}p_{\omega}^{\tiny MB}=1$.
\item Show that $\# \Omega = \binom{m+r-1}{r-1}$. Hence, $\sum\limits_{\omega\in \Omega}p_{\omega}^{\tiny BE}=1$.
\end{enumerate}

\end{exercise}

The two models are clearly different. Which one captures reality? We can arbitrarily label the balls for our convenience, and then erase the labels in the end. This clearly yields elementary probabilities $p^{\tiny MB}$. Or to put it another way, pick the balls one by one and assign them randomly to one of the urns. This suggests that $p^{\tiny MB}$ is the ``right one''. 

This leaves open the question of whether there is a natural mechanism of assigning balls to urns so that the probabilities $p^{\tiny BE}$ shows up. No such mechanism has been found. But this probability space does occur in the physical world. If $r$ photons (``indistinguishable balls'') are to occupy $m$ energy levels (``urns''), then empirically it has been verified that the correct probability space is the second one!\footnote{The probabilities $p^{\mbox{\tiny MB}}$ and $p^{\mbox{\tiny BE}}$ are called Maxwell-Boltzmann statistics and Bose-Einstein statistics. There is a third kind, called Fermi-Dirac statistics which is obeyed by electrons. For general $m\ge r$,  the sample space is $\Omega_{\mbox{\tiny FD}}=\{(\ell_{1},\ldots ,\ell_{m}){\; : \;} \ell_{i}=0 \mbox{ or }1 \mbox{ and }\ell_{1}+\ldots +\ell_{m}=r\}$ with equal probabilities for each element. In words, all distinguishable configurations are equally likely, with the added constraint that at most one electron can occupy each energy level.}
\end{example}




%\begin{example} \parag{Place $r$ balls in $n$ urns at random}. We give two proposals for probability space to capture this situation.\footnote{Standard notations: Always $\mathbb{N}=\{0,1,2,\ldots\}$ and $\mathbb{N}_{+}=\{1,2,\ldots \}$. For $n\in \mathbb{N}_{+}$, we write $[n]$ for the set $\{1,2,\ldots ,n\}$.}
%\begin{enumerate}\setlength\itemsep{6pt}
%\item Let $\Omega_{\mbox{\tiny MB}}=[n]^{r}=\{\underline{\ome}{\; : \;} \omega_{i}\in[n] \mbox{ for each }i=1,2,\ldots ,r\}$ and $p_{\underline{\ome}}=\frac{1}{n^{r}}$ for each $\underline{\ome}\in \Omega_{\mbox{\tiny MB}}$.
%\item Let $\Omega_{\mbox{\tiny BE}}=\{\underline{\ell}=(\ell_{1},\ell_{2},\ldots ,\ell_{n}){\; : \;} \ell_{i}\in \mathbb{N}, \; \ell_{1}+\ldots +\ell_{n}=r\}$ and $p_{\underline{\ell}}=\frac{1}{\#\Omega_{\mbox{\tiny BE}}}$ for each $\underline{\ome}\in \Omega_{\mbox{\tiny BE}}$. Actually $\# \Omega_{\mbox{\tiny BE}}=\binom{n+r-1}{n-1}$ but we keep that calculation for later.
%\end{enumerate}
%In the first case, $\omega_{i}$ indicates the urn number to which that $i^{\mbox{th}}$ ball goes. In the second case $\ell_{j}$ indicates the number of balls in the urn $j$. The two models are however not the same. To see this take a simple case of $r=2$ and $n=2$.
%\begin{example} When $r=n=2$, we get
%\begin{align*}
%\Omega_{\mbox{\tiny MB}}=\{(1,1),(1,2),(2,1),(2,2)\}, \qquad \Omega_{\mbox{\tiny BE}}=\{(2,0),(1,1),(0,2)\}.
%\end{align*}
%If we ask the question, ``What is the chance that there is one ball in each urn'', in the first case we get the answer $1/2$ (the event is $\{(1,2),(2,1)\}$) while in the second case we get the answer $1/3$ (the event is $\{(1,1)\}$). Which is correct?
%\end{example}
%Both answers are mathematically correct calculations in the corresponding probability spaces. What we really want to ask is which one approximates the real-life situation correctly. 

%If the balls are distinguishable (for eg., labelled $1,2,\ldots ,r$) and all distinguishable configurations are equally likely then the first probability space is correct. One mechanism that achieves this is simply to take the first ball and put it uniformly at random in one of the $n$ urns, then take the second ball and .... 

%If the balls are identical and all {\em distinguishable configurations} are equally likely then the second probability space is the right choice. What mechanism achieves this? If we follow the same mechanism as before but we the balls are indistingishable, then the sample space will be $\Omega_{\mbox{\tiny BE}}$ alright but the probabilities are not uniform. One way to see this is to close ones eyes when putting the balls in urns, that makes all balls identical, but the probability that there is one ball in each urn should not change! 
%\end{example}

%\begin{example}\parag{Children of a man-woman pair}. Consider a man and women. The set of all {\em potential} children they can have form a sample space. Each child they actually have is a realization of the (human-valued) random variable from this sample space. The height, weight etc are real-valued random variables. To make a mathematical model of it, we make a few simplifying assumptions.
%\begin{enumerate}\setlength\itemsep{6pt}
%\item Each human, in particular the two parents, have $N$ genes, each of which can be dominant or recessive.
%\item A child gets each gene from one of the two parents.
%\item Mendel's law asserting independence of distinct genes holds exactly (at this point we have not yet introduced the notion of independence, but this is just to motivate the model).
%\end{enumerate}
%The genes of the two parents can be denoted as $\alpha_{F},\alpha_{M}\in \{0,1\}^{N}$.  The space of all children is $\Omega=\{-1,+1\}^{N}$. Set $p_{\underline{\ome}}=2^{-N}$ for each $\underline{\ome}\in \Omega$. One can come up with more realistic assignments of probabilities, but that is not the purpose for the moment.

%This is an excellent example of a real-life situation of a probability space, where each parent pair defines a probability space on all possible humans. The difference between a sample point (all potential children) and a realization of a random experiment (an actual child) is quite clear. One can also see many interesting random variables such as the height, weight etc. In fact, if you consider the height of a child from birth to death, you get a function-valued random variable, called a {\em stochastic process}.
%\end{example}

\begin{example} \parag{Sampling with replacement from a population}. Define $\Omega=\{\underline{\ome}\in [N]^{k}{\; : \;} \omega_{i}\in [N] \mbox{ for }1\le i\le k\}$ with $p_{\underline{\ome}}=1/N^{k}$ for each $\underline{\ome}\in \Omega$. Here $[N]$ is the population (so the size of the population is $N$) and the size of the sample is $k$. Often the language used is of a box with $N$ coupons from which $k$ are drawn with replacement.
\end{example}

\begin{example} \parag{Sampling without replacement from a population}. Now we take 
\begin{align*}
\Omega=\left\{\underline{\ome}\in [N]^{k}{\; : \;} \omega_{i}\mbox{ are distinct elements of } [N]\right\}, \\
p_{\underline{\ome}}=\frac{1}{N(N-1)\ldots (N-k+1)}  \mbox{ for each }\underline{\ome}\in \Omega.
\end{align*}

Fix $m<N$ and define the random variable $X(\underline{\ome})=\sum_{i=1}^{k}{\mathbf 1}_{\omega_{i}\le m}$. If the population $[N]$ contains a subset, say $[m]$, (could be the subset of people having a certain disease), then $X(\underline{\ome})$ counts the number of people in the sample who have the disease. Using $X$ one can define  events such as $A=\{\underline{\ome}{\; : \;} X(\underline{\ome})=\ell\}$ for some $\ell\le m$. If $\underline{\ome}\in A$, then $\ell$ of the $\omega_{i}$ must be in $[m]$ and the rest in $[N]\setminus [m]$. Hence $$\#A=\binom{k}{\ell}m(m-1)\ldots (m-\ell+1)(N-m)(N-m-1)\ldots (N-m-(k-\ell)+1).$$
As the probabilities are equal for all sample points, we get
\begin{align*}
\mathbf{P}(A)&= \frac{\binom{k}{\ell}m(m-1)\ldots (m-\ell+1)(N-m)(N-m-1)\ldots (N-m-(k-\ell)+1)}{N(N-1)\ldots (N-k+1)} \\
 &= \frac{1}{\binom{N}{k}}\binom{m}{\ell}\binom{N-m}{k-\ell}.
\end{align*}
This expression arises whenever the population is subdivided into two parts and we count the number of samples that fall in one of the sub-populations.
\end{example}

\begin{example}\parag{Gibbs measures}. Let $\Omega$ be a finite set and let ${\mathcal H}:\Omega\rightarrow \mathbb{R}$ be a function. Fix $\beta\ge 0$. Define $Z_{\beta}=\sum_{\omega}e^{-\beta {\mathcal H}(\omega)}$ and then set $p_{\omega}=\frac{1}{Z_{\beta}}e^{-\beta {\mathcal H}(\omega)}$. This is clearly a valid assignment of probabilities.

This is a class of examples from statistical physics. In that context, $\Omega$ is the set of all possible states of a system and ${\mathcal H}(\omega)$ is the energy of the state $\omega$. In mechanics a system settles down to the state with the lowest possible energy, but if there are thermal fluctuations (meaning the ambient temperature is not absolute zero), then the system may also be found in other states, but higher energies are less and less likely. In the above assignment, for two states $\omega$ and $\omega'$, we see that $p_{\omega}/p_{\omega'}=e^{\beta ({\mathcal H}(\omega')-{\mathcal H}(\omega))}$ showing that higher energy states are less probable. When $\beta=0$, we get $p_{\omega}=1/|\Omega|$, the uniform distribution on $\Omega$. In statistical physics, $\beta$ is equated to $1/\kappa T$ where $T$ is the temperature and $\kappa$ is Boltzmann's constant.

Different physical systems are defined by choosing $\Omega$ and ${\mathcal H}$ differently. Hence this provides a rich class of examples which are of great importance in probability.
\end{example}

It may seem that probability is trivial, since the only problem is to find the sum of $p_{\omega}$ for $\omega$ belonging to event of interest. This is far from the case. The following example is an illustration.

\begin{example}\parag{Percolation}. Fix $m,n$ and consider a rectangle in $\mathbb{Z}^{2}$, $R=\{(i,j)\in \mathbb{Z}^{2}{\; : \;} 0\le i\le n, \ 0\le j\le m\}$. Draw this on the plane along with the grid lines. We see $(m+1)n$ horizontal edges and $(n+1)m$ vertical edges. Let $E$ be the set of $N=(m+1)n+(n+1)m$ edges and let $\Omega$ be the set of all subsets of $E$. Then $|\Omega|=2^{N}$. Let $p_{\omega}=2^{-N}$ for each $\omega \in \Omega$. An interesting event is 
\begin{align*}
A=\{\omega \in \Omega{\; : \;} & \mbox{ the subset of edges in }\omega \\
 & \mbox{ connect  the top side of }R \mbox{ to the bottom side of }R\}.
\end{align*}

This may be thought of as follows. Imagine that each edge is a pipe through which water can flow. However each tube may be blocked or open. $\omega$ is the subset of pipes that are open. Now pour water at the top of the rectangle $R$. Will water trickle down to the bottom? The answer is yes if and only if $\omega$ belongs to $A$.

Finding $\mathbf{P}(A)$ is a very difficult problem. When $n$ is large and $m=2 n$, it is expected that $\mathbf{P}(A)$ converges to a specific number, but proving it is an open problem as of today!\footnote{In a very similar problem on a triangular lattice, it was proved by Stanislav Smirnov (2001) for which he won a fields medal. Proof that computing probabilities is not always trivial!}
\end{example}



We now give two non-examples.
\begin{example}\parag{A non-example - Pick a natural number uniformly at random}. The sample space is clearly $\Omega=\mathbb{N}=\{1,2,3,\ldots\}$. The phrase ``uniformly at random'' suggests that the elementary probabilities should be the same for all elements. That is $p_{i}=p$ for all $i\in \mathbb{N}$ for some $p$. If $p=0$, then $\sum_{i\in \mathbb{N}}p_{i}=0$ whereas if $p>0$, then $\sum_{i\in \mathbb{N}}p_{i}=\infty$. This means that there is no way to assign elementary probabilities so that each number has the same chance to be picked.

This appears obvious, but many folklore puzzles and paradoxes in probability are based on the faulty assumption that it is possible to pick a natural number at random. For example, when asked a question like ``What is the probability that a random integer is odd?'', many people answer $1/2$. We want to emphasize that the probability space has to be defined first, and only then can probabilities of events be calculated. Thus, the question does not make sense to us and we do not have to answer it!\footnote{For those interested, there is one way to make sense of such questions. It is to consider a sequence of probability spaces $\Omega^{(n)}=\{1,2,\ldots ,n\}$ with elementary probabilities $p^{(n)}_{i}=1/n$ for each $i\in \Omega_{n}$. Then, for a subset $A\subseteq \mathbb{Z}$, we consider $\mathbf{P}_{n}(A\cap \Omega_{n})=\#(A\cap [n])/n$. If these probabilities converge to a limit $x$ as $n\rightarrow \infty$, then we could say that $A$ has asymptotic probability $x$. In this sense, the set of odd numbers does have  asymptotic probability $1/2$, the set of numbers divisible by $7$ has asymptotic probability $1/7$ and the set of prime numbers has asymptotic probability $0$. However, this notion of asymptotic probability has many shortcomings. Many subsets of natural numbers will not have an asymptotic probability, and even sets which do have asymptotic probability fail to satisfy basic rules of probability that we shall see later. Hence, we shall keep such examples out of our system.} 
\end{example}

\begin{example}\parag{Another non-example - Throwing darts}. A dart is thrown at a circular dart board. We assume that the dart does hit the board but were it hits is ``random'' in the same sense in which we say the a coin toss is random. Intuitively this appears to make sense. However our framework is not general enough to incorporate this example. Let us see why.

The dart board can be considered to be the disk $\Omega=\{(x,y){\; : \;} x^{2}+y^{2}\le r^{2}\}$ of given radius $r$. This is an uncountable set. We cannot assign elementary probabilities $p_{(x,y)}$ for each $(x,y)\in \Omega$ in any reasonable way. In fact the only reasonable assignment would be to set $p_{(x,y)}=0$ for each $(x,y)$ but then what is $\mathbf{P}(A)$ for a subset $A$? Uncountable sums are not well defined. 

We need a branch of mathematics called {\em measure theory} to make proper sense of uncountable probability spaces. This will not be done in this course although we shall later say a bit about the difficulties involved. The same difficulty shows up in the following ``random experiments'' also.
\begin{enumerate}\setlength\itemsep{6pt}
\item \parag{Draw a number at random from the interval $[0,1]$}. $\Omega=[0,1]$ which is uncountable.
\item \parag{Toss a fair coin infinitely many times}. $\Omega=\{0,1\}^{\mathbb{N}}:=\{\underline{\ome}=(\omega_{1},\omega_{2},\ldots ){\; : \;} \omega_{i}=0 \mbox{ or }1\}$. This is again an uncountable set.
\end{enumerate}
\end{example}
\begin{remark} In one sense, the first non-example is almost irredeemable but  the second non-example can be dealt with, except for technicalities beyond this course. We shall later give a set of working rules to work with such ``continuous probabilities''. Fully satisfactory development will have to wait for a course in measure theory.
\end{remark}

\newpage
\section{Countable and uncountable}
%We quickly recall the basic notions of counting and comparing infinite sets as will be required for us. The basic notions are of sets and functions about which we do not say anything here. We also assume that you know what it means for a function to be injective (one-one) or surjective (onto) or bijective (one-one and onto). We shall also assume that you know the set of natural numbers \footnote{Quickly, here is how natural numbers are constructed from basic notions of set theory. We start with the empty set $\emptyset$ and call it $0$ (zero). The power set of the empty set is the set $\{\emptyset\}$ which we call $1$ (one). The power set of $1$ is the set $\{\emptyset, \{\emptyset\}\}$ which we call $2$. And so on... }.

%\begin{definition} Let $A$ and $B$ be two sets. We say that $B$ is at least as large as $A$, and write $A\lesssim B$ if there exists an injective function $f:A\rightarrow B$.
%\end{definition}
%This agrees with the usual ideas we have regarding finite sets. For example, if there are a certain number of people in a room and each of them is sitting on a chair, and no two people are sitting in the same chair, then there must be at least as many chairs as there are people. This is also the case as per the above definition, since the function that takes a person to the chair he/she is sitting on, is injective. However, it certain things that


\begin{definition} An set $\Omega$ is said to be {\em finite} if there is an $n\in \mathbb{N}$ and a bijection from $\Omega$ onto $[n]$. An infinite set $\Omega$ is said to be countable if there is a bijection from $\mathbb{N}$ onto $\Omega$. 
\end{definition}
Generally, the word countable also includes finite sets. If $\Omega$ is an infinite countable set, then using any bijection $f:\mathbb{N}\rightarrow \Omega$, we can list the elements of $\Omega$ as a sequence $$f(1),f(2),f(3)\ldots$$ so that each element of $\Omega$ occurs exactly once in the sequence. Conversely, if you can write the elements of $\Omega$ as a sequence, it defines an injective function from natural numbers onto $\Omega$ (send $1$ to the first element of the sequence, $2$ to the second element etc).

\begin{example} The set of integers $\mathbb{Z}$ is countable. Define $f:\mathbb{N}\rightarrow \mathbb{Z}$ by 
$$
f(n)=\begin{cases}\frac{1}{2} n & \mbox{ if }n\mbox{ is even}. \\
-\frac{1}{2} (n-1) & \mbox{ if }n\mbox{ is odd}.
\end{cases}
$$ 
It is clear that $f$ maps $\mathbb{N}$ into $\mathbb{Z}$. Check that it is one-one and onto. Thus, we have found a bijection from $\mathbb{N}$ onto $\mathbb{Z}$ which shows that $\mathbb{Z}$ is countable. This function is a formal way of saying the we can list the elements of $\mathbb{Z}$ as
$$
0, +1, -1,+2,-2,+3,-3,\ldots.
$$
It is obvious, but good to realize there are wrong ways to try writing such a list. For example, if you list all the negative integers first, as $-1,-2,-3,\ldots$, then you will never arrive at $0$ or $1$, and hence the list is incomplete!
\end{example}

\begin{example} The set $\mathbb{N}\times \mathbb{N}$ is countable. Rather than give a formula, we list the elements of $\mathbb{Z}\times \mathbb{Z}$ as follows.
$$
(1,1), \; (1,2), (2,1), \; (1,3), (2,2), (3,1), \; (1,4),(2,3),(3,2),(4,1),  \; \ldots 
$$
The pattern should be clear. Use this list to define a bijection from $\mathbb{N}$ onto $\mathbb{N}\times \mathbb{N}$ and hence show that $\mathbb{N}\times \mathbb{N}$ is countable.
\end{example}

\begin{example} The set $\mathbb{Z}\times \mathbb{Z}$ is countable. This follows from the first two examples. Indeed, we have a bijection $f:\mathbb{N}\rightarrow \mathbb{Z}$ and a bijection $g:\mathbb{N}\times \mathbb{N} \rightarrow \mathbb{N}$. Define a bijection $F:\mathbb{N}\times \mathbb{N}\rightarrow \mathbb{Z}\times \mathbb{Z}$ by composing them, i.e., $F(n,m)=f(g(n))$. Then, $F$ is one-one and onto. This shows that $\mathbb{Z}\times \mathbb{Z}$ is indeed countable.
\end{example}

\begin{example} The set of rational numbers $\mathbb{Q}$ is countable. Recall that rational numbers other than $0$ can be written uniquely in the form $p/q$ where $p$ is a non-zero integer and $q$ is a strictly positive integer, and there are no common factors of $p$ and $q$ (this is called the {\em lowest form} of the rational number $r$). Consider the map $f:\mathbb{Q}\rightarrow \mathbb{Z}\times \mathbb{Z}$ defined by
$$
f(r) = \begin{cases} (0,1) & \mbox{ if }r=0 \\ (p,q) & \mbox{ if }r=\frac{p}{q} \mbox{ in the lowest form}. \end{cases}
$$
Clearly, $f$ is injective and hence, it appears that $\mathbb{Z}\times \mathbb{Z}$ is a ``bigger set'' than $\mathbb{Q}$. Next define the function $g:\mathbb{Z}\rightarrow \mathbb{Q}$ by setting $g(n)=n$. This is also injective and hence we may say that ``$\mathbb{Q}$ is a bigger set than $\mathbb{N}$. 

But we  have already seen that $\mathbb{N}$ and $\mathbb{Z}\times \mathbb{Z}$ are in bijection with each other, in that sense, they are of equal size. Since $\mathbb{Q}$ is sandwiched between the two it ought to be true that $\mathbb{Q}$ has the same size as $\mathbb{N}$, and thus countable.

This reasoning is not incorrect, but an argument is needed to make it an honest proof. This is indicated in the Schr\"{o}der-Bernstein theorem stated later. Use that to fill the gap in the above argument, or alternately, try to directly find a bijection between $\mathbb{Q}$ and $\mathbb{N}$.
\end{example}



\begin{example} The set of real numbers $\mathbb{R}$ is not countable. The extraordinarily proof of this fact is due to Cantor, and the core idea, called the {\em diagonalization trick} is one that can be used in many other contexts. 

Consider any function $f:\mathbb{N} \rightarrow [0,1]$. We show that it is not onto, and hence not a bijection. Indeed, use the decimal expansion to write a number $x\in [0,1]$ as $0.x_{1}x_{2}x_{3}\ldots$ where $x_{i}\in \{0,1,\ldots ,9\}$. Write the decimal expansion for each of the numbers $f(1),f(2),f(3),.\ldots$ as follows.
\begin{align*}
f(1)&=0.X_{1,1}X_{1,2}X_{1,3}\ldots \\
f(2)&=0.X_{2,1}X_{2,2}X_{2,3}\ldots \\
f(3)&=0.X_{3,1}X_{3,2}X_{3,3}\ldots \\
\cdots & \cdots \; \cdots \; \cdots
\end{align*}
Let $Y_{1},Y_{2},Y_{3},\ldots$ be any numbers in $\{0,1,\ldots ,9\}$ with the only condition that $Y_{i}\not= X_{i,i}$. Clearly it is possible to choose $Y_{i}$ like this. Now consider the number $y=0.Y_{1}Y_{2}Y_{3}\ldots$ which is a number in $[0,1]$. However, it does not occur in the above list. Indeed, $y$ disagrees with $f(1)$ in the first decimal place, disagrees with $f(2)$ in the second decimal place etc. Thus, $y\not= f(i)$ for any $i\in \mathbb{N}$ which means that $f$ is not onto $[0,1]$. 

Thus, no function $f:\mathbb{N}\rightarrow [0,1]$ is onto, and hence there is no bijection from $\mathbb{N}$ onto $[0,1]$ and hence $[0,1]$ is not countable. Obviously, if there is no onto function onto $[0,1]$, there cannot be an onto function onto $\mathbb{R}$. Thus, $\mathbb{R}$ is also uncountable.
\end{example}

\begin{example} Let $A_{1},A_{2},\ldots $ be subsets of a set $\Omega$. Suppose each $A_{i}$ is countable (finite is allowed). Then $\cup_{i}A_{i}$ is also countable. We leave it as an exercise. [{\em Hint:} If each $A_{i}$ is countably infinite and pairwise disjoint, then $\cup A_{i}$ can be thought of as $\mathbb{N}\times \mathbb{N}$].
\end{example}

\begin{lemma}[Schr\"{o}der-Bernstein] Let $A,B$ be two sets and suppose there exist injective functions $f:A\rightarrow B$ and $g:B\rightarrow A$. Then, there exists a bijective function $h:A\rightarrow B$.
\end{lemma}
We omit the proof as it is irrelevant to the rest of the course\footnote{For those interested, we describe the idea of the proof somewhat informally. Consider the two sets $A$ and $B$ (assumed to have no common elements) and draw a blue arrow from each $x\in A$ to $f(x)\in B$ and a red arrow from each $y\in B$ to $g(y)\in A$. Start at any $x\in A$ or $y\in B$ and follow the arrows in the forward and backward directions. There are only three possibilities
\begin{enumerate}\setlength\itemsep{6pt}
\item The search closes, and we discover a cycle of alternating blue and red arrows.
\item The backward search ends after finitely many steps and the forward search continues forever.
\item Both the backward and forward searches continue forever.
\end{enumerate}
The injectivity of $f$ and $g$ is used in checking that these are the only possibilities. In the first and third case, just use the blue arrows to define the function $h$. In the second case, if the first element of the chain is in $A$, use the blue arrows, and if the first element is in $B$ use the red arrows (but in reverse direction) to define the function $h$. Check that the resulting function is a bijection!}.

\section{On infinite sums} There were some subtleties in the definition of probabilities which we address now. The definition of $\mathbf{P}(A)$ for an event $A$ and $\mathbf{E}[X]$ for a random variable $X$ involve infinite sums (when $\Omega$ is countably infinite). In fact, in the very definition of probability space, we had the condition that $\sum_{\omega}p_{\omega}=1$, but what is the meaning of this sum when $\Omega$ is infinite? In this section, we make precise the notion of infinite sums. In fact we shall give two methods of approach, it suffices to consider only the first.

\subsection{First approach} Let $\Omega$ be a countable set, and let $f:\Omega\rightarrow \mathbb{R}$ be a function. We want to give a meaning to the infinite sum $\sum_{\omega\in \Omega}f(\omega)$. First we describe a natural attempt and then address the issues that it leaves open.

\para{The idea} By definition of countability, there is a bijection $\varphi:\mathbb{N}\rightarrow \Omega$ which allows us to list the elements of $\Omega$ as $\omega_{1}=\varphi(1),\omega_{2}=\varphi(2),\ldots$. Consider the partial sums $x_{n}=f(\omega_{1})+f(\omega_{2})+\ldots +f(\omega_{n})$. Since $f$ is non-negative, these numbers are non-decreasing, i.e., $x_{1}\le x_{2}\le x_{3}\le \ldots$. Hence, they converge to a finite number or to $+\infty$ (which is just another phrase for saying that the partial sums grow without bound). We would like to simply define the sum $\sum_{\omega\in \Omega}f(\omega)$ as the limit $L=\lim_{n\rightarrow \infty}(f(\omega_{1})+\ldots +f(\omega_{n})$, which may be finite or $+\infty$.

The problem is that this may depend on the bijection $\Omega$ chosen. For example, if $\psi:\mathbb{N}\rightarrow \Omega$ is a different bijection, we would write the elements of $\Omega$ in a different sequence $\omega_{1}'=\psi(1),\omega_{2}'=\psi(2),\ldots$, the partial sums $y_{n}=f(\omega_{1}')+\ldots +f(\omega_{n}')$ and then define $\sum_{\omega\in \Omega}f(\omega)$ as the limit $L'=\lim_{n\rightarrow \infty}(f(\omega_{1}')+\ldots +f(\omega_{n}')$. 

Is it necessarily true that $L=L'$? 

\para{Case I - Non-negative $f$} We claim that for any two bijections $\varphi$ and $\psi$ as above, the limits are the same (this means that the limits are $+\infty$ in both cases, or the same finite number in both cases). Indeed, fix any $n$ and recall that $x_{n}=f(\omega_{1})+\ldots +f(\omega_{n})$. Now, $\psi$ is surjective, hence there is some $m$ (possibly very large) such that $\{\omega_{1},\ldots,\omega_{n}\}\subseteq \{\omega_{1}',\ldots ,\omega_{m}'\}$. Now, we use the non-negativity of $f$ to observe that 
$$
f(\omega_{1})+\ldots +f(\omega_{n})\le f(\omega_{1}')+\ldots +f(\omega_{m}').
$$ 
This is the same as $x_{n}\le y_{m}$. Since $y_{k}$ are non-decreasing, it follows that $x_{n}\le y_{m}\le y_{m+1}\le y_{m+2}\ldots$, which implies that $x_{n}\le L'$. Now let $n\rightarrow \infty$ and conclude that $L\le L'$. Repeat the argument with the roles of $\varphi$ and $\psi$ reversed to conclude that $L'\le L$. Hence $L=L'$, as desired to show. 

In conclusion, for non-negative functions $f$, we can assign an unambiguous meaning to $\sum_{\omega}f(\omega)$ by setting it equal to $\lim_{n\rightarrow \infty}(f(\varphi(1)+\ldots +f(\varphi(n)))$, where $\varphi:\mathbb{N}\rightarrow \Omega$ is any bijection (the point being that the limit does not depend on the bijection chosen), and the limit here may be allowed to be $+\infty$ (in which case we say that the sum does not converge).

\para{Case II - General $f:\Omega\rightarrow \mathbb{R}$} The above argument fails if $f$ is allowed to take both positive and negative values (why?). In fact, the answers $L$ and $L'$ from different bijections may be completely different. An example is given later to illustrate this point. For now, here is how we deal with this problem. 

For a real number $x$ we introduce the notations, $x_{+}=x\vee 0$ and $x_{-}=(-x)\vee 0$. Then $x=x_{+}-x_{-}$ while $|x|=x_{+}+x_{-}$.  Define the non-negative functions $f_{+},f_{-}:\Omega\rightarrow \mathbb{R}_{+}$ by $f_{+}(\omega)=(f(\omega))_{+}$ and $f_{-}(\omega)=(f(\omega))_{-}$. Observe that $f_{+}(\omega)-f_{-}(\omega)=f(\omega)$ while $f_{+}(\omega)+f_{-}(\omega)=|f(\omega)|$, for all $\omega\in \Omega$. 
\begin{example} Let $\Omega=\{a,b,c,d\}$ and let $f(a)=1$, $f(b)=-1$, $f(c)=-3$, $f(4)=-0.3$. Then, $f_{+}(a)=1$ and $f_{+}(b)=f_{+}(c)=f_{+}(d)=0$ while $f_{-}(1)=0$ and $f_{-}(b)=1$, $f_{-}(c)=3$, $f_{-}(d)=0.3$.
\end{example} 
 
Since $f_{+}$ and $f_{-}$ are non-negative functions, we know how to define their sums. Let $S_{+}=\sum_{\omega}f_{+}(\omega)$ and $S_{-}=\sum_{\omega}f_{-}(\omega)$. Recall that one or both of $S_{+},S_{-}$ could be equal to $+\infty$, in which case we say that $\sum_{\omega}f(\omega)$ {\em does not converge absolutely} and do not assign it any value. If both $S_{+}$ and $S_{-}$ are finite, then we define $\sum_{\omega}f(\omega)= S_{+}-S_{-}$. In this case we say that $\sum f$ {\em converges absolutely}. 

This completes our definition of absolutely convergent sums. A few exercises to show that when working with absolutely convergent sums, the usual rules of addition remain valid. For example, we can add the numbers in any order.

\begin{exercise} Show that $\sum_{\omega}f(\omega)$ converges absolutely if and only if $\sum_{\omega}|f(\omega)|$ is finite (since $|f(\omega)|$ is a non-negative function, this latter sum is always defined, and may equal $+\infty$).
\end{exercise} 

For non-negative $f$, we can find the sum by using any particular bijection and then taking limits of partial sums. What about general $f$?
\begin{exercise} Let $f:\Omega \rightarrow \mathbb{R}$. Suppose $\sum_{\omega\in \Omega}f(\omega)$ be summable and let the sum be $S$. Then, for any bijection $\varphi:\mathbb{N}\rightarrow \Omega$, we have $\lim_{n\rightarrow \infty}(f(\varphi(1))+\ldots +f(\varphi(n)))=S$.

Conversely, if $\lim_{n\rightarrow \infty}(f(\varphi(1))+\ldots +f(\varphi(n)))$ exists and is the same finite number for any bijection $\varphi:\mathbb{N}\rightarrow \mathbb{R}$, then $f$ must be absolutely summable and $\sum_{\omega\in \Omega}f(\omega)$ is equal to this common limit.
\end{exercise}

The usual properties of summation without which life would not be worth living, are still valid.
\begin{exercise}\label{ex:linearitymonotonicityofsums} Let $f,g:\Omega\rightarrow \mathbb{R}_{+}$ and $a,b\in \mathbb{R}$. If $\sum f$ and $\sum g$ converge absolutely,  then  $\sum (af+bg)$ converges absolutely and $\sum (af+bg) = a\sum f +b\sum g$. Further, if $f(\omega)\le g(\omega)$ for all $\omega\in \Omega$, then $\sum f \le \sum g$.
\end{exercise}


\begin{example} This example will illustrate why we refuse to assign a value to $\sum_{\omega}f(\omega)$ in some cases. Let $\Omega=\mathbb{Z}$ and define $f(0)=0$ and $f(n)=1/n$ for $n\not=0$. At first one may like to say that $\sum_{n\in \mathbb{Z}}f(n)=0$, since we can cancel $f(n)$ and $f(-n)$ for each $n$. However, following our definitions
$$
f_{+}(n)=\begin{cases}\frac{1}{n} & \mbox{ if }n\ge 1\\ 0 &\mbox{ if }n\le 0, \end{cases} \qquad f_{-}(n)=\begin{cases}\frac{1}{n} & \mbox{ if }n\le -1\\ 0 &\mbox{ if }n\ge 0. \end{cases}
$$
Hence $S_{+}$ and $S_{-}$ are both $+\infty$ which means our definition does not assign any value to the sum $\sum_{\omega}f(\omega)$.

Indeed, by ordering the numbers appropriately, we can get any value we like! For example, here is how to get $10$. We know that $1+\frac{1}{2}+\ldots +\frac{1}{n}$ grows without bound. Just keep adding these positive number till the sum exceeds $10$ for the first time. Then start adding the negative numbers $-1-\frac{1}{2}-\ldots -\frac{1}{m}$ till the sum comes below $10$. Then add the positive numbers $\frac{1}{n+1}+\frac{1}{n+2}+\ldots +\frac{1}{n'}$ till the sum exceeds $10$ again, and then negative numbers till the sum falls below $10$ again, etc. Using the fact that the individual terms in the series are going to zero, it is easy to see that the partial sums then converge to $10$. There is nothing special about $10$, we can get any number we want!
\end{example}
One last remark on why we assumed $\Omega$ to be countable.
\begin{remark} What if $\Omega$ is uncountable? Take any $f:\Omega\rightarrow \mathbb{R}_{+}$. Define the sets $A_{n}=\{\omega{\; : \;} f(\omega)\ge 1/n\}$. For some $n$, if $A_{n}$ has infinitely many elements, then clearly the only reasonable value that we can assign to $\sum f(\omega)$ is $+\infty$ (since the sum over elements of $A_{n}$ itself is larger than any finite number). Therefore, for $\sum f(\omega)$ to be a finite number it is essential that $A_{n}$ is a finite set for each set.

Now, a countable union of finite sets is countable (or finite). Therefore $A=\bigcup_{n}A_{n}$ is a countable set. But note that $A$ is also the set $\{\omega{\; : \;} f(\omega)>0\}$ (since, if $f(\omega)>0$ it must belong to some $A_{n}$). Consequently, even if the underlying set $\Omega$ is uncountable, our function will have to be equal to zero except on a countable subset of $\Omega$. In other words, we are reduced to the case of countable sums!
\end{remark}

\subsection{Second approach} In the first approach, we assumed that you are already familiar with the notion of limits and series and used them to define countable sums. In the second approach, we start from scratch and define infinite sums. The end result is exactly the same. For the purposes of this course, you may ignore the rest of the section. 
\begin{definition} If $\Omega$ is a countable set and $f:\Omega \rightarrow \mathbb{R}_{+}$ is a non-negative function, then we \underline{define} 
$$
\sum\limits_{\omega}f(\omega) :=\sup\left\{\sum\limits_{\omega\in A}f(\omega) {\; : \;} A\subseteq \Omega \mbox{ is finite}\right\}
$$
where the supremum takes values in $\overline{\mathbb{R}}_{+}=\mathbb{R}_{+}\cup\{+\infty\}$. We say that $\sum f(\omega)$ converges if the supremum has a finite value. 
\end{definition}
\begin{exercise} Show that if $f,g:\Omega\rightarrow \mathbb{R}_{+}$ and $a,b\in \mathbb{R}_{+}$, then  $\sum (af+bg) = a\sum f +b\sum g$. Further, if $f(\omega)\le g(\omega)$ for all $\omega\in \Omega$, then $\sum f \le \sum g$.
\end{exercise}


Next, we would like to remove the condition of non-negativity. For a real number $x$ we write $x_{+}=x\vee 0$ and $x_{-}=(-x)\vee 0$. Then $x=x_{+}-x_{-}$ while $|x|=x_{+}+x_{-}$.  
\begin{definition} Now suppose $f:\Omega\rightarrow \mathbb{R}$ takes both positive and negative values. Then we first define the non-negative functions $f_{+},f_{-}:\Omega\rightarrow \mathbb{R}_{+}$ by $f_{+}(\omega)=(f(\omega))_{+}$ and $f_{-}(\omega)=(f(\omega))_{-}$ and set $S_{+}=\sum_{\omega}f_{+}(\omega)$ and $S_{-}=\sum_{\omega}f_{-}(\omega)$. If both $S_{+}$ and $S_{-}$ are finite, then we define $\sum_{\omega}f(\omega)= S_{+}-S_{-}$. 
\end{definition}
\begin{remark} The condition that $S_{+}$ and $S_{-}$ are both finite is the same as the condition that $\sum_{\omega}|f(\omega)|$ is finite. If these happen,  we say that the sum $\sum f(\omega)$ {\em converges absolutely}.
\end{remark}
\begin{remark} Sometimes it is convenient to set $\sum f(\omega)$ to $+\infty$ if  $S_{+}=\infty$ and $S_{-}<\infty$ and set $\sum f(\omega)$ to $-\infty$ if $S_{+}<\infty$ and $S_{-}=\infty$. But there is no reasonable value to assign if both the sums are infinite. 
\end{remark}

\begin{exercise} Show that the two approaches give the same answers.
\end{exercise}
%We end the section with a last remark on uncountable sums.
%\noindent{\bf Answer:} Suppose $\Omega$ is an arbitrary set and let $f:\Omega \rightarrow \mathbb{R}_{+}$ be a non-negative function. Let $A_{k}=\{\omega{\; : \;} f(\omega)>1/k\}$. If $A_{k}$ is an infinite set, obviously the supremum in the definition of $\sum f(\omega)$ is $+\infty$. One the other hand, if $A_{k}$ is a finite set for each $k$, then $\{\omega{\; : \;} f(\omega)>0\}=\cup_{k}A_{k}$ is a countable set. Hence even if we allow $\Omega$ to be uncountable, the only functions for which we can define the sum are those that vanish except at a countable set of points. 


%\vspace{3mm}
%\noindent{\bf Question:} A function on $\mathbb{N}$ is what is called a sequence and then one way to define its sum is as $\lim_{N\rightarrow \infty}\sum_{n=1}^{N}f(n)$, provided the limit exists. Since $\Omega$ is countable, couldn't we simply  take a bijection $\varphi:\mathbb{N}\rightarrow \Omega$ and define $\sum_{\omega}f(\omega)$ as $\lim_{N\rightarrow \infty}\sum_{k=1}^{N}f(\varphi(k))$ instead of such a roundabout definition?

%

%\noindent{\bf Answer:} The difference between $\mathbb{N}$ and a arbitrary countable set $\Omega$ is that integers come with a natural order, and this order is exploited in the definition of the sum of the  series. In other words, there are many bijections from $\mathbb{N}\rightarrow \Omega$ and there is no way to choose one over the other. If $\varphi$ and $\psi$ are two such, should we define $\sum_{\omega}f(\omega)$ as $\lim_{N\rightarrow \infty}\sum_{k=1}^{N}f(\varphi(k))$ or as $\lim_{N\rightarrow \infty}\sum_{k=1}^{N}f(\psi(k))$? There is no ambiguity if these limits are equal for all bijections, but is that the case? It is not hard to prove the following fact which justifies our round-about definition.

%\begin{exercise} Let $\Omega$ be countable and let $f:\Omega\rightarrow \mathbb{R}$. Then, the following are equivalent.
%\begin{enumerate}\setlength\itemsep{6pt}
%\item $\sum f$ is absolutely convergent in our sense.
%\item For any bijection $\varphi:\mathbb{N}\rightarrow \Omega$, the limit $\lim_{N\rightarrow \infty}\sum_{k=1}^{N}f(\varphi(k))$ exists and its value is the same for all bijections $\varphi$.
%\end{enumerate}
%\end{exercise}
%In summary, an arbitrary countable set can be put in one-to-one correspondence with $\mathbb{N}$, but if two people choose two different bijections, the induced order on $\Omega$ will be different. If we want an order independent sum, then our definition is the only choice. The following example is illustrative.

%
%\begin{example} Let $f(n)=(-1)^{n-1}/n$. Then, $f_{+}(n)=1/n$ for even $n$ and zero for odd $n$ while $f_{-}(n)=1/n$ for odd $n$ and zero for even $n$. Since $\sum 1/2n$ and $\sum 1/(2n+1)$ both diverge, the sum $\sum f(n)$ is not defined (by our definition). However $\lim_{N\rightarrow \infty}\sum_{n=1}^{N}f(n)$ does exist and equals $\log 2$. This latter depends very much on the order of summation.
%\end{example}

%In light of this wisdom of infinite sums, let us revisit the definition of probability space. The sample space is assumed to be countable, and $p:\Omega\rightarrow \mathbb{R}_{+}$ is such that $\sum_{\omega\in \Omega}p_{\omega}=1$. This just means that for any finite set $A\subseteq \Omega$, the sum $\sum_{\omega\in A}p_{\omega}\le 1$ and that there exist finite subsets for which this sum is arbitrarily close to $1$.

%Next, if $A\subseteq \Omega$ is an event, we defined $\mathbf{P}(A)=\sum_{\omega \in \Omega}p_{\omega}$. Again, this is the supremum over finite subsets of $A$. Since any finite subset of $A$ is a finite subset of $\Omega$, we see that $\mathbf{P}(A)$ is well defined and in $[0,1]$. Further, from the exercises on infinite sums, we see that $\mathbf{P}(A)+\mathbf{P}(B)=\mathbf{P}(A\cup B)$ if $A$ and $B$ are disjoint. More of this later.

%Lastly, if $X:\Omega\rightarrow \mathbb{R}$ is a random variable, we defined its expected value as $\sum_{\omega\in \Omega}X(\omega)p_{\omega}$. What ensures that this sum converges absolutely? There is no such assurance! We must amend our definition to
%\begin{definition} Let $X:\Omega\rightarrow \mathbb{R}$ be a random variable. If the sum $\sum_{\omega}X(\omega)p_{\omega}$ converges absolutely, then we say that $X$ has expectation and define its expected value as $\mathbf{E}[X]=\sum_{\omega}X(\omega)p_{\omega}$. 
%\end{definition}


%\begin{center}
%{\bf Homework 1 (due 21/Aug/2012)}

%\medskip
%\bf Try all the exercises. Submit only those marked with an asterisk (*). 
%\end{center}

%\begin{problem} (Feller, I.8.1) Among the digits 1,2,3,4,5 first one is chosen, and then a second selection is made among the remaining four digits. Assume that all twenty possible results have the same probability. Find the probability that an odd digit will be selected {\em (a)} the first time, {\em (b)} the second time, {\em (c)} both times.
%\end{problem}

%\medskip
%\begin{problem} (*) (Feller, II.10.3) In how many ways can two rooks of different colours be put on a chessboard so that they can take each other?
%\end{problem}

%\medskip
%\begin{problem} (Feller, II.10.5) The numbers $1,2,\ldots ,n$ are arranged in random order. Find the probability that the digits {\em (a)} 1 and 2,  {\em (b)} 1, 2, and 3,  \; appear as neighbours in the order named.
%\end{problem}

%\medskip
%\begin{problem} (Feller, II.10.8) What is the probability that among $k$ digits  {\em (a)} 0 does not appear;  {\em (b)} 1 does not appear; {\em (c)} neither 0 nor 1 appears; {\em (d)} at least one of the two digits $0$ or $1$ does not appear? Let $A$ and $B$ represent the events in {\em (a)} and {\em (b)}. Express the other events in terms of $A$ and $B$.
%\end{problem}

%\medskip
%\begin{problem} (Feller, II.10.11) A man is given $n$ keys of which only one fits his door. He tries them successively (sampling without replacement). The number of trials required may be $1,2,\ldots ,n$. Show that each of these outcomes has probability $1/n$.
%\end{problem}

%\medskip
%\begin{problem} (*) (Feller, II.10.20) From a population of $N$ elements a sample of size $k$ is taken. Find the probability that none of $m$ prescribed elements will be included in the sample, assuming the sample to be {\em (a)} without replacement, \; {\em (b)} with replacement. Compare the numerical values for the two methods when {\em (i)} $N=100$, $m=k=3$, and {\em (ii)} $N=100$, $m=k=10$.
%\end{problem}

%\medskip
%\begin{problem} (Feller, II.10.28) A group of $2N$ boys and $2N$ girls is divided into two equal groups. Find the probability $p$ that each group has equal number of boys and girls. Estimate $p$ using Stirling's approximation.
%\end{problem}

%\medskip
%\begin{problem} (*) (Feller, II.10.39) If $r_{1}$ indistinguishable red balls and $r_{2}$ indistinguishable blue balls are placed into $n$ cells, find the number of distinguishable arrangements.
%\end{problem}

%\medskip
%\begin{problem} (Feller, II.10.40) If $r_{1}$ dice and $r_{2}$ coins are thrown, how many results can be distinguished?
%\end{problem}

%\medskip
%\begin{problem} (Feller, II.12.1) Prove the following identities for $n\ge 2$. [{\em Convention:} Let $n$ be a positive integer. Then $\binom{n}{y}=0$ if $y$ is not an integer or if $y>n$].
%\begin{align*}
%1-\binom{n}{1}+\binom{n}{2} -\ldots &=0 \\
%\binom{n}{1}+2\binom{n}{2}+3\binom{n}{3}+\ldots &=n2^{n-1} \\
%\binom{n}{1}-2\binom{n}{2}+3\binom{n}{3}-\ldots &=0 \\
%2.1\binom{n}{2}+3.2\binom{n}{3}+4.3\binom{n}{4}+\ldots &=n(n-1)2^{n-2}
%\end{align*}
%\end{problem}

%\medskip
%\begin{problem} (Feller, I.12.10) Prove that 
%$$
%\binom{n}{0}^{2}+\binom{n}{1}^{2}+\ldots +\binom{n}{n}^{2}=\binom{2n}{n}.
%$$
%\end{problem}

%\medskip
%\begin{problem} (Feller, I.12.20) Using Stirling's formula, prove that $\frac{1}{2^{2n}}\binom{2n}{n}\sim \frac{1}{\sqrt{\pi n}}$. [{\em Convention:} $a_{n}\sim b_{n}$ is shorthand for $\lim\limits_{n\rightarrow \infty}\frac{a_{n}}{b_{n}}=1$].
%\end{problem}


%\newpage
\section{Basic rules of probability}
So far we have defined the notion of probability space and probability of an event. But most often, we do not calculate probabilities from the definition. This is like in integration, where one defined the integral of a function as a limit of Riemann sums, but that definition is used only to find integrals of $x^{n}$, $\sin(x)$ and a few such functions. Instead, integrals of complicated expressions such as $x\sin(x)+2\cos^{2}(x)\tan(x)$ are calculated by various rules, such as substitution rule, integration by parts etc. In probability we need some similar rules relating probabilities of various combinations of events to the individual probabilities.


\begin{proposition}\label{prop:basicrules} Let $(\Omega,p_{\cdot})$ be a discrete probability space. 
\begin{enumerate}\setlength\itemsep{6pt}
\item For any event $A$, we have $0\le \mathbf{P}(A)\le 1$. Also, $\mathbf{P}(\emptyset)=0$ and $\mathbf{P}(\Omega)=1$. 
\item {\em Finite additivity of probability:} If $A_{1},\ldots ,A_{n}$ are pairwise disjoint events, then $\mathbf{P}(A_{1}\cup \ldots \cup A_{n})=\mathbf{P}(A_{1})+\ldots +\mathbf{P}(A_{n})$.
In particular, $\mathbf{P}(A^{c})=1-\mathbf{P}(A)$ for any event $A$.
\item {\em Countable additivity of probability:} If $A_{1},A_{2},\ldots$ is a countable collection of pairwise disjoint events, then $\mathbf{P}(\cup A_{i})=\sum_{i}\mathbf{P}(A_{i})$.
\end{enumerate}
\end{proposition}
All of these may seem obvious, and indeed they would be totally obvious if we stuck to finite sample spaces. But the sample space could be countable, and then probability of events may involve infinite sums which need special care in manipulation. Therefore we must give a proof. In writing a proof, and in many future contexts, it is useful to introduce the following notation.

\para{Notation} Let $A\subseteq \Omega$ be an event. Then, we define a function ${\mathbf 1}_{A}:\Omega\rightarrow \mathbb{R}$, called the {\em indicator function of $A$},  as follows.
$$
{\mathbf 1}_{A}(\omega) = \begin{cases}
1 & \mbox{ if }\omega\in A,\\
0 & \mbox{ if }\omega\not\in A.
\end{cases}
$$ 
Since a function from $\Omega$ to $\mathbb{R}$ is called a random variable, the indicator of any event is a random variable. All information about the event $A$ is in its indicator function (meaning, if we know the value of ${\mathbf 1}_{A}(\omega)$, we know whether or not $\omega$ belongs to $A$). For example, we can write $\mathbf{P}(A)=\sum_{\omega\in \Omega}{\mathbf 1}_{A}(\omega)p_{\omega}$.

Now we prove the proposition.
\begin{proof} 
\begin{enumerate}\setlength\itemsep{6pt}
\item By definition of probability space $\mathbf{P}(\Omega)=1$ and $\mathbf{P}(\emptyset)=0$. If $A$ is any event, then ${\mathbf 1}_{\emptyset}(\omega)p_{\omega}\le {\mathbf 1}_{A}(\omega)p_{\omega}\le {\mathbf 1}_{\Omega}(\omega)p_{\omega}$. By Exercise~\ref{ex:linearitymonotonicityofsums}, we get
$$
\sum_{\omega\in \Omega}{\mathbf 1}_{\emptyset}(\omega)p_{\omega} \le \sum_{\omega\in \Omega}{\mathbf 1}_{A}(\omega)p_{\omega} \le \sum_{\omega\in \Omega}{\mathbf 1}_{\Omega}(\omega)p_{\omega}.
$$
As observed earlier, these sums are just $\mathbf{P}(\emptyset)$, $\mathbf{P}(A)$ and $\mathbf{P}(\Omega)$, respectively. Thus, $0\le \mathbf{P}(A)\le 1$.
\item It suffices to prove it for two sets (why?). Let $A,B$ be two events such that $A\cap B=\emptyset$. Let $f(\omega)=p_{\omega}{\mathbf 1}_{A}(\omega)$ and $g(\omega)=p_{\omega}{\mathbf 1}_{B}(\omega)$ and $h(\omega)=p_{\omega}{\mathbf 1}_{A\cup B}(\omega)$. Then, the disjointness of $A$ and $B$ implies that $f(\omega)+g(\omega)=h(\omega)$ for all $\omega\in \Omega$. Thus, by Exercise~\ref{ex:linearitymonotonicityofsums}, we get 
$$
\sum_{\omega\in \Omega}f(\omega)+\sum_{\omega\in \Omega}g(\omega) = \sum_{\omega\in \Omega}h(\omega).
$$
But the three sums here are precisely $\mathbf{P}(A)$, $\mathbf{P}(B)$ and $\mathbf{P}(A\cup B)$. Thus, we get $\mathbf{P}(A\cup B)=\mathbf{P}(A)+\mathbf{P}(B)$.
\item This is similar to finite additivity but needs a more involved argument. We leave it as an exercise for the interested reader. \qedhere
\end{enumerate}
\end{proof}
\begin{exercise} Adapt the proof to prove that for a countable family of events $A_{k}$ in a common probability space (no disjointness assumed), we have
$$
\mathbf{P}(\cup_{k}A_{k})\le \sum_{k}\mathbf{P}(A_{k}).
$$
\end{exercise}

%

%
%The first two are obvious but in writing an honest proof, one should understand that $\mathbf{P}(A)$ is defined by a possibly infinite sum, and the only meaning is as discussed in the previous section. For example, if $A,B$ are finite and disjoint, then the rule $\mathbf{P}(A\cup B)=\mathbf{P}(A)+\mathbf{P}(B)$ is a direct consequence of commutativity of addition. To show the same for infinite $A,B$, one should take finite subsets of them and use the definition of infinite sums. Once one understands this issue, the third property is also not hard to prove, but we write it out anyway.
%\begin{proof}[Countable additivity property] Let $A_{1},A_{2},\ldots$ is a countable collection of pairwise disjoint events in the discrete probability space $(\Omega,p_{\cdot})$. Let $A=A_{1}\cup A_{2}\cup \ldots$. Then, if $B\subseteq A$ is a finite subset, define $B_{k}=B\cap A_{k}$. Then, 
% (a) each $B_{k}$ is a finite subset, \; and \; (b) there is some $N$ such that $B_{k}$ is empty for every $k>N$. Thus, $B=B_{1}\cup \ldots \cup B_{N}$ and hence,
%\begin{align*}
%\mathbf{P}(B)=\mathbf{P}(B_{1})+\ldots +\mathbf{P}(B_{N})\le \mathbf{P}(A_{1})+\ldots +\mathbf{P}(A_{N})\le \sum_{k}\mathbf{P}(A_{k}).
%\end{align*}
%Since this is true for every finite subset of $A$, the definition of $\mathbf{P}(A)$ shows that $\mathbf{P}(A)\le \sum_{k}\mathbf{P}(A_{k})$. Observe that we did not use the disjointness of the sets $A_{k}$ so far.

%To get a bound the other way, fix any $\epsilon>0$. By definition of  $\sum_{\omega\in A_{k}}p_{\omega}$ as supremum over finite subsets, we can find finite subsets $B_{k}\subseteq A_{k}$ such that $\mathbf{P}(A_{k})\le \mathbf{P}(B_{k})+\epsilon^{k+1}$. Clearly, $B_{k}$ are pairwise disjoint. Further, by definition of the infinite sum $\sum_{k}\mathbf{P}(A_{k})$, there is a finite $N$ such that $\sum_{k}\mathbf{P}(A_{k})\le \epsilon+\sum_{k=1}^{N}\mathbf{P}(A_{k})$. Therefore, if we set $B=B_{1}\cup \ldots \cup B_{N}$, then 
%\begin{align*}
%\mathbf{P}(B)=\mathbf{P}(B_{1})+\ldots +\mathbf{P}(B_{N})\ge \sum_{k=1}^{N}\mathbf{P}(A_{k}) -\sum_{k=1}^{N}\epsilon^{k+1}\ge \sum_{k}\mathbf{P}(A_{k}) -\sum_{k=1}^{N+1}\epsilon^{k}
%\end{align*}
%If $\epsilon<1/2$, this implies that $\mathbf{P}(B)\ge  \sum_{k}\mathbf{P}(A_{k})-2\epsilon$. Thus, for every $\epsilon>0$, we can find a finite subset of $A$ having probability more than $\sum_{k}\mathbf{P}(A_{k})-2\epsilon$ which shows that $\mathbf{P}(A)\ge \sum_{k}\mathbf{P}(A_{k})$.
%\end{proof}
\begin{definition}[Limsup and liminf of sets] If $A_{k}$, $k\ge 1$, is a sequence of subsets of $\Omega$, we define 
\begin{equation*}
\limsup A_{k}=\bigcap_{N=1}^{\infty}\bigcup_{k=N}^{\infty}A_{k}, \qquad \mbox{ and } \qquad  \liminf A_{k}=\bigcup_{N=1}^{\infty}\bigcap_{k=N}^{\infty}A_{k}.
\end{equation*}
In words, $\limsup A_{k}$ is the set of all $\omega$ that belong to infinitely many of the $A_{k}$s, and $\liminf A_{k}$ is the set of all $\omega$ that belong to all but finitely many of the $A_{k}$s. 

Two special cases are of increasing and decreasing sequences of events. This means $A_{1}\subseteq A_{2}\subseteq A_{3}\subseteq \ldots$ and $A_{1}\supseteq A_{2}\supseteq A_{3}\supseteq \ldots$. In these cases, the limsup and liminf are the same (so we refer to it as the limit of the sequence of sets). It is $\cup_{k}A_{k}$ in the case of increasing events and $\cap_{k}A_{k}$ in the case of decreasing events.
\end{definition}

\begin{exercise} Events below are all contained in a discrete probability space. Use countable additivity of probability to show that
\begin{enumerate}\setlength\itemsep{6pt}
\item If $A_{k}$ are increasing events with limit $A$, show that $\mathbf{P}(A)$ is the increasing limit of $\mathbf{P}(A_{k})$. 
\item If $A_{k}$ are decreasing events with limit $A$, show that $\mathbf{P}(A)$ is the decreasing limit of $\mathbf{P}(A_{k})$. 
%\item If either of the above statements is assumed, show that 
\end{enumerate}
\end{exercise}  
Now we re-write the basic rules of probability as follows.

\para{The basic rules of probability}
\begin{enumerate}\setlength\itemsep{6pt}
\item $\mathbf{P}(\emptyset)=0$, $\mathbf{P}(\Omega)=1$ and $0\le \mathbf{P}(A)\le 1$ for any event $A$.
\item $\mathbf{P}\left(\bigcup\limits_{k}A_{k}\right)\le \sum\limits_{k} \mathbf{P}(A_{k})$ for any countable collection of events $A_{k}$.
\item $\mathbf{P}\left(\bigcup\limits_{k}A_{k}\right)=\sum\limits_{k}\mathbf{P}(A_{k})$ if $A_{k}$ is a countable collection of pairwise disjoint events.
\end{enumerate} 




%\newpage
\section{Inclusion-exclusion formula}
In general, there is no simple rule for $\mathbf{P}(A\cup B)$ in terms of $\mathbf{P}(A)$ and $\mathbf{P}(B)$. Indeed, consider the probability space $\Omega=\{0,1\}$ with $p_{0}=p_{1}=\frac{1}{2}$. If $A=\{0\}$ and $B=\{1\}$, then $\mathbf{P}(A)=\mathbf{P}(B)=\frac{1}{2}$ and $\mathbf{P}(A\cup B)=1$. However, if $A=B=\{0\}$, then $\mathbf{P}(A)=\mathbf{P}(B)=\frac{1}{2}$ as before, but $\mathbf{P}(A\cup B)=\frac{1}{2}$. This shows that $\mathbf{P}(A\cup B)$ cannot be determined from $\mathbf{P}(A)$ and $\mathbf{P}(B)$. Similarly for $\mathbf{P}(A\cap B)$ or other set constructions. 

However, it is easy to see that $\mathbf{P}(A\cup B)=\mathbf{P}(A)+\mathbf{P}(B)-\mathbf{P}(A\cap B)$. This formula is not entirely useless, because in special situations we shall later see that the probability of the intersection is easy to compute and hence we may compute the probability of the union. Generalizing this idea to more than two sets, we get the following surprisingly useful formula.
 \begin{proposition}[Inclusion-Exclusion formula]
Let $(\Omega,p)$ be a probability space and let $A_{1},\ldots ,A_{n}$ be events. Then,
$$
\mathbf{P}\left(\bigcup_{i=1}^{n}A_{i}\right) = S_{1}-S_{2}+S_{3}-\ldots +(-1)^{n-1}S_{n}
$$
where
$$ 
S_{k}=\sum\limits_{1\le i_{1}<i_{2}<\ldots <i_{k}\le n}\mathbf{P}(A_{i_{1}}\cap A_{i_{2}}\cap \ldots \cap A_{i_{k}}).
$$
\end{proposition}
We give two proofs, but the difference  is only superficial. It is a good exercise to reason out why the two arguments are basically the same. 
\begin{proof}[First proof] For each $\omega\in \Omega$ we compute its contribution to the two sides. If $\omega\not\in \bigcup_{i=1}^{n}A_{i}$, then $p_{\omega}$ is not counted on either side.  Suppose $\omega\in \bigcup_{i=1}^{n}A_{i}$ so that $p_{\omega}$ is counted once on the left side. We count the number of times $p_{\omega}$ is counted on the right side by splitting into cases depending on the exact number of $A_{i}$s that contain $\omega$.

Suppose $\omega$ belongs to exactly one of the $A_{i}$s. For simplicity let us suppose that $\omega\in A_{1}$ but $\omega\in A_{i}^{c}$ for $2\le i\le n$. Then $p_{\omega}$ is counted once in $S_{1}$ but not counted in $S_{2},\ldots ,S_{n}$.

Suppose $\omega$ belongs to $A_{1}$ and $A_{2}$ but not any other $A_{i}$. Then $p_{\omega}$ is counted twice in $S_{1}$ (once for $\mathbf{P}(A_{1})$ and once for $\mathbf{P}(A_{2})$) and subtracted once in $S_{2}$ (in $\mathbf{P}(A_{1}\cap A_{2})$). Thus, it is effectively counted once on the right side. The same holds if $\omega$ belongs to $A_{i}$ and $A_{j}$ but not any other $A_{k}$s.

If $\omega$ belongs to $A_{1},\ldots ,A_{k}$ but not any other $A_{i}$, then  on the right side, $p_{\omega}$ is added $k$ times in $S_{1}$, subtracted $\binom{k}{2}$ times in $S_{2}$, added $\binom{k}{3}$ times in $S_{k}$ and so on. Thus $p_{\omega}$ is effectively counted
$$
\binom{k}{1}-\binom{k}{2}+\binom{k}{3}-\ldots +(-1)^{k-1}\binom{k}{k}
$$
times. By the Binomial formula, this is just the expansion of  $1-(1-1)^{k}$ which is $1$. 
\end{proof}
% The usefulness of the inclusion-exclusion formula stems from the fact that in very special (but often considered) situations, probabilities of intersections are easy to find. We shall see this when we define independence of events. Here is an example for now.

%\section*{Lecture 5}
%Some recap and going slow over many combinatorial problems. Go over the inclusion-exclusion formula again.
%\begin{proposition}
%Let $(\Omega,p)$ be a probability space and let $A_{1},\ldots ,A_{n}$ be events. Then,
%$$
%\mathbf{P}\left(\bigcup_{i=1}^{n}A_{i}\right) = \sum\limits_{i=1}^{n}\mathbf{P}(A_{i})-\sum_{1\le i<j\le n}\mathbf{P}(A_{i}\cap A_{j}) + \sum_{1\le i<j<k\le n}\mathbf{P}(A_{i}\cap A_{j}\cap A_{k})-\ldots +(-1)^{n-1}\mathbf{P}(A_{1}\cap \ldots \cap A_{n}).
%$$
%\end{proposition}
\begin{proof}[Second proof] Use the definition to write both sides of the statement. Let $A=\cup_{i=1}^{n}A_{i}$.
$$
\mbox{LHS}= \sum\limits_{\omega\in A}p_{\omega} = \sum\limits_{\omega\in \Omega}{\mathbf 1}_{A}(\omega)p_{\omega}.
$$
Now we compute the right side. For any $i_{1}<i_{2}<\ldots <i_{k}$, we write
$$
\mathbf{P}\left(A_{i_{1}}\cap \ldots \cap A_{i_{k}}\right) =\sum\limits_{\omega\in \Omega}p_{\omega}{\mathbf 1}_{A_{i_{1}}\cap \ldots \cap A_{i_{k}}}(\omega)= \sum\limits_{\omega\in \Omega}p_{\omega}\prod\limits_{\ell=1}^{k}{\mathbf 1}_{A_{i_{\ell}}}(\omega).
$$
Hence, the right hand side is given by adding over $i_{1}<\ldots <i_{k}$, multiplying by $(-1)^{k-1}$ and then summing over $k$ from $1$ to $n$.
\bes
\mbox{RHS} &=& \sum\limits_{k=1}^{n}(-1)^{k-1}\sum\limits_{1\le i_{1}<\ldots <i_{k}\le n} \; \sum\limits_{\omega\in \Omega} \;p_{\omega}\prod\limits_{\ell=1}^{k}{\mathbf 1}_{A_{i_{\ell}}}(\omega) \\
&=& \sum\limits_{\omega \in \Omega}\sum\limits_{k=1}^{n}(-1)^{k-1}\sum\limits_{1\le i_{1}<\ldots <i_{k}\le n} p_{\omega}\prod\limits_{\ell=1}^{k}{\mathbf 1}_{A_{i_{\ell}}}(\omega) \\
&=& -\sum\limits_{\omega \in \Omega}p_{\omega}\sum\limits_{k=1}^{n}\sum\limits_{1\le i_{1}<\ldots <i_{k}\le n}\prod\limits_{\ell=1}^{k}(-{\mathbf 1}_{A_{i_{\ell}}}(\omega)) \\
&=& -\sum\limits_{\omega \in \Omega}p_{\omega} \left(\prod\limits_{j=1}^{n}(1-{\mathbf 1}_{A_{j}}(\omega)) \; - \; 1\right) \\
&=& \sum\limits_{\omega \in \Omega}p_{\omega}{\mathbf 1}_{A}(\omega).
\ees
because the quantity $\prod\limits_{j=1}^{n}(1-{\mathbf 1}_{A_{j}}(\omega))$ equals $-1$ if $\omega$ belongs to at least one of the $A_{i}$s, and is zero otherwise. Thus the claim follows.
\end{proof}
%\begin{remark} The way to think about the inclusion-exclusion is as follows (this is what the proof does, but we elaborate a little here). Count how many times each $p_{\omega}$ is counted on both sides of the equation.
%\begin{enumerate}\setlength\itemsep{6pt}
%\item If $\omega \not\in A$, then it is not counted on either side.
%\item If $\omega\in A$, then it is counted once on the left side. Further, there is a number $r$ such that $1\le r\le n$ such that $\omega$ belongs to exactly $r$ of the sets $A_{i}$.  

%Then $p_{\omega}$ is counted $r$ times in $\sum_{i}P(A_{i})$, subtracted $\binom{r}{2}$ times in $\sum_{i<j}\mathbf{P}(A_{i}\cap A_{j})$, added $\binom{r}{3}$ times in $\sum_{i}\mathbf{P}(A_{i}\cap A_{j}\cap A_{k})$ etc. Hence, the number of times $p_{\omega}$ occurs on the right side is
%$$
%\binom{r}{1}-\binom{r}{2}+\ldots +(-1)^{r-2}\binom{r}{r-1}+(-1)^{r-1}\binom{r}{r}
%$$
%which, by the binomial theorem is precisely the expansion of $1-(1-1)^{r}$ which is equal to $1$. 
%\end{enumerate}
%Thus the contribution of $p_{\omega}$ to both sides are the same.
%\end{remark}

As we remarked earlier,  it turns out that in many settings it is possible to compute the probabilities of intersections. We give an example now. 
\begin{example} \label{eg:matchingnumerisnearlypoisson} Let $\Omega=S_{52}\times S_{52}$ with $p_{\omega}=\frac{1}{(52!)^{2}}$ for all $\omega \in \Omega$. Consider the event $A=\{(\pi,{\sigma}){\; : \;} \pi(i)\not={\sigma}(i) \ \forall i\}$. Informally, we imagine two shuffled decks of cards kept side by side (or perhaps one shuffled deck and another permutation denoting a  ``psychic's predictions'' for the order in which the cards occur). Then $A$ is the event that there are no matches (or correct guesses).

Let $A_{i}=\{(\pi,{\sigma}){\; : \;} \pi(i)={\sigma}(i)\}$ so that $A^{c}=A_{1}\cup \ldots \cup A_{52}$. It is easy to see that $\mathbf{P}(A_{i_{1}}\cap A_{i_{2}}\ldots \cap A_{i_{k}})=\frac{1}{52(52-1)\ldots (52-k+1)}$ for any $i_{1}<i_{2}<\ldots <i_{k}$ (why?). Therefore, by the inclusion-exclusion formula, we get
\begin{align*}
\mathbf{P}(A^{c}) &=  \binom{52}{1}\frac{1}{52}-\binom{52}{2}\frac{1}{(52)(51)} +  \ldots + (-1)^{51} \binom{52}{52}\frac{1}{(52)(51)\ldots (1)}\\
&= 1-\frac{1}{2!}+\frac{1}{3!}-\frac{1}{4!}+\ldots -\frac{1}{52!} \\
&\approx 1-\frac{1}{e} \approx 0.6321
\end{align*}
by the expansion $e^{-1}=1-\frac{1}{2!}+\frac{1}{3!}-\ldots $. Hence $\mathbf{P}(A)\approx e^{-1}\approx 0.3679$.
\end{example} 
\begin{example}\label{eg:probofemptyurn2} Place $n$ distinguishable balls in $r$ distinguishable urns at random. Let $A$ be the event that some urn is empty. The probability space is $\Omega=\{\underline{\ome}=(\omega_{1},\ldots,\omega_{n}){\; : \;} 1\le \omega_{i}\le r\}$ with $p_{\underline{\ome}}=r^{-n}$. Let $A_{\ell}=\{\underline{\ome}{\; : \;} \omega_{i}\not=\ell\}$ for $\ell=1,2\ldots ,r$. Then, $A=A_{1}\cup \ldots \cup A_{r-1}$ (as $A_{r}$ is empty, we could include it or not, makes no difference).

It is easy to see that $\mathbf{P}(A_{i_{1}}\cap \ldots \cap A_{i_{k}})=(r-k)^{n}r^{-n}=(1-\frac{k}{r})^{n}$. We could use the inclusion-exclusion formula to write the expression
$$
\mathbf{P}(A)=r\left(1-\frac{1}{r}\right)^{n}-\binom{r}{2}\left(1-\frac{2}{r}\right)^{n}+\ldots +(-1)^{r-2}\binom{r}{r-1}\left(1-\frac{r-1}{r}\right)^{n}.
$$
The last term is zero (since all urns cannot be empty). I don't know if this expression can be simplified any more.
\end{example}

We mention two useful formulas that can be proved on lines similar to the inclusion-exclusion principle. If we say ``at least one of the events $A_{1}, A_{2},\ldots ,A_{n}$ occurs'', we are talking about the union, $A_{1}\cup A_{2}\cup \ldots \cup A_{n}$. What about ``at least $m$ of the events $A_{1}, A_{2},\ldots ,A_{n}$ occur'', how to express it with set operations. It is not hard to see that this set is precisely
$$
B_{m}=\bigcup_{ 1\le i_{1}<i_{2}<\ldots <i_{m}\le n} (A_{i_{1}}\cap A_{i_{2}}\cap \ldots \cap A_{i_{m}}).
$$
The event that ``exactly $m$ of the events $A_{1}, A_{2},\ldots ,A_{n}$ occur'' can be written as
$$
C_{m}=B_{m}\setminus B_{m+1} = \bigcup_{\stackrel{S\subseteq [n]}{|S|=m}} \; \left(\bigcap_{i\in S}A_{i}\right)\bigcap \left( \bigcap_{i\not\in S}A_{i}^{c}\right).
$$
\begin{exercise} Let $A_{1},\ldots ,A_{n}$ be events in a probability space $(\Omega,p)$ and let $m\le n$. Let $B_{m}$ and $C_{m}$ be as above. Show that 
\begin{align*}
\mathbf{P}(B_{m}) &= \sum_{k=m}^{n}(-1)^{k-m}\binom{k-1}{k-m}S_{k} \\
&= S_{m}-\binom{m}{1}S_{m+1}+\binom{m+1}{2}S_{m+2}-\binom{m+2}{3}S_{m+3}+\ldots  \\
\mathbf{P}(C_{m}) &= \sum_{k=m}^{n}(-1)^{k-m}\binom{k}{m}S_{k} \\
&= S_{m}-\binom{m+1}{1}S_{m+1}+\binom{m+2}{2}S_{m+2}-\binom{m+3}{3}S_{m+3}+\ldots  \\
\end{align*}
\end{exercise}

\begin{exercise} Return to the setting of exercise~\ref{eg:matchingnumerisnearlypoisson} but with $n$ cards in a deck, so that $\Omega=S_{n}\times S_{n}$ and $p_{(\pi,{\sigma})}=\frac{1}{(n!)^{2}}$. Let $A_{m}$ be the event that there are exactly $m$ matches between the two decks. 
\begin{enumerate}\setlength\itemsep{6pt}
\item For fixed $m\ge 0$, show that $\mathbf{P}(A_{m})\rightarrow e^{-1}\frac{1}{m!}$ as $n\rightarrow \infty$.
\item Assume that the approximations above are valid for $n=52$ and $m\le 10$. Find the probability that there are at least $10$ matches.
\end{enumerate}

\end{exercise}



%%%\newpage
\section{Bonferroni's inequalities}
Inclusion-exclusion formula is nice when we can calculate the probabilities of intersections of the events under consideration. Things are not always this nice, and sometimes that may be very difficult. Even if we could find them, summing them with signs according to the inclusion-exclusion formula may be difficult as the example~\ref{eg:probofemptyurn2} demonstrates. The {\em idea} behind the inclusion-exclusion formula can however be often used to compute {\em approximate values of probabilities}, which is very valuable in most applications. That is what we do next.


We know that $\mathbf{P}(A_{1}\cup \ldots \cup A_{n})\le \mathbf{P}(A_{1})+\ldots +\mathbf{P}(A_{n})$ for any events $A_{1},\ldots ,A_{n}$. This is an extremely useful inequality, often called the {\em union bound}. Its usefulness is in the fact that there is no assumption made about the events $A_{i}$s (such as whether they are disjoint or not). The following inequalities generalize the union bound, and gives both upper and lower bounds for the probability of the union of a bunch of events.

\begin{lemma}[Bonferroni's inequalities]\label{lem:Bonferroni's inequalities} Let $A_{1},\ldots, A_{n}$ be events in a probability space $(\Omega,p)$ and let $A=A_{1}\cup \ldots \cup A_{n}$. We have the following upper and lower bounds for  $\mathbf{P}(A)$.
\begin{align*}
\mathbf{P}(A) &\le \sum_{k=1}^{m}(-1)^{k-1}S_{k}, \hspace{2mm} \mbox{ for any odd }m. \\
\mathbf{P}(A) &\ge \sum_{k=1}^{m}(-1)^{k-1}S_{k}, \hspace{2mm} \mbox{ for any even }m. 
\end{align*}
\end{lemma}
\begin{proof} We shall write out the proof for the cases $m=1$ and $m=2$. When $m=1$, the inequality is just the union bound
$$
 \mathbf{P}(A)\le \mathbf{P}(A_{1})+\ldots +\mathbf{P}(A_{n})
$$
which we know. When $m=2$, the inequality to be proved is
$$
\mathbf{P}(A)\ge \sum_{k}\mathbf{P}(A_{k})-\sum_{k<\ell}\mathbf{P}(A_{k}\cap A_{\ell})
$$
To see this, fix $\omega\in \Omega$ and count the contribution of $p_{\omega}$ to both sides. Like in the proof of the inclusion-exclusion formula, for $\omega \not\in A_{1}\cup\ldots \cup A_{n}$, the contribution to both sides is zero. On the other hand, if $\omega$ belongs to exactly $r$ of the sets for some $r\ge 1$, then it is counted once on the left side and $r-\binom{r}{2}$ times on the right side. Not that $r-\binom{r}{2} = \frac{1}{2}r(3-r)$ which is always non-positive (one if $r=1$, zero if $r=2$ and non-positive if $r\ge 3$). Hence we get $\mbox{LHS}\ge \mbox{RHS}$.

Similarly, one can prove the other inequalities in the series. We leave it as an exercise. The key point is that $r-\binom{r}{2}+\ldots +(-1)^{k-1}\binom{r}{k}$ is non-negative if $k$ is odd and non-positive if $k$ is even (prove this). Here as always $\binom{x}{y}$ is interpreted as zero if $y>x$.
\end{proof}
Here is an application of these inequalities. 
\begin{example} Return to Example~\ref{eg:probofemptyurn2}. We obtained an exact expression for the answer, but that is rather complicated. For example, what is the probability of having at least one empty urn when $n=40$ balls are placed at random in $r=10$ urns? It would be complicated to sum the series. Instead, we could use Bonferroni's inequalities to get the following bounds.
$$ 
 r\left(1-\frac{1}{r}\right)^{n}-\binom{r}{2}\left(1-\frac{2}{r}\right)^{n}\le \mathbf{P}(A) \le r\left(1-\frac{1}{r}\right)^{n}.
$$
If we take $n=40$ and $r=10$, the bounds we get are $0.1418\le \mathbf{P}(A)\le 0.1478$. Thus, we get a pretty decent approximation to the probability. By experimenting with other numbers you can check that the approximations are good when $n$ is large compared to $r$ but not otherwise. Can you reason why?
\end{example}




\section{Independence - a first look}
We remarked in the context of inclusion-exclusion formulas that often the probabilities of intersections of events is easy to find, and then we can use them to find probabilities of unions etc. In many contexts, this is related to one of the most important notions in probability.

\begin{definition} Let $A,B$ be events in a common probability space. We say that $A$ and $B$ are {\em independent} is $\mathbf{P}(A\cap B)=\mathbf{P}(A)\mathbf{P}(B)$.
\end{definition}
\begin{example} Toss a fair coin $n$ times. Then $\Omega=\{\underline{\ome}{\; : \;} \underline{\ome}=(\omega_{1},\ldots ,\omega_{n}), \; \omega_{i}\mbox{ is }0\mbox{ or }1\}$ and $p_{\underline{\ome}}=2^{-n}$ for each $\underline{\ome}$. Let $A=\{\underline{\ome}{\; : \;} \omega_{1}=0\}$ and let $B=\{\underline{\ome}{\; : \;} \omega_{2}=0\}$. Then, from the definition of probabilities, we can see that $\mathbf{P}(A)=1/2$, $\mathbf{P}(B)=1/2$ (because the elementary probabilities are equal, and both the sets $A$ and $B$ contain exactly $2^{n-1}$ elements). Further, $A\cap B=\{\underline{\ome}{\; : \;} \omega_{1}=1, \omega_{2}=0\}$ has $2^{n-2}$ elements, whence $\mathbf{P}(A\cap B)=1/4$. Thus, $\mathbf{P}(A\cap B)=\mathbf{P}(A)\mathbf{P}(B)$ and hence $A$ and $B$ are independent. 
\end{example}
If two events are independent, then the probability of their intersection can be found from the individual probabilities. How do we check if two events are independent? By checking if the probability of the event is equal to the product of the individual probabilities! It seems totally circular and useless! There are many reasons why it is not an empty notion as we shall see. 

Firstly, in physical situationsdependence is related to a basic intuition we have about whether two events are related or not. For example, suppose you are thinking of betting Rs.1000 on a particular horse in a race. If you get the news that your cousin is getting married, it will perhaps not affect the amount you plan to bet. However, if you get the news that one of the other horses has been injected with undetectable drugs, it might affect the bet you want to place. In other words, certain events (like marriage of a cousin) have no bearing on the probability of the event of interest (the event that our horse wins) while other events (like the injection of drugs) do have an impact. This intuition is often put into the very definition of probability space that we have. 

For example, in the above example of tossing a fair coin $n$ times, it is our intuition that a coin does not remember how it fell previous times, and that chance of its falling head in any toss is just $1/2$, irrespective of how many heads or tails occured before\footnote{It may be better to attribute this to experience rather than intuition. There have been reasonable people in history who believed that if a coin shows heads in ten tosses in a row, then on the next toss it is more likely to show tails (to `compensate' for the overabundance of heads)! Clearly this is also someone's intuition, and different from ours. Only experiment can decide which is correct, and any number of experiments with real coins show that our intuition is correct, and coins have no memory.} And this intuition was used in defining the elementary probabilities as $2^{-n}$ each. Since we started with the intuitive notion of independence, and put that into the definition of the probability space, it is quite expected that the event that the first toss is a head should be independent of the event that the second toss is a tail. That is the calculation shown in above.

But how is independence useful mathematically if the conditions to check independence are the very conclusions we want?! The answer to this lies in the following fact (to be explained later). When certain events are independent, then many other collections of events that can be made out of them also turn out to be independent. For example, if $A,B,C,D$ are independent (we have not yet defined what this means!), then $A\cup B$ and $C\cup D$ are also independent. Thus, starting from independence of certain events, we get independence of many other events. For example, any event depending on the first four tosses is independent of eny event depending on the next five tosses.


\section{Conditional probability and independence}
\begin{definition} Let $A,B$ be two events in the same probability space. 
\begin{enumerate}\setlength\itemsep{6pt}
\item If $\mathbf{P}(B)\not=0$, we define the {\em conditional probability of $A$ given $B$} as $$\mathbf{P}\left(A\left.\vphantom{\hbox{\Large (}}\right| B\right):=\frac{\mathbf{P}(A\cap B)}{\mathbf{P}(B)}.$$
\item We say that $A$ and $B$ are {\em independent} if $\mathbf{P}(A\cap B)=\mathbf{P}(A)\mathbf{P}(B)$. If $\mathbf{P}(B)\not=0$, then $A$ and $B$ are independent if and only if $\mathbf{P}(A\left.\vphantom{\hbox{\large (}}\right| B)=\mathbf{P}(A)$ (and similarly with the roles of $A$ and $B$ reversed). If $\mathbf{P}(B)=0$, then $A$ and $B$ are necessarily independent since $\mathbf{P}(A\cap B)$ must also be $0$.
\end{enumerate}
\end{definition}
What do these notions mean intuitively? In real life, we keep updating probabilities based on information that we get. For example, when playing cards, the chance that a randomly chosen card is an ace is $1/13$, but having drawn a card, the probability for the next card may not be the same - if the first card was seen to be an ace, then the chance of the second being an ace falls to $3/51$. This updated probability is called a conditional probability. Independence of two events $A$ and $B$ means that knowing whether or not $A$ occured does not change the chance of occurrence of $B$. In other words, the conditional probability of $A$ given $B$ is the same as the unconditional (original) probability of $A$.

\begin{example} Let $\Omega=\{(i,j){\; : \;} 1\le i,j\le 6\}$ with $p_{(i,j)}=\frac{1}{36}$. This is the probability space corresponding to a throw of two fair dice. Let $A=\{(i,j){\; : \;} i\mbox{ is odd}\}$ and $B=\{(i,j){\; : \;} j \mbox{ is }1\mbox{ or }6\}$ and $C=\{(i,j){\; : \;} i+j=4\}$. Then $A\cap B=\{(i,j){\; : \;} i=1,3,\mbox{ or }5, \mbox{ and }j=1\mbox{ or }6\}$. Then, it is easy to see that
$$
\mathbf{P}(A\cap B)=\frac{6}{36}=\frac{1}{6}, \;\; \mathbf{P}(A)=\frac{18}{36}=\frac{1}{2}, \;\; \mathbf{P}(B)=\frac{12}{36}=\frac{1}{3}.
$$
In this case, $\mathbf{P}(A\cap B)=\mathbf{P}(A)\mathbf{P}(B)$ and hence $A$ and $B$ are independent. On the other hand, 
$$
\mathbf{P}(A\cap C)=\mathbf{P}\{(1,3),(2,2)\}=\frac{1}{18}, \;\; \mathbf{P}(C)=\mathbf{P}\{(1,3),(2,2),(3,1)\}=\frac{1}{12}.
$$
Thus, $\mathbf{P}(A\cap C)\not=\mathbf{P}(A)\mathbf{P}(C)$ and hence $A$ and $C$ are not independent. 

This agrees with the intuitive understanding of independence, since $A$ is an event that depends only on the first toss and $B$ is an event that depends only on the second toss. Therefore, $A$ and $B$ ought to be independent. However, $C$ depends on both tosses, and hence cannot be expected to be independent of $A$. Indeed, it is easy to see that $\mathbf{P}(C \left.\vphantom{\hbox{\large (}}\right| A)=\frac{1}{9}$. 
\end{example}

\begin{example} Let $\Omega=S_{52}$ with $p_{\pi}=\frac{1}{52!}$. Define the events
$$
A=\{\pi{\; : \;} \pi_{1}\in \{10,20,30,40\}\}, \qquad A=\{\pi{\; : \;} \pi_{2}\in \{10,20,30,40\}\}.
$$
Then both $\mathbf{P}(A)=\mathbf{P}(B)=\frac{1}{13}$. However, $\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A)=\frac{3}{51}$. One can also see that $\mathbf{P}(B \left.\vphantom{\hbox{\large (}}\right| A^{c})=\frac{4}{51}$.

In words, $A$ (respectively $B$) could be the event that the first (respectively second) card is an ace. Then $\mathbf{P}(B)=4/52$ to start with. When we see the first card, we update the probability. If the first card was not an ace, we update it to $\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A^{c})$ and if the first card was an ace, we update it to  $\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A)$.
\end{example}


\para{Caution} Independence should not be confused with disjointness! If $A$ and $B$ are disjoint, $\mathbf{P}(A\cap B)=0$ and hence $A$ and $B$ can be independent if and only if one of $\mathbf{P}(A)$ or $\mathbf{P}(B)$ equals $0$. Intuitively, if $A$ and $B$ are disjoint, then knowing that $A$ occurred gives us a lot of information about $B$ (that it did not occur!), so independence is not to be expected. 

\begin{exercise} If $A$ and $B$ are independent, show that the following pairs of events are also independent.
\begin{enumerate}\setlength\itemsep{6pt}
\item $A$ and $B^{c}$.
\item $A^{c}$ and $B$.
\item $A^{c}$ and $B^{c}$.
\end{enumerate}
\end{exercise}


\para{Total probability rule and Bayes' rule} Let $A_{1},\ldots ,A_{n}$ be pairwise disjoint and mutually exhaustive events in a probability space. Assume $\mathbf{P}(A_{i})>0$ for all $i$. This means that $A_{i}\cap A_{j}=\emptyset$ for any $i\not= j$ and $A_{1}\cup A_{2}\cup \ldots \cup A_{n}=\Omega$. We also refer to such a collection of events as a partition of the sample space.

 Let $B$ be any other event.
\begin{enumerate}\setlength\itemsep{6pt}
\item (Total probability rule). $\mathbf{P}(B)=\mathbf{P}(A_{1})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{1})+\ldots +\mathbf{P}(A_{n})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{n})$.
\item (Bayes' rule).  Assume that $\mathbf{P}(B)>0$. Then, for each $k=1,2,\ldots ,n$, we have
$$\mathbf{P}(A_{k}\left.\vphantom{\hbox{\large (}}\right| B)=\frac{\mathbf{P}(A_{k})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{k})}{\mathbf{P}(A_{1})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{1})+\ldots +\mathbf{P}(A_{n})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{n})}.$$
\end{enumerate}
\begin{proof} The proof is merely by following the definition. 
\begin{enumerate}\setlength\itemsep{6pt}
\item The right hand side is equal to
$$
\mathbf{P}(A_{1})\frac{\mathbf{P}(B\cap A_{1})}{\mathbf{P}(A_{1})}+\ldots +\mathbf{P}(A_{n})\frac{\mathbf{P}(B\cap A_{n})}{\mathbf{P}(A_{n})}=\mathbf{P}(B\cap A_{1})+\ldots +\mathbf{P}(B\cap A_{n})
$$
which is equal to $\mathbf{P}(B)$ since $A_{i}$ are pairwise disjoint and exhaustive.
\item Without loss of generality take $k=1$. Note that $\mathbf{P}(A_{1}\cap B)=\mathbf{P}(A_{1})\mathbf{P}(B\cap A_{1})$. Hence
\begin{align*}
\mathbf{P}(A_{1}\left.\vphantom{\hbox{\large (}}\right| B) &= \frac{\mathbf{P}(A_{1}\cap B)}{\mathbf{P}(B)} \\
 &= \frac{\mathbf{P}(A_{1})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{1})}{\mathbf{P}(A_{1})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{1})+\ldots +\mathbf{P}(A_{n})\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A_{n})}
\end{align*}
where we used the total probability rule to get the denominator. \qedhere
\end{enumerate}
\end{proof}
\begin{exercise} Suppose $A_{i}$ are events such that $\mathbf{P}(A_{1}\cap \ldots \cap A_{n})>0$. Then show that $$\mathbf{P}(A_{1}\cap \ldots \cap A_{n})=\mathbf{P}(A_{1})\mathbf{P}(A_{2}\left.\vphantom{\hbox{\large (}}\right| A_{1})\mathbf{P}(A_{3}\left.\vphantom{\hbox{\large (}}\right| A_{1}\cap A_{2})\ldots \mathbf{P}(A_{n}\left.\vphantom{\hbox{\large (}}\right| A_{1}\cap \ldots \cap A_{n-1}).$$
\end{exercise}

\begin{example} Consider a rare disease $X$ that affects one in a million people. A medical test is used to test for the presence of the disease. The test is 99\% accurate in the sense that if a person has no disease, the chance that the test shows positive is 1\% and if the person has disease, the chance that the test shows negative is also 1\%. 

Suppose a person is tested for the disease and the test result is positive. What is the chance that the person has the disease $X$?

Let $A$ be the event that the person has the disease $X$. Let $B$ be the event that the test shows positive. The given data may be summarized as follows.
\begin{enumerate}\setlength\itemsep{6pt}
\item $\mathbf{P}(A)=10^{-6}$. Of course $\mathbf{P}(A^{c})=1-10^{-6}$.
\item $\mathbf{P}(B \left.\vphantom{\hbox{\large (}}\right| A)=0.99$ and $\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A^{c})=0.01$.
\end{enumerate}
What we want to find is $\mathbf{P}(A\left.\vphantom{\hbox{\large (}}\right| B)$. By Bayes' rule (the relevant partition is $A_{1}=A$ and $A_{2}=A^{c}$), 
$$
\mathbf{P}(A\left.\vphantom{\hbox{\large (}}\right| B) = \frac{\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A)\mathbf{P}(A)}{\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A)\mathbf{P}(A)+\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A^{c})\mathbf{P}(A^{c})} = \frac{0.99 \times 10^{-6}}{0.99\times 10^{-6}+0.01\times (1-10^{-6})} = 0.000099.
$$
The test is quite an accurate one, but the person tested positive has a really low chance of actually having the disease! Of course, one should observe that the chance of having disease is now approximately $10^{-4}$ which is considerably higher than $10^{-6}$. 

A calculation-free understanding of this surprising looking phenomenon can be achieved as follows: Let everyone in the population undergo the test. If there are $10^{9}$ people in the population, then there are only $10^{3}$ people with the disease. The number of true positives is approximately $10^{3}\times 0.99\approx 10^{3}$ while the number of false positives is $(10^{9}-10^{3})\times 0.01\approx 10^{7}$. In other words, among all positives, the false positives are way more numerous than true positives.
\end{example}
The surprise here comes from not taking into account the relative sizes of the sub-populations with and without the disease. Here is another manifestation of exactly the same fallacious reasoning.

\para{Question} A person $X$ is introverted,  very systematic in thinking and somewhat absent-minded. You are told that he is a doctor or a mathematician. What would be your guess - doctor or mathematician?

As we saw in class, most people answer ``mathematician''. Even accepting the stereotype that a mathematician is more likely to have all these qualities than a doctor, this answer ignores the fact that there are perhaps a hundred times more doctors in the world than mathematicians! In fact, the situation is identical to the one in the example above, and the mistake is in confusing $\mathbf{P}(A\big| B)$ and $\mathbf{P}(B\big| A)$.


\section{Independence of three or more events} 
\begin{definition} Events $A_{1},\ldots ,A_{n}$ in a common probability space are said to be independent if
$
\mathbf{P}\left(A_{i_{1}}\cap A_{i_{2}}\cap \ldots \cap A_{i_{m}}\right)=\mathbf{P}(A_{i_{1}})\mathbf{P}(A_{i_{2}})\ldots \mathbf{P}(A_{i_{m}})$ for every choice of $m\le n$ and every choice of $1\le i_{1}<i_{2}<\ldots <i_{m}\le n$.
\end{definition}
 The independence of $n$ events requires us to check $2^{n}$ equations (that many choices of $i_{1},i_{2},\ldots$). Should it not suffice to check that each pair of $A_{i}$ and $A_{j}$ are independent? The following example shows that this is not the case!
\begin{example} Let $\Omega=\{0,1\}^{n}$ with $p_{\underline{\ome}}=2^{-n}$ for each $\underline{\ome}\in \Omega$. Define the events $A=\{\underline{\ome}{\; : \;} \omega_{1}=0\}$, $A=\{\underline{\ome}{\; : \;} \omega_{2}=0\}$ and $C=\{\underline{\ome}{\; : \;} \omega_{1}+\omega_{2}=0 \mbox{ or }2\}$. In words, we toss a fair coin $n$ times and $A$ denotes the event that the first toss is a tail, $B$ denotes the event that the second toss is a tail and $C$ denotes the event that out of the first two tosses are both heads or both tails. Then $\mathbf{P}(A)=\mathbf{P}(B)=\mathbf{P}(C)=\frac{1}{4}$. Further, 
\[\begin{aligned}
\mathbf{P}(A\cap B)=\frac{1}{4}, \; \mathbf{P}(B\cap C)=\frac{1}{4}, \; P(A\cap C)=\frac{1}{4}, \; \mathbf{P}(A\cap B\cap C)=\frac{1}{4}.
\end{aligned}\]
Thus,  $A,B,C$ are independent {\em pairwise}, but not independent by our definition because $\mathbf{P}(A\cap B\cap C)\not= \frac{1}{8}=\mathbf{P}(A)\mathbf{P}(B)\mathbf{P}(C)$.

Intuitively this is right. Knowing $A$ does not given any information about $C$ (similarly with $A$ and $B$ or $B$ and $C$), but knowing $A$ and $B$ tells us completely whether or not $C$ occurred! Thus is is right that the definition should not declare them to be independent.
\end{example}

\begin{exercise} Let $A_{1},\ldots ,A_{n}$ be events in a common probability space. Then, $A_{1}, A_{2},\ldots ,A_{n}$ are independent if and only if the following equalities hold: For each $i$, define $B_{i}$ as $A_{i}$ and $A_{i}^{c}$. Then
\[\begin{aligned}
\mathbf{P}(B_{1}\cap B_{2}\cap \ldots \cap B_{n})=\mathbf{P}(B_{1})\mathbf{P}(B_{2})\ldots \mathbf{P}(B_{n}).
\end{aligned}\]
\end{exercise}
\para{Note} This should hold for any possible choice of $B_{i}$s. In other words, the system of $2^{n}$ equalities in the definition of independence may be replaced by this new set of $2^{n}$ equalities. The latter system has the advantage that it immediately tells us that if $A_{1},\ldots ,A_{n}$ are independent, then $A_{1},A_{2}^{c}, A_{3},\ldots $ (for each $i$ choose $A_{i}$ or its complement) are independent.

\section{Subtleties of conditional probability}
Conditional probabilities are quite subtle. Apart from the common mistake of confusing $\mathbf{P}(A\left.\vphantom{\hbox{\large (}}\right| B)$ for $\mathbf{P}(B\left.\vphantom{\hbox{\large (}}\right| A)$, there are other points one sometimes overlooks. In fact, most of the paradoxical sounding puzzles in probability are based on confusing aspects of probability. Let us see one.

\begin{question} A man says ``I have two children, and one of them is a boy''. What is the chance that the other one is a girl?
\end{question}
There are four possibilities $BB, BG,GB,GG$, of which $GG$ has been eliminated. Of the remaining three, two are favourable, hence the chance is $2/3$ that the other child is a girl. This is a possible solution. If you accept this as reasonable, here is another question.


\begin{question} A man says ``I have two children, and one of them is a boy born on a Sunday''. What is the chance that the other one is a girl?
\end{question}
Does the addition of the information about the boy change the probability? One opinion is that it should not. The other is to follow the same solution pattern as before. Write down all the $2\times2\times7\times7$ possibilities: $BBss$ (boy, boy, sunday, sunday), $BBsm$, etc. The given information that one is a boy who was born on Sunday eliminates many possibilities and what remain are $27$ possibilities $BGt*$, $GB*t$, $BBt*$, $BB*t$ where $*$ is any day of the week. Take care to not double count $BBtt$ to see that there are $27$ possibilities. Of these, $14$ are favourable (i.e., the other child is a girl), hence we conclude that the probability is $14/27$.

Is the correct answer $14/27$ as calculated here or is it $2/3$ (since the  information of the day of birth of the boy is irrelevant, why should we change our earlier answer of $2/3$?)?

We leave it as food for thought. If you want a hint, the point is that to compute conditional probabilities, {\em it is not enough to know what the person said, but also what else he could have said}. Not realizing  this point is the main source of confusion in many  popular puzzles in probability.  

\section{Discrete probability distributions}
Let $(\Omega,p)$ be a probability space and $X:\Omega\rightarrow \mathbb{R}$ be a random variable. We define two objects associated to $X$.

\parag{Probability mass function (pmf)}. The range of $X$ is a countable subset of $\mathbb{R}$, denote it by $\mbox{Range}(X)=\{t_{1},t_{2},\ldots\}$. Then, define $f_{X}:\mathbb{R}\rightarrow [0,1]$ as the function 
$$
f_{X}(t)=\begin{cases}
\mathbf{P}\{\omega {\; : \;} X(\omega)=t\} & \mbox{ if }t\in \mbox{Range}(X). \\
0 & \mbox{ if }t\not\in \mbox{Range}(X). 
\end{cases}
$$
One obvious property is that $\sum_{t\in \mathbb{R}}f_{X}(t)=1$. Conversely, any non-negative function $f$ that is non-zero on a countable set $S$ and such that $\sum_{t\in \mathbb{R}} f(t)=1$ is a pmf of some random variable.

\parag{Cumulative distribution function (CDF)}. Define $F_{X}:\mathbb{R}\rightarrow [0,1]$ by 
$$F_{X}(t)=\mathbf{P}\{\omega{\; : \;} X(\omega)\le t\}.$$

\begin{example} Let $\Omega=\{(i,j){\; : \;} 1\le i,j\le 6\}$ with $p_{(i,j)}=\frac{1}{36}$ for all $(i,j)\in \Omega$. Let $X:\Omega \rightarrow \mathbb{R}$ be the random variable defined by $X(i,j)=i+j$. Then, $\mbox{Range}(X)=\{2,3,\ldots ,12\}$. The pmf and CDF of $X$ are given by
$$
f_{X}(k) = \begin{cases} 
1/36 & \mbox{ if }k=2. \\
2/36 & \mbox{ if }k=3. \\
3/36 & \mbox{ if }k=4. \\
4/36 & \mbox{ if }k=5. \\
5/36 & \mbox{ if }k=6. \\
6/36 & \mbox{ if }k=7. \\
5/36 & \mbox{ if }k=8. \\
4/36 & \mbox{ if }k=9. \\
3/36 & \mbox{ if }k=10. \\
2/36 & \mbox{ if }k=11. \\
1/36 & \mbox{ if }k=12. \\
\end{cases}
\qquad
F_{X}(t) = \begin{cases} 
0 & \mbox{ if }t<2. \\
1/36 & \mbox{ if }t\in[2,3). \\
3/36 & \mbox{ if }t\in[3,4). \\
6/36 & \mbox{ if }t\in[4,5). \\
10/36 & \mbox{ if }t\in[5,6). \\
15/36 & \mbox{ if }t\in[6,7). \\
21/36 & \mbox{ if }t\in[7,8). \\
26/36 & \mbox{ if }t\in[8,9). \\
30/36 & \mbox{ if }t\in[9,10). \\
33/36 & \mbox{ if }t\in[10,11). \\
35/36 & \mbox{ if }t\in[11,12). \\
1 & \mbox{ if }t\ge 12. \\
\end{cases}
$$
\end{example}
%  \begin{figure}[tb]
%    \centering
%    \includegraphics[scale=0.5]{pdfofbinomial.jpg}
%    \includegraphics[scale=0.25]{cdfbinomial.jpg}
%    \label{fig:pdfandcdfofbinomial}
%    \caption{The probability mass function and the cumulative distribution function of Binomial distribution with parameters $n=10$ and $p=0.3$}
%    \end{figure}
A picture of the pmf and CDF for a Binomial distribution are shown in Figure~\ref{fig:pdfandcdfofbinomial}.


\para{Basic properties of a CDF} The following observations are easy to make.
\begin{enumerate}\setlength\itemsep{6pt}
\item $F$ is an increasing function on $\mathbb{R}$.
\item $\lim\limits_{t\rightarrow +\infty}F(t)=1$ and $\lim\limits_{t\rightarrow -\infty}F(t)=0$.
\item $F$ is right continuous, that is, $\lim\limits_{h\searrow 0}F(t+h)=F(t)$ for all $t\in \mathbb{R}$.
\item $F$ increases only in jumps. This means that if $F$ has no jump discontinuities (an increasing function has no other kind of discontinuity anyway) in an interval $[a,b]$, then $F(a)=F(b)$.
\end{enumerate}
Since $F(t)$ is the probability of a certain event, these statements can be proved using the basic rules of probability that we saw earlier. 

\begin{proof} Let $t<s$. Define two events, $A=\{\omega {\; : \;} X(\omega)\le t\}$ and $B=\{\omega{\; : \;} X(\omega)\le s\}$. Clearly $A\subseteq B$ and hence $F(t)=\mathbf{P}(A)\le \mathbf{P}(B)=F(s)$. This proves the first property. 

To prove the second property, let $A_{n}=\{\omega {\; : \;} X(\omega)\le n\}$ for $n\ge 1$. Then, $A_{n}$ are increasing in $n$ and $\bigcup_{n=1}^{\infty}A_{n}=\Omega$. Hence, $F(n)=\mathbf{P}(A_{n})\rightarrow \mathbf{P}(\Omega)=1$ as $n\rightarrow \infty$. Since $F$ is increasing, it follows that $\lim_{t\rightarrow +\infty}F(t)=1$. Similarly one can prove that $\lim_{t\rightarrow -\infty}F(t)=0$.

Right continuity of $F$ is also proved the same way, by considering the events $B_{n}=\{\omega {\; : \;} X(\omega)\le t+\frac{1}{n}\}$. We omit details. 
\end{proof}
\begin{remark}
It is easy to see that one can recover the pmf from the CDF and vice versa. For example, given the pmf $f$, we can write the CDF as $F(t)=\sum_{u:u\le t}f(u)$. Conversely, given the CDF, by looking at the locations of the jumps and the sizes of the jumps, we can recover the pmf.
\end{remark}

The point is that probabilistic questions about $X$ can be answered by knowing its CDF $F_{X}$. Therefore, in a sense, the probability space becomes irrelevant. For example, the expected value of a random variable can be computed using its CDF only. Hence, we shall often make statements like ``$X$ is a random variable with pmf $f$'' or ``$X$ is a random variable with CDF $F$'', without bothering to indicate the probability space. 

Some distributions (i.e., CDF or the associated pmf) occur frequently enough to merit a name.

\begin{example} Let $f$ and $F$ be the pmf, CDF pair 
$$
f(t)=\begin{cases}p & \mbox{ if }t=1, \\ q & \mbox{ if }t=0, \end{cases} \qquad F_{X}(t)=\begin{cases} 1 &\mbox{ if } t\ge 1, \\ q & \mbox{ if }t\in [0,1), \\ 0 & \mbox{ if }t< 0. \end{cases}
$$ 
A random variable $X$ having this pmf (or equivalently the CDF) is said to have {\em Bernoulli distribution} with parameter $p$ and  write $X\sim \mbox{Ber}(p)$. For example, if $\Omega=\{1,2,\ldots ,10\}$ with $p_{i}=1/10$, and $X(\omega)={\mathbf 1}_{\omega\le 3}$, then $X\sim \mbox{Ber}(0.3)$. Any random variable taking only the values $0$ and $1$, has Bernoulli distribution.
\end{example}

\begin{example}  Fix $n\ge 1$ and $p\in [0,1]$. The pmf defined by $f(k)=\binom{n}{k}p^{k}q^{n-k}$ for $0\le k\le n$ is called the {\em Binomial  distribution} with parameters $n$ and $p$ and is denoted Bin($n,p$). The CDF is as usual defined by $F(t)=\sum_{u:\u\le t}f(u)$, but it does not have any particularly nice expression. 

For example, if $\Omega=\{0,1\}^{n}$ with $p_{\underline{\ome}}=p^{\sum_{i}\omega_{i}}q^{n-\sum_{i}\omega_{i}}$, and $X(\underline{\ome})=\omega_{1}+\ldots +\omega_{n}$, then $X\sim \mbox{Bin}(n,p)$. In words, the number of heads in $n$ tosses of a $p$-coin has $\mbox{Bin}(n,p)$ distribution.
\end{example}

\begin{example} Fix $p\in (0,1]$ and let $f(k)=q^{k-1}p$ for $k\in \mathbb{N}_{+}$. This is called the {\em Geometric  distribution} with parameter $p$ and is denoted Geo($p$). The CDF is
$$
F(t) = \begin{cases}
0 & \mbox{ if } t<1, \\
1-q^{k} & \mbox{ if } k\le t<k+1, \mbox{ for some }k\ge 1.
\end{cases}
$$
For example, the number of tosses of a $p$-coin till the first head turns up, is a random variable with $\mbox{Geo}(p)$ distribution.
\end{example} 

\begin{example} Fix $\lambda>0$ and define the pmf $f(k)=e^{-\lambda}\frac{\lambda^{k}}{k!}$. This is called the {\em Poisson  distribution} with parameter $\lambda$ and is denoted Pois($\lambda$).

In the problem of a psychic (randomly) guessing the cards in a deck, we have seen that the number of matches (correct guesses) had an {\em approximately} Pois($1$) distribution.
\end{example}

\begin{example} Fix positive integers $b,w$ and $m\le b+w$. Define the pmf  $f(k)=\frac{\binom{b}{k}\binom{w}{m-k}}{\binom{b+w}{m}}$ where the binomial coefficient $\binom{x}{y}$ is interpreted to be zero if $y>x$ (thus $f(k)>0$ only for $\max\{m-w,0\}\le k\le b$). This is called the {\em Hypergeometric distribution} with parameters $b,w,m$ and we shall denote it by  Hypergeo($b,w,m$).

Consider a population with $b$ men and $w$ women. The number of men in a random sample (without replacement) of size $m$, is a random variable with the Hypergeo($b,w,m$) distribution.
\end{example}

\parag{Computing expectations from the pmf} Let $X$ be a random variable on $(\Omega,p)$ with pmf $f$. Then we claim that
$$
\mathbf{E}[X] = \sum_{t\in \mathbb{R}}tf(t).
$$
Indeed, let $\mbox{Range}(X)=\{x_{1},x_{2},\ldots\}$. Let $A_{k}=\{\omega{\; : \;} X(\omega)=x_{k}\}$. By definition of pmf we have $\mathbf{P}(A_{k})=f(x_{k})$. Further, $A_{k}$ are pairwise disjoint and exhaustive. Hence 
$$
\mathbf{E}[X] = \sum_{\omega\in \Omega}X(\omega)p_{\omega} = \sum_{k}\sum_{\omega\in A_{k}}X(\omega)p_{\omega} = \sum_{k}x_{k}\mathbf{P}(A_{k})=\sum_{k}x_{k}f(x_{k}).
$$
Similarly, $\mathbf{E}[X^{2}]=\sum_{k}x_{k}^{2}f(x_{k})$. More generally, if $h:\mathbb{R}\rightarrow \mathbb{R}$ is any function, then the random variable $h(X)$ has expectation $\mathbf{E}[h(X)]=\sum_{k}h(x_{k})f(x_{k})$. Although this sounds trivial, there is a very useful point here. To calculate $\mathbf{E}[X^{2}]$ we do not have to compute the pmf of $X^{2}$ first, which can be done but would be more complicated. Instead, in the above formulas, $\mathbf{E}[h(X)]$ has been computed directly in terms of the pmf of $X$.
\begin{exercise} Find $\mathbf{E}[X]$ and $\mathbf{E}[X^{2}]$ in each case.
\begin{enumerate}\setlength\itemsep{6pt}
\item $X\sim \mbox{Bin}(n,p)$.
\item $X\sim \mbox{Geo}(p)$.
\item $X\sim \mbox{Pois}(\lambda)$.
\item $X\sim \mbox{Hypergeo}(b,w,m)$.
\end{enumerate}
\end{exercise}

\section{General probability distributions}
We take the first three of the four properties of CDF proved in the previous section as the {\em definition} of a CDF or distribution function, in general.
\begin{definition} A (cumulative) distribution function (or CDF for short) is any function $F:\mathbb{R}\rightarrow [0,1]$ be a non-decreasing, right continuous function such that $F(t)\rightarrow 0$ as $t\rightarrow -\infty$ and $F(t)\rightarrow 1$ as $t\rightarrow +\infty$.
\end{definition}
If $(\Omega,p)$ is a discrete probability space and $X:\Omega\mapsto \mathbb{R}$ is any random variable, then the function $F(t)=\mathbf{P}\{\omega{\; : \;} X(\omega)\le t\}$ is a CDF, as discussed in the previous section. However, there are distribution functions that do not arise in this manner.
\begin{example}\label{eg:uniformcf} Let 
\[\begin{aligned}
F(t)=\begin{cases} 0 & \mbox{ if } t\le 0, \\ t &\mbox{ if }0<t<1, \\ 1 &\mbox{ if }t\ge 1. \end{cases}
\end{aligned}\]
Then it is easy to see that $F$ is a distribution function. However, it has no jumps and hence it does not arise as the CDF of any random variable on a discrete probability space. 
\end{example}
There are two ways to rectify this issue.
\begin{enumerate}\setlength\itemsep{6pt}
\item The first way is to learn the notion of uncountable probability spaces, which poses many subtleties. It requires a semester or so of real analysis and measure theory. But after that one can define random variables on uncountable probability spaces and the above example will turn out to be the CDF of some random variable on some (uncountable) probability space.
\item Just regard CDFs such as in the above example as reasonable approximations to CDFs of some discrete random variables. For example, if $\Omega=\{\omega_{0},\omega_{1},\ldots ,\omega_{N}\}$ and $p(\omega_{k})=1/(N+1)$ for all $0\le k\le N$, and $X:\Omega\mapsto \mathbb{R}$ is defined by $X(\omega_{k})=k/n$, then it is easy to check that the CDF of $X$ is the function $G$ given by
\[\begin{aligned}
G(t)=\begin{cases} 0 & \mbox{ if } t\le 0, \\ \frac{k}{N+1} &\mbox{ if }\frac{k-1}{N}\le t<\frac{k}{N}\mbox{ for some }k=1,2,\ldots ,N \\ 1 &\mbox{ if }t\ge 1. \end{cases}
\end{aligned}\]
Now, if $N$ is very large, then the function $G$ looks approximately like the function $F$. Just as it is convenient to regard water as a continuous medium in some problems (although water is made up of molecules and is discrete at small scales), it is convenient to use the continuous function $F$ as a reasonable approximation to the step function $G$. 
\end{enumerate}
We shall take the second option out. Whenever we write continuous distribution functions such as in the above example, at the back of our mind we have a discrete random variable (taking a large number of closely placed values) whose CDF is approximated by our distribution function. The advantage of using continuous objects instead of discrete ones is that the powerful tools of Calculus become available to us.



%%%\newpage
{\color{magenta}
\section{Uncountable probability spaces - conceptual difficulties}
The following two ``random experiments'' are easy to imagine, but difficult to fit into the framework of probability spaces\footnote{This section should be omitted by everyone other than those who are keen to know what we meant by the conceptual difficulties of uncountable probability spaces}.
\begin{enumerate}\setlength\itemsep{6pt}
\item Toss a $p$-coin infinitely many times: Clearly the sample space is $\Omega=\{0,1\}^{\mathbb{N}}$. But what is $p_{\underline{\ome}}$ for any $\underline{\ome}\in \Omega$? The only reasonable answer is $p_{\underline{\ome}}=0$ for all $\omega$. But then how to define $\mathbf{P}(A)$ for any $A$?   For example, if $A=\{\underline{\ome}{\; : \;} \omega_{1}=0,\omega_{2}=0,\omega_{3}=1\}$, then everyone agrees that $\mathbf{P}(A)$ ``ought to be'' $q^{2}p$, but how does that come about? The basic problem is that $\Omega$ is uncountable, and probabilities of events are not got by summing probabilities of singletons.
\item Draw a number at random from $[0,1]$: Again, it is clear that $\Omega=[0,1]$, but it also seems reasonable that $p_{x}=0$ for all $x$. Again, $\Omega$ is uncountable, and probabilities of events are not got by summing probabilities of singletons. It is ``clear'' that if $A=[0.1,0.4]$, then $\mathbf{P}(A)$ ``ought to be'' $0.3$, but it gets confusing when one tries to derive this from something more basic!
\end{enumerate}

\para{The resolution} Let $\Omega$ be uncountable. There is a class of {\em basic subsets} (usually not singletons) of $\Omega$ for which we take the probabilities as given. We also take the rules of probability, namely, countable additivity, as axioms. Then we use the rules to compute the probabilities of more complex events (subsets of $\Omega$) by expressing those events in terms of the basic sets using countable intersections, unions and complements and applying the rules of probability.

\begin{example} In the example of infinite sequence of tosses, $\Omega=\{0,1\}^{\mathbb{N}}$. Any set of the form $A=\{\underline{\ome}{\; : \;} \omega_{1}=\epsilon_{1},\ldots ,\omega_{k}=\epsilon_{k}\}$ where $k\ge 1$ and $\epsilon_{i}\in \{0,1\}$ will be called a basic set and its probability is \underline{defined} to be $\mathbf{P}(A)=\prod_{j=1}^{k}p^{\epsilon_{j}}q^{1-\epsilon_{j}}$ where we assume that $p>0$. Now consider a more complex event, for example, $B=\{\underline{\ome} {\; : \;} \omega_{k}=1\mbox{ for some }k\}$. We can write $B=A_{1}\cup A_{2}\cup A_{3}\cup\ldots$ where $A_{k}=\{\underline{\ome}{\; : \;} \omega_{1}=0,\ldots ,\omega_{k-1}=0,\omega_{k}=1\}$. Since $A_{k}$ are pairwise disjoint, the rules of probability demand that $\mathbf{P}(B)$ should be $\sum_{k}\mathbf{P}(A_{k})=\sum_{k}q^{k-1}p$ which is in fact equal to $1$.
\end{example}
\begin{example} In the example of drawing a number at random from $[0,1]$, $\Omega=[0,1]$. Any interval $(a,b)$ with $0\le a<b\le 1$ is called a basic set and its probability is defined as $\mathbf{P}(a,b)=b-a$. Now consider a non-basic event $B=[a,b]$. We can write $B=A_{1}\cup A_{2}\cup A_{3}\ldots$ where $A_{k}=(a+(1/k),b-(1/k))$. Then $A_{k}$ is an increasing sequence of events and the rules of probability say that $\mathbf{P}(B)$ must be equal to $\lim_{k\rightarrow \infty}\mathbf{P}(A_{k})=\lim_{k\rightarrow \infty}(b-a-(2/k)) = b-a$. Another example could be $C=[0.1,0.2)\cup(0.3,0.7]$. Similarly argue that $\mathbf{P}(\{x\})=0$ for any $x\in [0,1]$. A more interesting one is $D=\mathbb Q \cap [0,1]$. Since it is a countable union of singletons, it must have zero probability! Even more interesting is the $1/3$-Cantor set. Although uncountable, it has zero probability!
\end{example}
 
\para{Consistency} Is this truly a solution to the question of uncountable spaces? Are we assured of never running into inconsistencies? Not always. 
\begin{example} Let $\Omega=[0,1]$ and let intervals $(a,b)$ be open sets with their probabilities defined as $\mathbf{P}(a,b)=\sqrt{b-a}$. This quickly leads to problems. For example, $\mathbf{P}(0,1)=1$ by definition. But $(0,1)=(0,0.5)\cup(0.5,1)\cup \{1/2\}$ from which the rules of probability would imply that $\mathbf{P}(0,1)$ must be at least $\mathbf{P}(0,1/2)+\mathbf{P}(1/2,1)=\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}=\sqrt{2}$ which is greater than $1$. Inconsistency!
\end{example}

\begin{exercise} Show that we run into inconsistencies if we define $\mathbf{P}(a,b)=(b-a)^{2}$ for $0\le a<b\le 1$.
\end{exercise}

Thus, one cannot arbitrarily assign probabilities to basic events. However, if we use the notion of distribution function to assign probabilities to intervals, then no inconsistencies arise.
\begin{theorem}\label{thm:consistencyofmeasures} Let $\Omega=\mathbb{R}$ and let intervals of the form $(a,b]$ with $a<b$ be called basic sets. Let $F$ be any distribution function. {\em Define} the probabilities of basic sets as $\mathbf{P}\{(a,b]\}=F(b)-F(a)$. Then, applying the rules of probability to compute probabilities of more complex sets (got by taking countable intersections, unions and complements) will never lead to inconsistency.
\end{theorem}
Let $F$ be any CDF. Then, the above consistency theorem really asserts that there exists (a possibly uncountable) probability space and a random variable such that $F(t)=\mathbf{P}\{X\le t\}$ for all $t$. We say that $X$ has distribution $F$. However, it takes a lot of technicalities to define what uncountable probability spaces look like and what random variables mean in this more general setting, we shall never define them. 

The job of a probabilist consists in taking a CDF $F$ (then the probabilities of intervals are already given to us as $F(b)-F(a)$ etc.) and find probabilities of more general subsets of $\mathbb{R}$. Here are the working rules.  Instead we can use the following simple working rules to answer questions about the distribution of a random variable.
\begin{enumerate}\setlength\itemsep{6pt}
\item For an $a<b$, we set $\mathbf{P}\{a<X\le b\}:=F(b)-F(a)$.
\item If $I_{j}=(a_{j},b_{j}]$ are countably many pairwise disjoint intervals, and $I=\bigcup_{j}I_{j}$, then we define $\mathbf{P}\{X\in I\}:=\sum_{j}F(b_{j})-F(a_{j})$.
\item For a general set $A\subseteq \mathbb{R}$, here is a general scheme: Find countably many pairwise disjoint intervals $I_{j}=(a_{j},b_{j}]$ such that $A\subseteq \cup_{j}I_{j}$. Then we define $\mathbf{P}\{X\in A\}$ as the infimum (over all such coverings by intervals) of the quantity $\sum_{j}F(b_{j})-F(a_{j})$. 
\end{enumerate}

{\em\underline{ All of probability in another line}}: Take an (interesting) random variable $X$ with a given CDF $F$ and an (interesting) set $A\subseteq \mathbb{R}$. Find $\mathbf{P}\{X\in A\}$. 


\medskip
There are loose threads here but they can be safely ignored for  this course. We just remark about them for those who are curious to know.

\begin{remark} The above method starts from a CDF $F$ and defines $\mathbf{P}\{X\in A\}$ for all subsets $A\subseteq \mathbb{R}$. However, for most choices of $F$,  the countable additivity property turns out to be  violated! However, the sets which do violate them rarely arise in practice and hence we ignore them for the present.
\end{remark}
 
 \begin{exercise} Let $X$ be  a random variable with distribution $F$. Use the working rules to find the following probabilities.
 \begin{enumerate}\setlength\itemsep{6pt}
 \item Write $\mathbf{P}\{a<X<b\}$, $\mathbf{P}\{a\le X<b\}$, $\mathbf{P}\{a\le X\le b\}$ in terms of $F$.
 \item Show that $\mathbf{P}\{X=a\}=F(a)-F(a-)$. In particular, this probability is zero unless $F$ has a jump at $a$.
 \end{enumerate}
 \end{exercise}
 
 We now illustrate how to calculate the probabilities of rather non-trivial sets in a special case. It is not always possible to get an explicit answer as here.
\begin{example} Let $F$ be the CDF defined in example~\ref{eg:uniformcf}. We calculate $\mathbf{P}\{X\in A\}$ for two sets $A$.

\parag{1}. $A=\mathbb{Q}\cap [0,1]$.  Since $A$ is countable, we may write $A=\cup_{n}\{r_{n}\}$ and hence $A\subseteq \cup_{n}I_{n}$ where $I_{n}=(r_{n},r_{n}+\delta2^{-n}]$ for any fixed $\delta>0$. Hence $\mathbf{P}\{X\in A\}\le \sum_{n}F(r_{n}+\delta 2^{-n})-F(r_{n}) \le 2\delta$. Since this is true for every $\delta>0$, we must have $\mathbf{P}\{X\in A\}=0$. (We stuck to the letter of the recipe described earlier. It would have been simpler to say that any countable set is a countable union of singletons, and by the countable additivity of probability, must have probability zero. Here we used the fact that singletons have zero probability since $F$ is continuous).
\end{example}

\parag{2}. $A=\mbox{Cantor's set}$\footnote{To define the Cantor set, recall that any  $x\in [0,1]$ may be written in ternary expansion as $x=0.u_{1}u_{2}\ldots :=\sum_{n=1}^{\infty}u_{n}3^{-n}$ where $u_{n}\in \{0,1,2\}$. This expansion is unique except if $x$ is a rational number of the form $p/3^{m}$ for some integers $p,m$ (these are called triadic rationals). For triadic rationals, there are two possible ternary expansions, a terminating one and a non-terminating one (for example, $x=1/3$ can be written as $0.100\ldots$ or as $0.0222\ldots$). For definiteness, for triadic rationals we shall always take the non-terminating ternary expansion. With this preparation, the Cantor set is defined as the set of all $x$ which do not have the digit $1$ in their ternary expansion.} How to find $\mathbf{P}\{X\in A\}$? Let $A_{n}$ be the set of all $x\in [0,1]$ which do not have $1$ in the first $n$ digits of their ternary expansion. Then $A\subseteq A_{n}$. Further, it is not hard to see that $A_{n}=I_{1}\cup I_{2}\cup \ldots \cup I_{2^{n}}$ where each of the intervals $I_{j}$ has length equal to $3^{-n}$. Therefore, $\mathbf{P}\{X\in A\}\le \mathbf{P}\{X\in A_{n}\}= 2^{n}3^{-n}$ which goes to $0$ as $n\rightarrow \infty$. Hence, $\mathbf{P}\{X\in A\}=0$.
}
%%%\newpage
\section{Examples of continuous distributions}
Cumulative distributions will also be referred to as simply distribution functions or distributions. We start by giving two large classes of CDFs. There are CDFs that do not belong to either of these classes, but for practical purposes they may be ignored (for now).
\begin{enumerate}\setlength\itemsep{6pt}
\item (CDFs with pmf). Let $f$ be a pmf, i.e., let $t_{1},t_{2},\ldots$ be a countable subset of reals and let $f(t_{i})$ be non-negative numbers such that $\sum_{i}f(t_{i})=1$. Then,  define $F:\mathbb{R}\rightarrow \mathbb{R}$ by
$$
 F(t) := \sum_{i: t_{i}\le t}f(t_{i}).
$$
Then, $F$ is a CDF. Indeed, we have seen that it is the CDF of a discrete random variable. A special feature of this CDF is that it increases only in jumps (in more precise language, if $F$ is continuous on an interval $[s,t]$, then $F(s)=F(t)$).
\item (CDFs with pdf). Let $f:\mathbb{R}\rightarrow\mathbb{R}_{+}$ be a  function (convenient to assume that it is a piece-wise continuous function) such that $\int_{-\infty}^{+\infty}f(u)du=1$. Such a function is called a {\em probability density function} or pdf for short.  Then,  define $F:\mathbb{R}\rightarrow \mathbb{R}$ by
\[\begin{aligned}
F(t) :=\int_{-\infty}^{t}f(u) du.
\end{aligned}\]
Again, $F$ is a CDF. Indeed, it is clear that $F$ has the increasing property (if $t>s$, then $F(t)-F(s)=\int_{s}^{t}f(u)du$ which is non-negative because $f(u)$ is non-negative for all $u$), and its limits at $\pm \infty$ are as they should be (why?). As for right-continuity, $F$ is in-fact continuous. Actually $F$ is differentiable except at points where $f$ is discontinuous and $F'(t)=f(t)$.
\end{enumerate}

\begin{remark} We understand the pmf. For example if $X$ has pmf $f$, then $f(t_{i})$ is just the probability that $X$ takes the value $t_{i}$. How to interpret the pdf? If $X$ has pdf $f$, then as we already remarked, the CDF is continuous and hence $\mathbf{P}\{X=t\}=0$. Therefore $f(t)$ cannot be interpreted as $\mathbf{P}\{X=t\}$ (in fact, pdf can take values greater than $1$, so it cannot be a probability!).

To interpret $f(a)$, take a small positive number $\delta$ and look at  
$$
F(a+\delta)-F(a)  = \int\limits_{a}^{a+\delta}f(u) du \approx \delta f(a).
$$
 In other words, $f(a)$ measures the chance of the random variable taking values near $a$. Higher the pdf, greater the chance of taking values near that point.
\end{remark}


Among distributions with pmf, we have seen the Binomial, Poisson, Geometric and Hypergeometric families of distributions. Now we give many important examples of distributions (CDFs) with densities.
%\begin{example} Let $X$ be a random variable on a discrete probability space. Define $$F(t)=\mathbf{P}\{\omega{\; : \;} X(\omega)\le t\}=\sum\limits_{\omega{\; : \;} X(\omega)\le t}p_{\omega}.$$ Then show that $F$ is a distribution function. It is referred to as the CDF of $X$. Note that such distribution functions increase only in jumps.
%\end{example}
%Conversely, if $F$ is a distribution function that increases only in jumps, then it is easy to find a discrete probability space and a random variable which has CDF $F$. At the other end we have CDFs that do not have any jumps, i.e., the CDFs are continuous. We now see examples of these. 

%\para{Densities} Almost all CDFs we shall see will be either of the discrete type or else of the following type.
%\begin{definition} A non-negative function $f:\mathbb{R}\rightarrow [0,\infty)$ satisfying $\int_{-\infty}^{+\infty}f(t)dt=1$ is called a {\em probability density function} (PDF).
%\end{definition}
%Given a pdf $f$, define the function $F:\mathbb{R}\rightarrow \mathbb{R}$ by $F(t)=\int_{-\infty}^{t}f(u)du$. Then it is easy to check that $F$ is a (continuous) CDF. If $F$ is the CDF of a random variable $X$, we also say that $f$ is the pdf of $X$ (or of $F$). Quite often it is convenient to specify the pdf instead of the CDF. We shall do so in examples below.

\begin{example} \para{Uniform distribution on the interval $[a,b]$}, denoted Unif($[a,b]$) where $a<b$ is the distribution with density and distribution given by 
$$
\mbox{PDF:}\; f(t) = \begin{cases}\frac{1}{b-a} & \mbox{if } t\in(a,b) \\ 0 & \mbox{otherwise} \end{cases}\qquad 
\mbox{CDF:}\; F(t) = \begin{cases}0 & \mbox{if } t\le a \\ \frac{t-a}{b-a} & \mbox{if }t\in (a,b) \\ 1 & \mbox{if }t\ge b.\end{cases}
$$

\end{example}

\begin{example} \para{Exponential distribution with parameter $\lambda$}, denoted Exp($\lambda$) where $\lambda>0$ is the distribution with density and distribution given by 
$$
\mbox{PDF:}\; f(t) = \begin{cases}\lambda e^{-\lambda t}& \mbox{if } t>0 \\ 0 & \mbox{otherwise} \end{cases}\qquad 
\mbox{CDF:}\; F(t) = \begin{cases}0 & \mbox{if } t\le 0 \\ 1-e^{-\lambda t} & \mbox{if }t>0.\end{cases}
$$
\end{example}
\begin{example} \para{Normal distribution with parameters $\mu,{\sigma}^{2}$}, denoted N($\mu,{\sigma}^{2}$) where $\mu\in \mathbb{R}$ and ${\sigma}^{2}>0$ is the distribution with density and distribution given by 
$$
\mbox{PDF:}\; \varphi_{\mu,{\sigma}^{2}}(t) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2{\sigma}^{2}}(t-\mu)^{2}}\qquad 
\mbox{CDF:}\; \Phi_{\mu,{\sigma}^{2}}(t) = \int\limits_{-\infty}^{t}\varphi_{\mu,{\sigma}^{2}}(u)du.
$$
There is no closed form expression for the CDF. It is standard notation to write $\varphi$ and $\Phi$ to denote the normal density and CDF when $\mu=0$ and ${\sigma}^{2}=1$. N($0,1$) is called the standard normal distribution. By a change of variable one can check that $\Phi_{\mu,{\sigma}^{2}}(t)=\Phi(\frac{t-\mu}{{\sigma}})$.

We said that the normal CDF has no simple expression, but is it even clear that it is a CDF?! In other words, is the proposed density a true pdf? Clearly $\varphi(t)=\frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}$ is non-negative. We need to check that its integral is $1$.

\begin{lemma} Fix $\mu\in \mathbb{R}$ and ${\sigma}>0$ and let $\varphi(t)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2{\sigma}^{2}}(t-\mu)^{2}}$. Then, $\int\limits_{-\infty}^{\infty}\varphi(t) dt =1$.
\end{lemma}

\begin{proof} It suffices to check the case $\mu=0$ and ${\sigma}^{2}=1$ (why?).   To find its integral is quite non-trivial. Let $I=\int_{-\infty}^{\infty} \varphi(t)dt$. We introduce the two-variable function $h(t,s):=\varphi(t)\varphi(s)=(2\pi)^{-1}e^{-(t^{2}+s^{2})/2}$. On the one hand,
$$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h(t,s)dtds = \left(\int_{-\infty}^{+\infty}\varphi(t)dt\right) \left(\int_{-\infty}^{+\infty}\varphi(s)ds\right)=I^{2}.
$$
On the other hand, using polar co-ordinates $t=r\cos\theta$, $s=r\sin \theta$, we see that
$$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h(t,s)dtds =\int_{0}^{\infty}\int_{0}^{2\pi}(2\pi)^{-1}e^{-r^{2}/2}rd\theta dr = \int_{0}^{\infty}re^{-r^{2}/2}dr =1
$$
since $\frac{d}{dr}e^{-r^{2}/2}=-re^{-r^{2}/2}$. Thus $I^{2}=1$ and hence $I=1$.
\end{proof}
\end{example}
\begin{example} \para{Gamma distribution with shape parameter $\nu$ and scaler parameter $\lambda$}, where $\nu>0$ and $\lambda>0$, denoted Gamma($\nu,\lambda$) is the distribution with density and distribution given by - 
$$
\mbox{PDF:}\; f(t) = \begin{cases}\frac{1}{\Gamma(\nu)}\lambda^{\nu} t^{\nu-1}e^{-\lambda t}& \mbox{if } t>0 \\ 0 & \mbox{otherwise} \end{cases}\qquad 
\mbox{CDF:}\; F(t) = \begin{cases}0 & \mbox{if } t\le 0 \\ \int_{0}^{t}f(u)du & \mbox{if }t>0.\end{cases}
$$
Here $\Gamma(\nu):=\int_{0}^{\infty}t^{\nu-1}e^{-t}dt$. Firstly, $f$ is a density, that is, that it integrates to $1$. To see this, make the change of variable $\lambda t=u$ to see that
$$
\int_{0}^{\infty}\lambda^{\nu}e^{-\lambda t}t^{\nu-1}dt = \int_{0}^{\infty}e^{-u}u^{\nu-1}d\nu = \Gamma(\nu).
$$
Thus, $\int_{0}^{\infty} f(t)dt=1$.

When $\nu=1$, we get back the exponential distribution. Thus, the Gamma family subsumes the exponential distributions.   For positive integer values of $\nu$, one can actually write an expression for the CDF of Gamma($\nu,\lambda$)  as (this is a homework problem)
$$
F_{\nu,\lambda}(t)=1-e^{-\lambda t}\sum\limits_{k=0}^{\nu-1}\frac{(\lambda t)^{k}}{k!}.
$$
Once the expression is given, it is easy to check it by induction (and integration by parts). A curious observation is that the right hand side is exactly $\mathbf{P}(N\ge \nu)$ where $N\sim \mbox{Pois}(\lambda t)$. This is in fact indicating a deep connection between Poisson distribution and the Gamma distributions. The function $\Gamma(\nu)$, also known as Euler's Gamma function, is an interesting and important  one and occurs all over mathematics.
\footnote{\noindent{\bf The Gamma function:} The function $\Gamma:(0,\infty)\rightarrow \mathbb{R}$ defined by $\Gamma(\nu)=\int_{0}^{\infty}e^{-t}t^{\nu-1}dt$ is a very important function that often occurs in mathematics and physics. There is no simpler expression for it, although one can find it explicitly for special values of $\nu$. One of its most important properties is that $\Gamma(\nu+1)=\nu\Gamma(\nu)$. To see this, consider
$$
\Gamma(\nu+1)=\int_{0}^{\infty}e^{-t}t^{\nu}dt = -e^{-t}t^{\nu}\left.\vphantom{\hbox{\Large (}}\right|_{0}^{\infty}+\nu\int_{0}^{\infty}e^{-t}t^{\nu-1}dt = \nu \Gamma(\nu).
$$
Starting with  $\Gamma(1)=1$ (direct computation) and using the above relationship repeatedly one sees that $\Gamma(\nu)=(\nu-1)!$ for positive integer values of $\nu$. Thus, the Gamma function interpolates the factorial function (which is defined only for positive integers).  Can we compute it for any other $\nu$? The answer is yes, but only for special values of $\nu$. For example, 
\[\begin{aligned}
\Gamma(1/2)= \int_{0}^{\infty}x^{-1/2}e^{-x}dx = \sqrt{2}\int_{0}^{\infty}e^{-y^{2}/2}dy
\end{aligned}\]
by substituting $x=y^{2}/2$. The last integral was computed above in the context of the normal distribution and equal to $\sqrt{\pi/2}$. Hence we get $\Gamma(1/2)=\sqrt{\pi}$. From this, using again the relation $\Gamma(\nu+1)=\nu\Gamma(\nu)$, we can compute $\Gamma(3/2)=\frac{1}{2}\sqrt{\pi}$, $\Gamma(5/2)=\frac{3}{4}\sqrt{\pi}$, etc. Yet another useful fact about the Gamma function is its asymptotics as $\nu\rightarrow\infty$.

{\bf Stirling's approximation:} $\frac{\Gamma(\nu+1)}{\nu^{\nu+\frac{1}{2}}e^{-\nu}\sqrt{2\pi}}\rightarrow 1$ as $\nu\rightarrow \infty$.

\para{A small digression} It was Euler's  idea to observe that $n!=\int_{0}^{\infty}x^{n}e^{-x}dx$ and that on the right side $n$ could be replaced by  any real number greater than $-1$. But this was his second approach to defining the Gamma function. His first approach was as follows. Fix a positive integer $n$. Then for any $\ell\ge 1$ (also a positive integer), we may write
\[\begin{aligned}
n!=\frac{(n+\ell)!}{(n+1)(n+2)\ldots (n+\ell)} = \frac{\ell!(\ell+1)\ldots (\ell+n)}{(n+1)\ldots (n+\ell)} = \frac{\ell! \ \ell^{n}}{(n+1)\ldots (n+\ell)}\cdot\frac{(\ell+1)\ldots (\ell+n)}{\ell^{n}}
\end{aligned}\] 
The second factor approaches $1$ as $\ell\rightarrow \infty$. Hence,
\[\begin{aligned}
n!=\lim_{\ell\rightarrow \infty}\frac{\ell! \ \ell^{n}}{(n+1)\ldots (n+\ell)}.
\end{aligned}\]
Euler then showed (by a rather simple argument that we skip) that the limit on the right exists if we replace $n$ by any complex number other than $\{-1,-2,-3,\ldots \}$ (negative integers are a problem as they make the denominator zero). Thus, he extended the factorial function to all complex numbers except negative integers! It is a fun exercise to check that this agrees with the definition by the integral given earlier. In other words, for $\nu>-1$, we have
\[\begin{aligned}
\lim_{\ell\rightarrow \infty}\frac{\ell! \ \ell^{\nu}}{(\nu+1)\ldots (\nu+\ell)}=\int_{0}^{\infty}x^{\nu}e^{-x}dx.
\end{aligned}\]
}
\end{example}

\begin{example} \para{Beta distributions} Let $\alpha,\beta>0$. The Beta distribution with parameters $\alpha,\beta$, denoted Beta($\alpha,\beta$), is the distribution with density and distribution given by - 
\[\begin{aligned}
\mbox{PDF:}\; f(t) = \begin{cases}\frac{1}{B(\alpha,\beta)}t^{\alpha-1}(1-t)^{\beta-1}& \mbox{if } t\in(0,1) \\ 0 & \mbox{otherwise} \end{cases}\qquad 
\mbox{CDF:}\; F(t) = \begin{cases}0 & \mbox{if } t\le 0 \\ \int_{0}^{t}f(u)du & \mbox{if }t\in(0,1) \\
0 &\mbox{if }t\ge 1.\end{cases}
\end{aligned}\]
Here $B(\alpha,\beta):=\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt$. Again, for special values of $\alpha,\beta$ (eg., positive integers), one can find the value of $B(\alpha,\beta)$, but in general there is no simple expression. However, it can be expressed in terms of the Gamma function!
\begin{proposition} For any $\alpha,\beta>0$, we have $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$.
\end{proposition}
\begin{proof} For $\beta=1$ we see that $B(\alpha,1)=\int_{0}^{1}t^{\alpha-1}=\frac{1}{\alpha}$ which is also equal to $\frac{\Gamma(\alpha)\Gamma(1)}{\Gamma(\alpha+1)}$ as required. Similarly (or by the symmetry relation $B(\alpha,\beta)=B(\beta,\alpha)$), we see that $B(1,\beta)$ also has the desired expression. 

Now for any other {\em positive integer} value of $\alpha$ and real $\beta>0$ we can integrate by parts and get
\begin{align*}
B(\alpha,\beta)&=\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt \\
&= -\frac{1}{\beta}t^{\alpha-1}(1-t)^{\beta}\left.\vphantom{\hbox{\Large (}}\right|_{0}^{1} + \frac{\alpha-1}{\beta}\int_{0}^{1}t^{\alpha-2}(1-t)^{\beta}dt \\
&= \frac{\alpha-1}{\beta}B(\alpha-1,\beta+1).
\end{align*}
Note that the first term vanishes because $\alpha> 1$ and $\beta>0$. When $\alpha$ is an integer, we repeat this for $\alpha$ times and get
$$
B(\alpha,\beta)=\frac{(\alpha-1)(\alpha-2)\ldots 1}{\beta(\beta+1)\ldots (\beta+\alpha-2)}B(1,\beta+\alpha-1).
$$
But we already checked that $B(1,\beta+\alpha-1)=\frac{\Gamma(1)\Gamma(\alpha+\beta-1)}{\Gamma(\alpha+\beta)}$ from which we get
$$
B(\alpha,\beta) = \frac{(\alpha-1)(\alpha-2)\ldots 1}{\beta(\beta+1)\ldots (\beta+\alpha-2)}\frac{\Gamma(1)\Gamma(\alpha+\beta-1)}{\Gamma(\alpha+\beta)} =\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
$$
by the recursion property of the Gamma function. Thus we have proved the proposition when $\alpha$ is a positive integer. By symmetry the same is true when $\beta$ is a positive integer (and $\alpha$ can take any value). We do not bother to prove the proposition for general $\alpha,\beta>0$ here.
\end{proof}
\end{example}
\begin{example} \para{The standard Cauchy distribution}  is the distribution with density and distribution given by  
$$
\mbox{PDF:}\; f(t) = \frac{1}{\pi(1+t^{2})}\qquad 
\mbox{CDF:}\; F(t) = \frac{1}{2}+\frac{1}{\pi}\tan^{-1}t.
$$
One can also make a parametric family of Cauchy distributions with parameters $\lambda>0$ and $a\in \mathbb{R}$ denoted Cauchy($a,\lambda$) and having density and CDF
$$
f(t)=\frac{\lambda}{\pi(\lambda^{2}+(t-a)^{2})}\qquad F(t)=\frac{1}{2}+\frac{1}{\pi}\tan^{-1}\left(\frac{t-a}{\lambda}\right).
$$
\end{example}

\begin{remark} Does every CDF come from a pdf? Not necessarily. For example any CDF that is not continuous (for example, CDFs of discrete distributions such as Binomial, Poisson, Geometric etc.). In fact even continuous CDFs may not have densities (there is a good example manufactured out of the $1/3$-Cantor set, but that would take us out of the topic now). However, suppose $F$ is a {\em continuous} CDF and suppose $F$ is differentiable except at finitely many points and that the derivative is a continuous function. Then $f(t):=F'(t)$ defines a pdf which by the fundamental theorm of Calculus satisfies $F(t)=\int_{-\infty}^{t}f(u)du$. 
\end{remark}

%\newpage
\section{Simulation}
As we have emphasized, probability is applicable to many situations in the real world. As such one may conduct experiments to verify the extent to which theorems are actually valid. For this we need to be able to draw numbers at random from any given distribution. 

For example, take the case of Bernoulli($1/2$) distribution. One experiment that can give this is that of physically tossing a coin. This is not entirely satisfactory for several reasons. Firstly, are real coins fair? Secondly, what if we change slightly and want to generate from Ber($0.45$)? In this section, we describe how to draw random numbers from various distributions on a computer. We do not fully answer this question. Instead what we shall show is

\noindent{\em If one can generate random numbers from Unif($[0,1]$) distribution, then one can draw random numbers from any other distribution. 
 More precisely, suppose $U$ is a random variable with Unif($[0,1])$ distribution. We want to simulate random numbers from a given distribution $F$. Then, we shall find a function $\psi:[0,1]\rightarrow \mathbb{R}$ so that the random variable $X:=\psi(U)$ has the given distribution $F$.}



The question of how to draw random numbers from Unif($[0,1]$) distribution is a very difficult one and we shall just make a few superficial remarks about that. 

\para{Drawing random numbers from a discrete pmf} First start with an example.

\begin{example} Suppose we want to draw random numbers from Ber($0.4$) distribution. Let $\psi:[0,1]\rightarrow \mathbb{R}$ be defined as $\psi(t)={\mathbf 1}_{t\le 0.4}$. Let $X=\psi(U)$, i.e., $X=1$ if $U\le 0.4$ and $X=0$ otherwise. Then
$$
\mathbf{P}\{X=1\}=\mathbf{P}\{U\le 0.4\}=0.4, \qquad \mathbf{P}\{X=0\} = \mathbf{P}\{U>0.4\}=0.6.
$$
Thus, $X$ has Ber($0.4$) distribution. 
\end{example}
It is clear how to generalize this. 

\para{General rule} Suppose we are given a pmf $f$
$$
\left(\begin{array}{cccc} t_{1} & t_{2} & t_{3} & \ldots \\ f(t_{1}) & f(t_{2}) & f(t_{3}) & \ldots \end{array} \right).
$$
Then, define $\psi:[0,1]\rightarrow \mathbb{R}$ as
$$
\psi(u) = \begin{cases} t_{1} & \mbox{ if }u\in [0,f(t_{1})] \\
t_{2} & \mbox{ if }u\in (f(t_{1}),f(t_{1})+f(t_{2})] \\
t_{3} & \mbox{ if }u\in (f(t_{1})+f(t_{2}),f(t_{1})+f(t_{2})+f(t_{3})] \\
\vdots & \vdots \end{cases}.
$$
Then define $X=f(U)$. Clearly $X$ takes the values $t_{1},t_{2},\ldots$ and 
$$
\mathbf{P}\{X=t_{k}\} = \mathbf{P}\left\{\sum_{j=1}^{k-1}f(t_{j})<U\le \sum_{j=1}^{k}f(t_{j})\right\} = f(t_{k}).
$$
Thus $X$ has pmf $f$.

\begin{exercise} Draw 100 random numbers from each of the following distributions and draw the histograms. Compare with the pmf.
\begin{enumerate}\setlength\itemsep{6pt}
\item Bin($n,p$) for $n=10,20,40$ and $p=0.5, 0.3, 0.9$.
\item Geo($p$) for $p=0.9,0.5,0.3$.
\item Pois($\lambda$) with $\lambda=1,4,10$.
\item Hypergeo($N_{1},N_{2},m$) with $N_{1}=100,N_{2}=50,m=20$, $N_{1}=1000,N_{2}=1000,m=40$.
\end{enumerate}
\end{exercise}


\para{Drawing random numbers from a pdf} Clearly the procedure used for generating from a pmf is inapplicable here. First start with two  examples. As before $U$ is a Unif($[0,1]$) random variable.
\begin{example} Suppose we want to draw from the Unif($[3,7]$) distribution. Set $X=4U+3$. Clearly 
$$
\mathbf{P}\{X\le t\} = \mathbf{P}\{U\le \frac{t-3}{4}\} =\begin{cases}
0 & \mbox{ if }t<0 \\
(t-3)/4 & \mbox{ if }3\le t \le 7 \\
1 & \mbox{ if }t>7
\end{cases}.
$$
This is precisely the CDF of Unif($[3,7]$) distribution. 
\end{example}
\begin{example} Here let us do the opposite, just take some function of a uniform variable and see what CDF we get. Let $\psi(t)=t^{3}$ and let $X=\varphi(U)=U^{3}$. Then, 
$$
F(t):=\mathbf{P}\{X\le t\} = \mathbf{P}\{U\le t^{1/3}\} =\begin{cases}
0 & \mbox{ if }t<0 \\
t^{1/3} & \mbox{ if }0\le t \le 1 \\
1 & \mbox{ if }t>1
\end{cases}.
$$
Differentiating the CDF, we get the density
$$
f(t)=F'(t) = \begin{cases}
\frac{1}{3}t^{-2/3} & \mbox{ if }0<t<1 \\
0 & \mbox{ otherwise}.
\end{cases}
$$
The derivative does not exist at $0$ and $1$, but as remarked earlier, it does not matter if we change the value of the density at finitely many points (as the integral over any interval will remain the same). Anyway, we notice that the density is that of Beta($1/3,1$). Hence $X\sim \mbox{Beta}(1/3,1)$.
\end{example}
This gives us the idea that to generate random number from a CDF $F$, we should find a function $\psi:[0,1]\rightarrow \mathbb{R}$ such that $X:=\psi(U)$ has the distribution $F$. How to find the distribution of $X$?

\begin{lemma} Let $\psi:(0,1)\rightarrow \mathbb{R}$ be a  strictly increasing function with $a=\psi(0+)$ and $b=\psi(1-)$. Let $X=\psi(U)$. Then $X$ has CDF 
$$
F(t)=\begin{cases} 0 & \mbox{ if }t\le a \\ \psi^{-1}(t) & \mbox{ if }a<t<b \\ 1 & \mbox{ if }t\ge b. \end{cases}
$$ 
If is $\psi$  also differentiable and the derivative does not vanish anywhere (or vanishes at finitely many points only), then $X$ has pdf 
$$
f(t)=\begin{cases} \left(\psi^{-1}\right)'(t) & \mbox{ if }a<t<b\\ 0 & \mbox{ if }t\not\in (a,b). \end{cases}
$$
\end{lemma}
\begin{proof} Since $\psi$ is strictly increasing, $\psi(u)\le t$ if and only if $u\le \psi^{-1}(t)$. Hence,
$$
F(t)=\mathbf{P}\{X\le t\} = \mathbf{P}\{U\le \psi^{-1}(t)\} = \begin{cases} 0 & \mbox{ if }t\le a \\ \psi^{-1}(t) & \mbox{ if }a<t<b \\ 1 & \mbox{ if }t\ge b. \end{cases}
$$
If $\psi$ is differentiable at and $\psi(u)\not=0$, then $\psi^{-1}$ is differentiable at $t=\psi(u)$ (and indeed, $(\psi^{-1})'(t)=\frac{1}{\psi'(u)}$). Thus we get the formula for the density.
\end{proof}

From this lemma, we immediately get the following rule for generating random numbers from a density. 

\para{How to simulate from a CDF} Let $F$ be a CDF that is strictly increasing on an interval $[A,B]$ where $F(A)=0$ and $F(B)=1$ (it is allowed to take $A=-\infty$ and/or $B=+\infty$). Then define $\psi:(0,1)\rightarrow (A,B)$ as $\psi(u)=F^{-1}(u)$. Let $U\sim \mbox{Unif}([0,1])$ and let $X=\psi(U)$. Then $X$ has CDF equal to $F$.

This follows from the lemma because $\psi$ is define as the inverse of $F$ and hence $F$ (restricted to $(A,B)$) is the inverse of $\psi$. Further, as the inverse of a strictly increasing function, the function $\psi$  is also strictly increasing.

\begin{example} Consider the Exponential distribution with parameter $\lambda$ whose CDF is 
$$
F(t)=\begin{cases} 0 & \mbox{ if }t\le 0 \\ 1-e^{-\lambda t} \mbox{ if }t>0 \end{cases}
$$ Take $A=0$ and $B=+\infty$. Then $F$ is increasing on $(0,\infty)$ and its inverse is the function $\psi(u)=-\frac{1}{\lambda}\log(1-u)$. Thus to simulate a random number from Exp($\lambda$) distribution, we set $X=-\frac{1}{\lambda}\log(1-U)$.
\end{example}

When the CDF is not explicitly available as a function we can still adopt the above procedure but only numerically. Consider an example.
\begin{example} Suppose $F=\Phi$, the CDF of $N(0,1)$ distribution. Then we do not have an explicit form for either $\Phi$ or for its inverse $\Phi^{-1}$. With a computer we can do the following. Pick a large number of closely placed points, for example divide the interval $[-5,5]$ into $1000$ equal intervals of length $0.01$ each. Let the endpoints of these intervals be labelled $t_{0}<t_{1}<\ldots <t_{1000}$. For each $i$, calculate $\Phi(t_{i})=\int_{-\infty}^{t_{i}}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}dx$ using numerical methods for integration, say the numerical value obtained is $w_{i}$. This is done only once and create the table of values 
$$
\begin{array}{cccccc}
t_{0} & t_{1} & t_{2} & \ldots & \ldots & t_{1000} \\
w_{0} & w_{1} & w_{2} & \ldots & \ldots & w_{1000}
\end{array}.
$$
Now draw a uniform random number $U$. Look up the table and find the value of $i$ for which $w_{i}<U<w_{i+1}$. Then set $X=t_{i}$. If it so happens that $U<w_{0}$, set $X=t_{0}=-5$ and if $U>w_{1000}$ set $X=t_{1000}=5$. But since $\Phi(-5)<0.00001$ and $\Phi(5)>0.99999$, it is highly unlikely that the last two cases will occur. The random variable $X$ has a distribution close to $N(0,1)$.
\end{example}

\begin{exercise} Give an explicit method to draw random numbers from the following densities.
\begin{enumerate}\setlength\itemsep{6pt}
\item Cauchy distribution with density $\frac{1}{\pi(1+x^{2})}$.
\item Beta($\frac{1}{2},\frac{1}{2}$) density $\frac{1}{\pi}\frac{1}{\sqrt{x(1-x)}}$ on $[0,1]$ (and zero elsewhere). 
\item Pareto($\alpha$) distribution which by definition has the density $$f(t)=\begin{cases} \alpha t^{-\alpha-1} & \mbox{ if }t\ge 1, \\ 0 & \mbox{ if }t<1. \end{cases}$$ 
\end{enumerate}
\end{exercise}


We have described a general principle. When we do more computations with random variables and understand the relationships between different distributions, better tricks can be found.  For example, we shall see later that we can generate two $N(0,1)$ random numbers as follows: Pick two uniform random numbers $U,V$ and set $X=\sqrt{-2\log(1-U)}\cos(2\pi V)$ and $Y=\sqrt{-2\log(1-U)}\sin(2\pi V)$. Then it turns out that $X$ and $Y$ have exactly $N(0,1)$ distribution! As another example, suppose we need to generate from Gamma($3,1$) distribution, we can first generate three uniforms $U_{1},U_{2},U_{3}$ and set $\xi_{i}=-\log(1-U_{i})$ (so $\xi_{i}$ have exponential distribution) and then define $X=\xi_{1}+\xi_{2}+\xi_{3}$. It turns out that $X$ has Gamma($3,1$) distribution!


\begin{remark} We have conveniently skipped the question of how to draw random numbers from uniform distribution in the first place. This is a difficult topic and various results, proved and unproved, are used in generating such numbers. For example, 
\end{remark}

%\newpage
\section{Joint distributions}
In many situations we study several random variables at once. In such a case, knowing the individual distributions is not sufficient to answer all relevant questions. This is like saying that knowing $\mathbf{P}(A)$ and $\mathbf{P}(B)$ is insufficient to calculate $\mathbf{P}(A\cap B)$ or $\mathbf{P}(A\cup B)$ etc.

\begin{definition}[Joint distribution] Let $X_{1},X_{2},\ldots ,X_{m}$ be random variables on the same probability space. We call ${\bf X}=(X_{1},\ldots ,X_{m})$ a {\em random vector}, as it is just a vector of random variables. The CDF of ${\bf X}$, also called the joint CDF of $X_{1},\ldots,X_{m}$ is the function $F:\mathbb{R}^{m}\rightarrow \mathbb{R}$ defined as
$$
F(t_{1},\ldots ,t_{m})=\mathbf{P}\{X_{1}\le t_{1},\ldots ,X_{m}\le t_{m}\} = \mathbf{P}\left\{\bigcap_{i=1}^{m}\{X_{i}\le t_{i}\} \right\}.
$$. 
\end{definition}
\begin{example} Consider two events $A$ and $B$ in the probability space and let $X={\mathbf 1}_{A}$ and $Y={\mathbf 1}_{B}$ be their indicator random variables. Their joint CDF is given by
$$
F(s,t)=\begin{cases}
0 & \mbox{ if }s< 0 \mbox{ or }t< 0 \\
\mathbf{P}(A^{c}\cap B^{c}) & \mbox{ if }s\ge 0, \ t<1\mbox{ or }t\ge 0, \ s<1 \\
\mathbf{P}(A) & \mbox{ if }0\le s<1\mbox{ and }t\ge 1 \\
\mathbf{P}(B) & \mbox{ if }0\le t<1\mbox{ and }s\ge 1 \\
\mathbf{P}(A\cap B) & \mbox{ if } s\ge1, t\ge 1
\end{cases}
$$
\end{example}

\para{Properties of joint CDFs} The following properties of the joint CDF $F:\mathbb{R}^{m}\rightarrow [0,1]$ are analogous to those of the 1-dimensional CDF and the proofs are similar.
\begin{enumerate}\setlength\itemsep{6pt}
\item $F$ is increasing in each co-ordinate. That is, if $s_{1}\le t_{1},\ldots ,s_{m}\le t_{m}$, then $F(s_{1},\ldots,s_{m})\le F(t_{1},\ldots ,t_{m})$.
\item $\lim F(t_{1},\ldots,t_{m})=0$ if $\max\{t_{1},\ldots ,t_{m}\}\rightarrow -\infty$ (i.e., one of the $t_{i}$ goes to $-\infty$).
\item $\lim F(t_{1},\ldots,t_{m})=1$ if $\min\{t_{1},\ldots ,t_{m}\}\rightarrow +\infty$ (i.e., all of the $t_{i}$ goes to $+\infty$).
\item $F$ is right continuous in each co-ordinate. That is $F(t_{1}+h_{1},\ldots ,t_{m}+h_{m})\rightarrow F(t_{1},\ldots ,t_{m})$ as $h_{i}\rightarrow 0+$.
\end{enumerate}
Conversely any function having these four properties is the joint CDF of some random variables.

From the joint CDF, it is easy to recover the individual CDFs. Indeed, if $F:\mathbb{R}^{m}\rightarrow \mathbb{R}$ is the CDF of ${\bf X}=(X_{1},\ldots ,X_{m})$, then the CDF of $X_{1}$ is given by $F_{1}(t):=F(t,+\infty,\ldots,+\infty):=\lim F(t,s_{2},\ldots,s_{m})$ as $s_{i}\rightarrow +\infty$ for each $i=2,\ldots ,m$. This is true because if $A_{n}:=\{X_{1}\le t\}\cap\{X_{2}\le n\}\cap \ldots \cap\{X_{m}\le n\}$, then as $n\rightarrow \infty$, the events $A_{n}$ increase to the event $A=\{X_{1}\le t\}$. Hence $\mathbf{P}(A_{n})\rightarrow \mathbf{P}(A)$. But $\mathbf{P}(A_{n})=F(t,n,n,\ldots,n)$ and $\mathbf{P}(A)=F_{1}(t)$. Thus we see that $F_{1}(t):=F(t,+\infty,\ldots,+\infty)$.

 More generally, we can recover the joint CDF of any subset of $X_{1},\ldots ,X_{n}$, for example, the joint CDF of $X_{1},\ldots ,X_{k}$ is just $F(t_{1},\ldots ,t_{k},+\infty,\ldots ,+\infty)$.

\para{Joint pmf and pdf} Just like in the case of one random variable, we can consider the following two classes of random variables.
\begin{enumerate}\setlength\itemsep{6pt}
\item Distributions with a pmf. These are CDFs for which there exist points ${\bf t}_{1},{\bf t}_{2},\ldots $ in $\mathbb{R}^{m}$ and non-negative numbers $w_{i}$ such that $\sum_{i}w_{i}=1$ (often we write $f(t_{i})$ in place of $w_{i}$) and such that for every ${\bf t}\in \mathbb{R}^{m}$ we have
$$ F({\bf t})=\sum_{i{\; : \;} {\bf t}_{i}\le {\bf t}} w_{i} $$
where ${\bf s}\le {\bf t}$ means that each co-ordinate of $s$ is less than or equal to the corresponding co-ordinate of ${\bf t}$.
\item Distributions with a pdf. These are CDFs for which there is a non-negative function (may assume piecewise continuous for convenience) $f:\mathbb{R}^{m}\rightarrow \mathbb{R}_{+}$ such that for every ${\bf t}\in \mathbb{R}^{m}$ we have
$$ F({\bf t})=\int\limits_{-\infty}^{t_{1}}\!\ldots \int\limits_{-\infty}^{t_{m}} f(u_{1},\ldots ,u_{m})du_{1}\ldots du_{m}. $$
\end{enumerate}
We give two examples, one of each kind. 
\begin{example} (Multinomial distribution). Fix parameters $r,m$ (two  positive integers) and $p_{1},\ldots ,p_{m}$ (positive numbers that add to $1$). The {\em multinomial pmf} with these parameters is given by
$$
f(k_{1},\ldots ,k_{m-1})=\frac{r!}{k_{1}!k_{2}!\ldots k_{m-1}!(r-\sum_{i=1}^{m-1}k_{i})!} p_{1}^{k_{1}}\ldots p_{m-1}^{k_{m-1}}p_{m}^{r-\sum_{i=1}^{m-1}k_{i}},
$$
if $k_{i}\ge 0$ are integers such that $k_{1}+\ldots +k_{m-1}\le r$. One situation where this distribution arises is when $r$ balls are randomly placed in $m$ bins, with each ball going into the $j$th bin with probability $p_{j}$, and we look at the random vector $(X_{1},\ldots ,X_{m-1})$ where $X_{k}$ is the number of balls that fell into the $k$th bin. This random vector has the multinomial pmf\footnote{In some books, the distribution of $(X_{1},\ldots ,X_{m})$ is called the multinomial distribution. This has the pmf $$g(k_{1},\ldots ,k_{m})\frac{r!}{k_{1}!k_{2}!\ldots k_{m-1}!k_{m}!} p_{1}^{k_{1}}\ldots p_{m-1}^{k_{m-1}}p_{m}^{k_{m}}$$ where $k_{i}$ are non-negative integers such that $k_{1}+\ldots +k_{m}=r$. We have chosen our convention so that the binomial distribution is a special case of the multinomial\dots }

In this case, the marginal distribution of $X_{k}$ is $\mbox{Bin}(r,p_{k})$. More generally, $(X_{1},\ldots ,X_{\ell})$ has multinomial distribution with parameters $r,\ell,p_{1},\ldots ,p_{\ell},p_{0}$ where $p_{0}=1-(p_{1}+\ldots +p_{\ell})$. This is easy to prove, but even easier to see from the balls in bins interpretation (just think of the last $n-\ell$ bins as one).
\end{example}

\begin{example} (Bivariate normal distribution). This is the density on $\mathbb{R}^{2}$ given by
$$
f(x,y)=\frac{\sqrt{ab-c^{2}}}{2\pi}e^{-\frac{1}{2}\left[a(x-\mu)^{2}+b(y-\nu)^{2}+2c(x-\mu)(y-\nu) \right]},
$$
where $\mu,\nu,a,b,c$ are real parameters. We shall impose the conditions that $a>0$, $b>0$ and $ab-c^{2}>0$ (otherwise the above does not give a density, as we shall see).

The first thing is to check that this is indeed a density. We recall the one-dimensional Gaussian integral 
\begin{equation}\label{eq:onedimgaussian}
\int\limits_{-\infty}^{+\infty}e^{-\frac{\tau}{2}(x-a)^{2}}dx = \sqrt{2\pi}\frac{1}{\sqrt{\tau}} \mbox{ for any }\tau>0 \mbox{ and any }a\in \mathbb{R}.
\end{equation}
We shall take $\mu=\nu=0$ (how do you compute the integral  if they are not?). Then, the exponent in the density has the form
$$
ax^{2}+by^{2}+2cxy = b\left(y+\frac{c}{b}\right)^{2}+\left(a-\frac{c^{2}}{b}\right)x^{2}.
$$
Therefore, 
\begin{align*}
\int\limits_{-\infty}^{\infty}e^{-\frac{1}{2}\left[ax^{2}+by^{2}+2cxy \right]} dy &= e^{-\frac{1}{2}(a-\frac{c^{2}}{b})x^{2}} \int\limits_{-\infty}^{\infty}e^{-\frac{b}{2}(y+\frac{c}{b})^{2}} \\
&= e^{-\frac{1}{2}(a-\frac{c^{2}}{b})x^{2}}\frac{\sqrt{2\pi}}{\sqrt{b}}
\end{align*}
by \eqref{eq:onedimgaussian} but ony if $b>0$. Now we integrate over $x$ and use \eqref{eq:onedimgaussian} again (and the fact that $a-\frac{c^{2}}{b}>0$) to get
\begin{align*}
\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}e^{-\frac{1}{2}\left[a(x-\mu)^{2}+b(y-\nu)^{2}+2c(x-\mu)(y-\nu) \right]}dydx &= \frac{\sqrt{2\pi}}{\sqrt{b}} \int\limits_{-\infty}^{\infty} e^{-\frac{1}{2}(a-\frac{c^{2}}{b})x^{2}}dx \\
&= \frac{\sqrt{2\pi}}{\sqrt{b}} \frac{\sqrt{2\pi}}{\sqrt{a-\frac{c^{2}}{b}}} = \frac{2\pi}{ab-c^{2}}.
\end{align*}
This completes the proof that $f(x,y)$ is indeed a density. Note that $b>0$ and $ab-c^{2}>0$ also implies that $a>0$.
\end{example}
%\begin{example} (Multivariate normal distribution). Let $\mu$ be an $m\times 1$ vector and let $\Sigma$ be an $m\times m$ positive definite matrix. Then, define
%$$
%f(\x) = \frac{1}{(2\pi)^{m/2}\sqrt{\det(\Sigma)}} \exp\left\{ -\frac{1}{2}(\x-\mu)^{t}\Sigma^{-1}(\x-\mu)\right\}.
%$$
%This is a density on $\mathbb{R}^{m}$ and is called the Multivariate normal density with mean vector $\mu$ and covariance matrix $\Sigma$. This can be seen from the multivariate change of variables formula (introduced later). Further, it is a fact that the marginal distribution of $X_{k}$ if $N(\mu_{k},\sigma_{k,k})$.
%\end{example}

%\para{More on the bivariate normal distribution} Let us explain the special case of bivariate normal distribution in greater detail. 

%\para{The univariate normal} Let $g:\mathbb{R}\rightarrow \mathbb{R}_{+}$ be defined as $g(x)=e^{-x^{2}/2\tau}$. Then, $
%\int_{-\infty}^{+\infty}g(x)dx$ is finite if and only if $\tau>0$ and in that case the integral is equal to $\sqrt{2\pi \tau}$. Thus, writing $\tau={\sigma}^{2}$ for some ${\sigma}>0$ we can define the probability density  $f(x)=\frac{1}{{\sigma}\sqrt{2\pi}}e^{-x^{2}/2{\sigma}^{2}}$. We can further take any $\mu\in \mathbb{R}$ and define a density $\varphi(x):=\frac{1}{{\sigma}\sqrt{2\pi}}e^{-(x-\mu)^{2}/2{\sigma}^{2}}$ (the integral will not change as is easily seen by substituting $u=x-\mu$). This is what we called the normal density with parameters $\mu$ and ${\sigma}^{2}$. We thus get the following formula which will be frequently used below.
%$$
%\int\limits_{-\infty}^{+\infty}e^{-\frac{\tau}{2}(x-a)^{2}}dx = \sqrt{2\pi}\frac{1}{\sqrt{\tau}} \mbox{ for any }\tau>0 \mbox{ and any }a\in \mathbb{R}.
%$$

%\para{The isotropic bivariate normal density} Define $g:\mathbb{R}^{2}\rightarrow \mathbb{R}_{+}$ by $g(x,y)=\frac{1}{2\pi}e^{-(x^{2}+y^{2})/2}$. It is easy to see that 
%$$
%\int\limits_{-\infty}^{+\infty}\!\!\!\int\limits_{-\infty}^{+\infty} \frac{1}{2\pi}e^{-(x^{2}+y^{2})/2}dy dx = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}e^{-x^{2}/2}\left(\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{+\infty} e^{-y^{2}/2}dy \right)dx  = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}e^{-x^{2}/2} dx =1.
%$$
%All we used in each step was the single variable Gaussian integral $\int_{-\infty}^{+\infty} e^{-x^{2}/2\tau}dx=\sqrt{2\pi}\sqrt{\tau}$. Thus, $g(x,y)$ is a density in $\mathbb{R}^{2}$.

%\para{An anisotropic bivariate Gaussian} Fix $a,b\in \mathbb{R}$ and consider the function $g(x,y)=e^{-(ax^{2}+by^{2})/2}$. Then
%$$
%\int\limits_{-\infty}^{+\infty}\!\!\!\int\limits_{-\infty}^{+\infty}g(x,y) dydx =  \int\limits_{-\infty}^{+\infty}e^{-ax^{2}/2}\left(\int\limits_{-\infty}^{+\infty} e^{-by^{2}/2}dy \right)dx  = \sqrt{\frac{2\pi}{b}}\int\limits_{-\infty}^{+\infty}e^{-ax^{2}/2} dx =\frac{2\pi}{\sqrt{ab}}.
%$$
%Observe that for the validity of these integrals we must have $b>0$ and $a>0$. Thus, $\frac{\sqrt{ab}}{2\pi}e^{-(ax^{2}+by^{2})/2}$ is a density in $\mathbb{R}^{2}$ for any strictly positive numbers $a,b$.

%\para{The general bivariate Gaussian density} Now we let $g(x,y)=e^{-(ax^{2}+by^{2}+2cxy)/2}$ where $a,b,c\in \mathbb{R}$. Again we want to figure out when it is possible to normalize this to a density and how.
%\begin{align*}
%\int\limits_{-\infty}^{+\infty}\!\!\!\int\limits_{-\infty}^{+\infty}g(x,y) dydx &= \int\limits_{-\infty}^{+\infty} e^{-ax^{2}/2}\left(\int\limits_{-\infty}^{+\infty} e^{-\frac{by^{2}+2cxy}{2}}dy \right)dx.
%\end{align*}
%To compute the inner integral we complete squares and write $by^{2}+2cxy = b(y+\frac{cx}{b})^{2}-\frac{x^{2}c^{2}}{b}$. Therefore
%$$
%\int\limits_{-\infty}^{+\infty} e^{-\frac{by^{2}+2cxy}{2}}dy = e^{\frac{x^{2}c^{2}}{2b}}\int\limits_{-\infty}^{+\infty} e^{-\frac{1}{2}b(y+\frac{cx}{b})^{2}}dy = e^{\frac{x^{2}c^{2}}{2b}}\sqrt{\frac{2\pi}{b}}
%$$
%provided $b>0$ (the integral is infinite if $b<0$). Consequently, we get
% \begin{align*}
%\int\limits_{-\infty}^{+\infty}\!\!\!\int\limits_{-\infty}^{+\infty}g(x,y) dydx &= \int\limits_{-\infty}^{+\infty} e^{-\frac{a}{2}x^{2}}e^{\frac{c^{2}}{2b}x^{2}} = \int\limits_{-\infty}^{+\infty} e^{-\frac{1}{2}(a-\frac{c^{2}}{b})x^{2}}dx = \frac{\sqrt{2\pi}}{\sqrt{a-\frac{c^{2}}{b}}}
%\end{align*}
%provided $a-\frac{c^{2}}{b}>0$. Thus,
%$$
%\int\limits_{-\infty}^{+\infty}\!\!\!\int\limits_{-\infty}^{+\infty}g(x,y) dydx = \frac{2\pi}{\sqrt{b}\sqrt{a-\frac{c^{2}}{b}}} = \frac{2\pi}{\sqrt{ab-c^{2}}} 
%$$
%if $b>0$ and $a-\frac{c^{2}}{b}>0$. These two conditions can be re-written as the three conditions $a>0,b>0,ab-c^{2}>0$. Under these conditions,
%$$
%f(x,y)=\frac{\sqrt{ab-c^{2}}}{2\pi}e^{-\frac{1}{2}(ax^{2}+by^{2}+2cxy)}
%$$
%is a density in $\mathbb{R}^{2}$.

\para{Matrix form of writing the density} Let $\Sigma^{-1}=\left[\begin{array}{cc} a & c \\ c & b \end{array}\right]$. Then, $\det(\Sigma)=\frac{1}{\det(\Sigma^{-1})}=\frac{1}{ab-c^{2}}$. Hence, we may re-write the density above as (let $\u$ be the column vector with co-ordinates $x,y$)
$$
f(x,y) = \frac{1}{2\pi \sqrt{\det(\Sigma)}} e^{-\frac{1}{2}\u^{t}\Sigma^{-1}\u }.
$$
This is precisely in the form in which we wrote for general $n$ in the example earlier. The conditions $a>0,b>0,ab-c^{2}>0$ translate precisely to what is called positive-definiteness. One way to say it is that $\Sigma$ is a symmetric matrix and all its eigenvalues are strictly positive.

\para{Final form} We can now introduce an extra pair of parameters $\mu_{1},\mu_{2}$ and define a density
$$
f(x,y) = \frac{1}{2\pi \sqrt{\det(\Sigma)}} e^{-\frac{1}{2}(\u-\mu)^{t}\Sigma^{-1}(\u-\mu) }.
$$
where $\mu$ is a column vector with co-ordinates $\mu_{1},\mu_{2}$. This is the full bi-variate normal density.

\begin{example} (A class of examples). Let $f_{1},f_{2},\ldots ,f_{m}$ be one-variable densities. In other words, $f_{i}:\mathbb{R}\rightarrow \mathbb{R}_{+}$ and $\int_{-\infty}^{\infty}f_{i}(x)dx=1$. Then, we can make a multivariate density as follows. Define $f:\mathbb{R}^{m}\rightarrow \mathbb{R}_{+}^{m}$ by $f(x_{1},\ldots ,x_{m})=f_{1}(x_{1})\ldots f_{m}(x_{m})$. Then $f$ is a density.

If $X_{i}$ are random variables on a common probability space and the joint density of $(X_{1},\ldots ,X_{m})$  if $f(x_{1},\ldots ,x_{m})$, then we say that $X_{i}$ are {\em independent random variables}. It is easy to see that the marginal density of $X_{i}$ if $f_{i}$. It is also the case that the joint CDF factors as $F_{X}(x_{1},\ldots ,x_{m})=F_{X_{1}}(x_{1})\ldots F_{X_{m}}(x_{m})$. 
\end{example}
\section{Change of variable formula}
Let ${\bf X}=(X_{1},\ldots ,X_{m})$ be a random vector with density $f(t_{1},\ldots ,t_{m})$. Let $T:\mathbb{R}^{m}\rightarrow \mathbb{R}^{m}$ be a one-one function which is continuously differentiable (many exceptions can be made as remarked later). 

Let ${\bf Y}=T({\bf X})$. In co-ordinates we may write ${\bf Y}=(Y_{1},\ldots ,Y_{m})$ and $Y_{1}=T_{1}(X_{1},\ldots ,X_{m})$\dots $Y_{m}=T_{m}(X_{1},\ldots ,X_{m})$ where $T_{i}:\mathbb{R}^{m}\rightarrow \mathbb{R}$ are the components of $T$. 

\para{Question} What is the joint density of $Y_{1},\ldots ,Y_{m}$?

\para{The change of variable formula} In the setting described above,  the joint density of $Y_{1},\ldots ,Y_{m}$ is given by
$$
g({\bf y})=f\left(T^{-1}{\bf y}\right)\left.\vphantom{\hbox{\large (}}\right| J[T^{-1}]({\bf y})\left.\vphantom{\hbox{\large (}}\right|
$$
where $J[T^{-1}]({\bf y})$ is the Jacobian determinant of the function $T^{-1}$ at the point ${\bf y}=(y_{1},\ldots ,y_{m})$.

\para{Justification} We shall not prove this formula, but give a imprecise but convincing justification that can be made into a proof. There are two factors on the right.  The first one, $f(T^{-1}{\bf y})$ is easy to understand - if ${\bf Y}$ is to be close to ${\bf y}$, then ${\bf X}$ must be close to $T^{-1}{\bf y}$. The second factor involving the Jacobian determinant comes from the volume change. Let us explain with analogy with mass density which is a more familiar quantity.

Consider a solid cube with non-uniform density. If you rotate it, the density at any point now is the same as the original density, but at a different point (the one which came to the current position). Instead of rotating, suppose we uniformly expand the cube so that the center stays where it is and the side of the cube becomes twice what it is. What happens to the density at the center? It goes down by a factor of $8$. This is simply because of volume change - the same mass spreads over a larger volume. More generally, we can have non-uniform expansion, we may cool some parts of the cube, heat some parts and to varying degrees. What happens to the density? At each point, the density changes by a factor given by the Jacobian determinant.

Now for a slightly more mathematical justification. We use the language for two variables ($m=2$) but the same reasoning works for any $m$. Fix twp point $\x=(x_{1},x_{2})$ and $\y=(y_{1},y_{2})$ such that $\y=T(\x)$ (and hence $\x=T^{-1}(\y)$). The density of ${\bf Y}$ at $\y$ is given by 
\begin{align*} 
g(\y)&\approx \frac{1}{\mbox{area}(\mathcal N)}\mathbf{P}\{{\bf Y}\in \mathcal N\}
%f(\x)&\approx \frac{1}{\mbox{area}(\mathcal N)}\mathbf{P}\{{\bf X}\in \mathcal N\}
\end{align*}
where $\mathcal N$ is a small neighbourhood of the point $\y$ (for example a disk of small radius $\delta$ centered at $\y$). By the one-one nature of $T$ and the relationship ${\bf Y}=T({\bf X})$, we see that $$\mathbf{P}\{{\bf Y}\in \mathcal N\}=\mathbf{P}\{{\bf X}\in T^{-1}(\mathcal N)\}$$ where $T^{-1}(\mathcal N)$ is the image of $\mathcal N$ after mapping by $T^{-1}$.  Now, $T^{-1}(\mathcal N)$ is a small neighbourhood of $\x$ (if $\mathcal N$ is a disk, then $T^{-1}(\mathcal N)$ would be an approximate ellipse) and hence, by the same interpretation of density we see that
\begin{align*} 
\mathbf{P}\{{\bf X}\in T^{-1}(\mathcal N)\}&\approx \mbox{area}(T^{-1}(\mathcal N))f(\x)
%f(\x)&\approx \frac{1}{\mbox{area}(\mathcal N)}\mathbf{P}\{{\bf X}\in \mathcal N\}
\end{align*}
Putting the three displayed equations together, we arrive at the formula
$$
g(\y)\approx f(\x)\frac{\mbox{area}(T^{-1}(\mathcal N))}{\mbox{area}(\mathcal N)}
$$
Thus the problem boils down to how areas change under transformations. A linear map $S(\y)=A\y$ where $A$ is a $2\times 2$ matrix changes area of any region by a factor of $|\det(A)|$, i.e., $\mbox{area}(S(\mathcal R))=|\det(A)|\mbox{area}(\mathcal R)$. 

The differentiability of $T$ means that in a small neighbourhood of $\y$, the mapping $T^{-1}$ looks like a linear map, $T^{-1}(\y+\mathbf h)\approx \x+DT^{-1}(\y){\bf h}$. Therefore, the areas of small neighbourhoods of $\y$ change by a factor equal to $|\det(DT^{-1}(\y))|$ which is the Jacobian determinant. In other words, $\mbox{area}(T^{-1}(\mathcal N))\approx |JT^{-1}(\y)|\mbox{area}(\mathcal N)$. Consequently $g(\y)=f(T^{-1}\y)|JT^{-1}(\y)|$.


\para{Enlarging the applicability of the change of variable formula} The change of variable formula is applicable in greater generality than we stated above. 
\begin{enumerate}\setlength\itemsep{6pt}
\item Firstly, $T$ does not have to be defined on all of $\mathbb{R}^{m}$. It is sufficient if it is defined on the range of ${\bf X}$ (i.e., if $f(t_{1},\ldots ,t_{m})=0$ for $(t_{1},\ldots ,t_{m})=\mathbb{R}^{m}\setminus A$, then it is enough if $T$ is defined on $A$.
\item Even within the range of ${\bf X}$, we can allow $T$ to be undefined, but ${\bf X}$ must have zero probability to fall in the set where it is undefined. For example, it can happen at finitely many points, or on a line (if $m\ge 2$) or on a plane (if $m\ge 3$) etc.
\item Similarly, the differentiability of $T$ is required only on a subset outside of which ${\bf X}$ has probability $0$ of falling.
\item One-one property of $T$ is important, but there are  special cases which can be dealt with by a slight modification. For example, if  $T(x)=x^{2}$ or $T(x_{1},x_{2})=(x_{1}^{2},x_{2}^{2})$ where we can split the space into parts on each of which $T$ is one-one.

\end{enumerate}


\begin{example} Let $X_{1},X_{2}$ be independent Exp($\lambda$) random variables. Let $T(x_{1},x_{2})=(x_{1}+x_{2},\frac{x_{1}}{x_{1}+x_{2}})$. This is well-defined on $\mathbb{R}_{+}^{2}$ (and note that $\mathbf{P}\{(X_{1},X_{2})\in \mathbb{R}_{+}^{2}\}=1$) and its range is $\mathbb{R}_{+}\times (0,1)$. The inverse function is $T^{-1}(y_{1},y_{2})=(y_{1}y_{2},y_{1}(1-y_{2}))$. Its Jacobian determinant is
$$
J[T^{-1}](y_{1},y_{2})=\det\left[\begin{array}{cc}y_{2} & y_{1}\\ 1-y_{2} & -y_{1} \end{array}\right]=-y_{1}.
$$
$(X_{1},X_{2})$ has density $f(x_{1},x_{2})=\lambda^{2}e^{-\lambda(x_{1}+x_{2})}$ for $x_{1},x_{2}>0$ (henceforth it will be a convention that the density is zero except where we specify it).
Hence, the random variables $Y_{1}=X_{1}+X_{2}$ and $Y_{2}=\frac{X_{1}}{X_{1}+X_{2}}$ have joint density
$$
g(y_{1},y_{2})=f(y_{1}y_{2},y_{1}(1-y_{2}))|J[T^{-1}](y_{1},y_{2})| = \lambda^{2}e^{-\lambda(y_{1}y_{2}+y_{1}(1-y_{2}))}y_{1}=\lambda^{2}y_{1}e^{-\lambda y_{1}}
$$
for $y_{1}>0$ and $y_{2}\in (0,1)$. 

In particular, we see that $Y_{1}=X_{1}+X_{2}$ has density $h_{1}(t)=\int_{0}^{1}\lambda^{2}te^{-\lambda t}ds = \lambda^{2}te^{-\lambda t}$ (for $t>0$)  which means that $Y_{1}\sim \mbox{Gamma}(2,\lambda)$. Similarly, $Y_{2}=\frac{X_{1}}{X_{1}+X_{2}}$ has density $h_{2}(s)=\int_{0}^{\infty} \lambda^{2}t e^{-\lambda t}dt =1$ (for $s\in (0,1)$) which means that $Y_{2}$ has $\mbox{Unif}(0,1)$ distribution. In fact, $Y_{1}$ and $Y_{2}$ are also independent since $g(u,v)=h_{1}(u)h_{2}(v)$.
\end{example}

\begin{exercise} Let $X_{1}\sim \mbox{Gamma}(\nu_{1},\lambda)$ and $X_{2}\sim \mbox{Gamma}(\nu_{2},\lambda)$ (note that the shape parameter is the same) and assume that they are independent. Find the joint distribution of $X_{1}+X_{2}$ and $\frac{X_{1}}{X_{1}+X_{2}}$.
\end{exercise}

\begin{example} Suppose we are given that $X_{1}$ and $X_{2}$ are independent and each has $\mbox{Exp}(\lambda)$ distribution. What is the distribution of the random variable $X_{1}+X_{2}$? 

The change of variable formula works for transformations from $\mathbb{R}^{m}$ to $\mathbb{R}^{m}$ whereas here we have two random variables $X_{1},X_{2}$ and our interest is in one random variable $X_{1}+X_{2}$. To use the change of variable formula, we must introduce an {\em auxiliary} variable. For example, we take $Y_{1}=X_{1}+X_{2}$ and $Y_{2}=X_{1}/(X_{1}+X_{2})$. Then as in the first example, we find the joint density of $(Y_{1},Y_{2})$ using the change of variable formula and then integrate out the second variable to get the density of $Y_{1}$. 

Let us emphasize the point that if our interest is only in $Y_{1}$, then we have a lot of freedom in choosing the auxiliary variable. The only condition is that from $Y_{1}$ and $Y_{2}$ we should be able to recover $X_{1}$ and $X_{2}$. Let us repeat the same using $Y_{1}=X_{1}+X_{2}$ and $Y_{2}=X_{2}$. Then, $T(x_{1},x_{2})=(x_{1}+x_{2},x_{2})$ maps $\mathbb{R}_{+}^{2}$ onto $Q:=\{(y_{1},y_{2}){\; : \;} y_{1}>y_{2}>0\}$ in a one-one manner. The inverse function is $T^{-1}(y_{1},y_{2})=(y_{1}-y_{2},y_{2})$. It is easy to see that $JT^{-1}(y_{1},y_{2})=1$ (check!). Hence,  by the change of variable formula, the density of $(Y_{1},Y_{2})$ is given by 
\begin{align*}
g(y_{1},y_{2}) &= f(y_{1}-y_{2},y_{2})\cdot 1 \\
&= \lambda^{2}e^{-\lambda(y_{1}-y_{2})}e^{-\lambda y_{2}} \hspace{2mm}(\mbox{if }y_{1}>y_{2}>0) \\
&= \lambda^{2}e^{-\lambda y_{1}}{\mathbf 1}_{y_{1}>y_{2}>0}.
\end{align*}
To get the density of $Y_{1}$, we integrate out the second variable. The density of $Y_{1}$ is 
\begin{align*}
h(u) &= \int\limits_{-\infty}^{\infty}\lambda^{2}e^{-\lambda y_{1}}{\mathbf 1}_{y_{1}>y_{2}>0} dy_{2} \\
&= \lambda^{2}e^{-\lambda y_{1}}\int\limits_{0}^{y_{1}}dy_{2} \\
&= \lambda^{2}y_{1}e^{-\lambda y_{1}}
\end{align*}
which agrees with what we found before. 
\end{example}

\begin{example} Suppose $R\sim \mbox{Exp}(\lambda)$ and $\Theta\sim \mbox{Unif}(0,2\pi)$ and the two are independent. Define $X=\sqrt{R}\cos(\Theta)$ and $Y=\sqrt{R}\sin(\Theta)$. We want to find the distribution of $(X,Y)$. For this, we first write the joint density of $(R,\Theta)$ which is given by 
$$
f(r,\theta)=\frac{1}{2\pi}\lambda e^{-\lambda r} \hspace{2mm}\mbox{ for }r>0,\theta\in (0,2\pi).
$$
Define the transformation $T:\mathbb{R}_{+}\times (0,2\pi)\rightarrow \mathbb{R}^{2}$ by $T(r,\theta)=(\sqrt{r}\cos\theta,\sqrt{r}\sin \theta)$. The image of $T$ consists of all $(x,y)\in \mathbb{R}^{2}$ with $y\not=0$. The inverse is $T^{-1}(x,y)=(x^{2}+y^{2},\arctan(y/x))$ where $\arctan(y/x)$ is defined so as to take values in $(0,\pi)$ when $y>0$ and to take values in $(\pi,2\pi)$ when $y<0$. Thus
$$
JT^{-1}(x,y) = \det\matrices{2x}{2y}{\frac{-y}{x^{2}+y^{2}}}{\frac{x}{x^{2}+y^{2}}}=2.
$$
Therefore, $(X,Y)$ has joint density 
$$
g(x,y) = 2f(x^{2}+y^{2},\arctan(y/x)) = \frac{\lambda}{\pi} e^{-\lambda(x^{2}+y^{2})}.
$$
This is for $(x,y)\in \mathbb{R}^{2}$ with $y\not=0$, but as we have remarked earlier, the value of a pdf in $\mathbb{R}^{2}$ on a line does not matter, we may define $g(x,y)$ as above for all $(x,y)$ (main point is that the CDF does not change). Since $g(x,y)$ separates into a function of $x$ and a function of $y$, $X,Y$ are independent $N(0,\frac{1}{2\lambda})$. 
\end{example}
\begin{remark} Relationships between random variables derived by the change of variable formulas can be used for simulation too. For instance, the CDF of $N(0,1)$ is not explicit and hence simulating from that distribution is difficult (must resort to numerical methods). However, we can easily simulate it as follows. Simulate an $\mbox{Exp}(1/2)$ random variable $R$ (easy, as the distribution function can be inverted) and simulate an independent $\mbox{Unif}(0,2\pi)$ random variable $\Theta$. Then set $X=\sqrt{R}\cos(\Theta)$ and $Y=\sqrt{R}\sin(\Theta)$. These are two independent $N(0,1)$ random numbers. Here it should be noted that the random numbers in $(0,1)$ given by a random number generator are supposed to be independent uniform random numbers (otherwise, it is not acceptable as a random number generator).
\end{remark}


\section{Independence and conditioning of random variables}
\begin{definition} Let ${\bf X}=(X_{1},\ldots ,X_{m})$ be a random vector (this means that $X_{i}$ are random variables on a common probability space). We say that $X_{i}$ are {\em independent} if $F_{{\bf X}}(t_{1},\ldots ,t_{m})=F_{1}(t_{1})\ldots F_{m}(t_{m})$ for all $t_{1},\ldots ,t_{m}$. 
\end{definition}
\begin{remark} Recalling the definition of independence of events, the equality $F_{{\bf X}}(t_{1},\ldots ,t_{m})=F_{1}(t_{1})\ldots F_{m}(t_{m})$ is just saying that the events $\{X_{1}\le t_{1}\}, \ldots \{X_{m}\le t_{m}\}$ are independent. More generally, it is true that $X_{1},\ldots ,X_{m}$ are independent if and only if $\{X_{1}\in A_{1}\},\ldots ,\{X_{m}\in A_{m}\}$ are independent events for any $A_{1},\ldots, A_{m}\subseteq \mathbb{R}$.
\end{remark}

\begin{remark} In case $X_{1},\ldots ,X_{m}$ have a joint pmf or a joint pdf (which we denote by $f(t_{1},\ldots ,t_{m})$), the condition for independence is equivalent to 
$$
f(t_{1},\ldots ,t_{m})=f_{1}(t_{1})f_{2}(t_{2})\ldots f_{m}(t_{m})
$$ 
where $f_{i}$ is the marginal density (or pmf)  of $X_{i}$. This fact can be derived from the definition easily. For example, in the case of densities, observe that 
\begin{align*}
f(t_{1},\ldots ,t_{m}) &= \frac{\partial^{m}}{\partial t_{1}\ldots \partial t_{m}}F(t_{1},\ldots,t_{m}) \hspace{4mm}(\mbox{true for any joint density})\\
 &= \frac{\partial^{m}}{\partial t_{1}\ldots \partial t_{m}}F_{1}(t_{1})\ldots F_{m}(t_{m}) \hspace{4mm}(\mbox{by independence}) \\
 &= F_{1}'(t_{1})\ldots F_{m}'(t_{m}) \\
 &=f_{1}(t_{1})\ldots f_{m}(t_{m}).
\end{align*}
\end{remark}
When we turn it around, this gives us a quicker way to check independence.

\para{Fact} Let $X_{1},\ldots ,X_{m}$ be random variables with joint pdf $f(t_{1},\ldots ,t_{m})$. Suppose we can write this pdf as $f(t_{1},\ldots ,t_{m})=cg_{1}(t_{1})g_{2}(t_{2})\ldots g_{m}(t_{m})$ where $c$ is a constant and $g_{i}$ are some functions of one-variable. Then,  $X_{1},\ldots ,X_{m}$ are independent. Further, the marginal density of $X_{k}$ is $c_{k}g_{k}(t)$ where $c_{k}=\frac{1}{\int_{-\infty}^{+\infty}g_{k}(s)ds}$. An analogous statement holds when $X_{1},\ldots ,X_{m}$ have a joint pmf instead of pdf.



\begin{example} Let $\Omega=\{0,1\}^{n}$ with $p_{\underline{\ome}}=p^{\sum \omega_{k}}q^{n-\sum \omega_{k}}$. Define $X_{k}:\Omega\rightarrow \mathbb{R}$ by $X_{k}(\underline{\ome})=\omega_{k}$. In words, we are considering the probability space corresponding to $n$ tosses of a fair coin and $X_{k}$ is the result of the $k$th toss. We claim that $X_{1},\ldots ,X_{n}$ are independent. Indeed, the joint pmf of $X_{1},\ldots ,X_{n}$ is 
$$
f(t_{1},\ldots ,t_{n})=p^{\sum t_{k}}q^{n-\sum t_{k}} \hspace{3mm} \mbox{ where }t_{i}=0\mbox{ or }1 \mbox{ for each }i\le n.
$$
Clearly $f(t_{1},\ldots ,t_{m})=g(t_{1})g(t_{2})\ldots g(t_{n})$ where $g(s)=p^{s}q^{1-s}$ for $s=0\mbox{ or }1$ (this is just a terse way of saying that $g(s)=p$ if $s=1$ and $g(s)=q$ if $s=0$). Hence $X_{1},\ldots ,X_{n}$ are independent and $X_{k}$ has pmf $g$ (i.e., $X_{k}\sim \mbox{Ber}(p)$).
\end{example}

\begin{example} Let $(X,Y)$ have the bivariate normal density 
$$
f(x,y)=\frac{\sqrt{ab-c^{2}}}{\sqrt{2\pi}}e^{-\frac{1}{2}(a(x-\mu_{1})^{2}+b(y-\mu_{2})^{2}+2c(x-\mu_{1})(y-\mu_{2}))}.
$$
If $c=0$, we observe that 
$$
f(x,y) = C_{0} e^{-\frac{a(x-\mu_{1})^{2}}{2}}e^{-\frac{b(y-\mu_{2})^{2}}{2}} \qquad (C_{0}\mbox{ is a constant, exact value unimportant})
$$
from which we deduce that $X$ and $Y$ are independent and $X\sim N(\mu_{1},\frac{1}{a})$ while $Y\sim N(\mu_{2},\frac{1}{b})$.

Can you argue that if $c\not=0$, then $X$ and $Y$ are not independent?
\end{example}


\begin{example} Let $(X,Y)$ be a random vector with density $f(x,y)=\frac{1}{\pi}{\mathbf 1}_{x^{2}+y^{2}\le 1}$ (i.e., it equals $1$ if $x^{2}+y^{2}\le 1$ and equals  $0$ otherwise). This corresponds to picking a point at random from the disk of radius $1$ centered at $(0,0)$. We claim that $X$ and $Y$ are not independent. A quick way to see this is that if $I=[0.8,1]$, then $\mathbf{P}\{(X,Y)\in [0.8,1]\times [0.8,1]\}=0$ whereas $\mathbf{P}\{(X,Y)\in [0.8,1]\}\mathbf{P}\{(X,Y)\in [0.8,1]\}\not= 0$ (If $X,Y$ were independent, we must have had $\mathbf{P}\{(X,Y)\in [a,b]\times [c,d]\}=\mathbf{P}\{X\in [a,b]\}\mathbf{P}\{Y\in [c,d]\}$ for any $a<b$ and $c<d$).
\end{example}

A very useful (and intuitively acceptable!) fact about independence is as follows.

\para{Fact} Suppose $X_{1},\ldots ,X_{n}$ are independent random variables. Let $k_{1}<k_{2}<\ldots <k_{m}=n$. Let $Y_{1}=h_{1}(X_{1},\ldots ,X_{k_{1}})$, $Y_{2}=h_{2}(X_{k_{1}+{1}},\ldots ,X_{k_{2}}), \ldots Y_{m}=h_{m}(X_{k_{m-1}},\ldots ,X_{k_{m}})$. Then, $Y_{1},\ldots ,Y_{m}$ are also independent.


\begin{remark} In the previous section we defined independence of events and now we have defined independence of random variables. How are they related? We leave it to you to check that events $A_{1},\ldots ,A_{n}$ are independent (according the definition of the previous section) if and only if the random variables ${\mathbf 1}_{A_{1}},\ldots ,{\mathbf 1}_{A_{m}}$ are independent (according the definition of this section)
\end{remark}


{\color{magenta}
\para{Conditioning on random variables}\footnote{This part was not covered in class and may be safely omitted .} 
Let $X_{1},\ldots ,X_{k+\ell}$ be random variables on a common probability space. Let $f(t_{1},\ldots ,t_{k+\ell})$ be the pmf of $(X_{1},\ldots ,X_{k+\ell})$ and let $g(t_{1},\ldots ,t_{\ell})$ be the pmf of $(X_{k+1},\ldots ,X_{k+\ell})$ (of course we can compute $g$ from $f$ by summing over the first $k$ indices). Then, for any $s_{1},\ldots ,s_{\ell}$ such that $\mathbf{P}\{X_{k+1}=s_{1},\ldots X_{m}=s_{\ell}\}>0$, we can define
\ben\label{eq:conditionalpmf}
h_{s_{1},\ldots ,s_{\ell}}(t_{1},\ldots,t_{k})=\mathbf{P}\{X_{1}= t_{1},\ldots ,X_{k}= t_{k}\left.\vphantom{\hbox{\Large (}}\right| X_{k+1}=s_{1},\ldots X_{m}=s_{\ell}\}=\frac{f(t_{1},\ldots ,t_{k},s_{1},\ldots ,s_{\ell})}{g(s_{1},\ldots ,s_{\ell})}.
\een
It is easy to see that $h_{s_{1},\ldots,s_{\ell}}(\cdot)$ is a pmf on $\mathbb{R}^{k}$. It is called the conditional pmf of $(X_{1},\ldots ,X_{k})$ given that $X_{k+1}=s_{1},\ldots X_{m}=s_{\ell}$. 

Its interpretation is as follows. Originally we had random observables $X_{1},\ldots ,X_{k}$ which had a certain joint pmf. Then we observe the values of the random variables $X_{k+1},\ldots ,X_{k+\ell}$, say they turn out to be $s_{1},\ldots ,s_{\ell}$, respectively. Then we update the distribution (or pmf) of $X_{1},\ldots ,X_{k}$ according to the above recipe. The conditional pmf is the new function $h_{s_{1},\ldots,s_{\ell}}(\cdot)$.

\begin{exercise} Let $(X_{1},\ldots ,X_{n-1})$ be a random vector with multinomial distribution with parameters $r,n,p_{1},\ldots ,p_{n}$. Let $k<n-1$. Given that $X_{k+1}=s_{1},\ldots ,X_{n-1}=s_{n-k+1}$, show that the conditional distribution of $(X_{1},\ldots ,X_{k})$ is multinomial with parameters $r',n'$, $q_{1},\ldots ,q_{k+1}$ where $r'=r-(s_{1}+\ldots +s_{n-k+1})$, $n'=k+1$, $q_{j}=p_{j}/(p_{1}+\ldots +p_{k}+p_{n})$ for $j\le k$ and $q_{k+1}=p_{n}/(p_{1}+\ldots +p_{k}+p_{n})$.

This looks complicated, but is utterly obvious if you think in terms of assigning $r$ balls into $n$ urns by putting each ball into the urns with probabilities $p_{1},\ldots ,p_{n}$ and letting $X_{j}$ denote the number of balls that end up in the $j^{\mbox{\tiny th}}$ urn.
\end{exercise}

\parag{Conditional densities} Now suppose $X_{1},\ldots ,X_{k+\ell}$ have joint density $f(t_{1},\ldots ,t_{k+\ell})$ and let $g(s_{1},\ldots ,s_{\ell})$ by the density of $(X_{k+1},\ldots ,X_{k+\ell})$. Then, we define the conditional density of $(X_{1},\ldots ,X_{k})$ given $X_{k+1}=s_{1},\ldots ,X_{k+\ell}=s_{\ell}$ as
\ben\label{eq:conditionalpdf}
h_{s_{1},\ldots ,s_{\ell}}(t_{1},\ldots,t_{k})=\frac{f(t_{1},\ldots ,t_{k},s_{1},\ldots ,s_{\ell})}{g(s_{1},\ldots ,s_{\ell})}.
\een
This is well-defined whenever $g(s_{1},\ldots ,s_{\ell})>0$.

\begin{remark} Note the difference between \eqref{eq:conditionalpmf} and \eqref{eq:conditionalpdf}. In the latter we have left out the middle term because $\mathbf{P}\{X_{k+1}=s_{1},\ldots ,X_{k+\ell}=s_{\ell}\}=0$. In \eqref{eq:conditionalpmf} the definition of pmf comes from the definition of conditional probability of events but in \eqref{eq:conditionalpdf} this is not so. We simply define the conditional density by analogy with the case of conditional pmf.  This is similar to the difference between interpretation of pmf ($f(t)$ is actually the probability of an event) and pdf ($f(t)$ is not the probability of an event but the density of probability near $t$).
\end{remark}

\begin{example} Let $(X,Y)$ have bivariate normal density $f(x,y)=\frac{\sqrt{ab-c^{2}}}{2\pi}e^{-\frac{1}{2}(ax^{2}+by^{2}+2cxy)}$ (so we assume $a>0,b>0, ab-c^{2}>0$). In the mid-term you showed that the marginal distribution of $Y$ is $N(0,\frac{a}{ab-c^{2}})$, that is it has density $g(y)=\frac{\sqrt{ab-c^{2}}}{\sqrt{2\pi a}}e^{-\frac{ab-c^{2}}{2a}y^{2}}$. Hence, the conditional density of $X$ given $Y=y$ is 
$$
h_{y}(x)=\frac{f(x,y)}{g(y)} =\frac{\sqrt{a}}{\sqrt{2\pi}}e^{-\frac{a}{2}(x+\frac{c}{a}y)^{2}}.
$$
Thus the conditional distribution of $X$ given $Y=y$ is $N(-\frac{cy}{a},\frac{1}{a})$. Compare this with marginal (unconditional) distribution of $X$ which is $N(0,\frac{b}{ab-c^{2}})$. 

In the special case when $c=0$, we see that for any value of $y$, the conditional distribution of $X$ given $Y=y$ is the same as the unconditional distribution of $X$. What does this mean? It is just another way of saying that $X$ and $Y$ are independent! Indeed, when $c=0$, the joint density $f(x,y)$ splits into a product of two functions, one of $x$ alone and one of $y$ alone.
\end{example}

\begin{exercise} Let $(X,Y)$ have joint density $f(x,y)$. Let the marginal densities of $X$ and $Y$ be $g(x)$ and $h(y)$ respectively. Let $h_{x}(y)$ be the conditional density of $Y$ given $X=x$. 
\begin{enumerate}\setlength\itemsep{6pt}
\item If $X$ and $Y$ are independent, show that for any $x$, we have $h_{x}(y)=h(y)$ for all $y$.
\item If $h_{x}(y)=h(y)$ for all $y$ and for all $x$, show that $X$ and $Y$ are independent.
\end{enumerate}
Analogous statements hold for the case of pmf.
\end{exercise}
}

\section{Mean and Variance}
Let $X$ be a random variable with distribution $F$. We shall assume that it has pmf or pdf denoted by $f$. 
\begin{definition} The {\em expected value} (also called {\em mean}) of $X$ is defined as the quantity $\mathbf{E}[X]=\sum_{t}tf(t)$ if $f$ is a pmf and $\mathbf{E}[X]=\int_{-\infty}^{+\infty} t f(t)dt$ if $f$ is a pdf (provided the sum or the integral converges absolutely). \end{definition}
Note that this agrees with the definition we gave earlier for random variables with pmf. it is possible to define expect value for distributions without pmf or pdf, but we shall not do it here.


\para{Properties of expectation} Let $X,Y$ be random variables both having pmf $f,g$ or  pdf $f,g$, respectively. 
\begin{enumerate}\setlength\itemsep{6pt}
\item Then, $\mathbf{E}[aX+bY]=a\mathbf{E}[X]+b\mathbf{E}[Y]$ for any $a,b\in \mathbb{R}$. In particular, for a constant random variable (i.e., $X=a$ with probability $1$ for some $a$, $\mathbf{E}[X]=a$). This is called {\em linearity} of expectation.
\item If $X\ge Y$ (meaning, $X(\omega)\ge Y(\omega)$ for all $\omega$), then $\mathbf{E}[X]\ge \mathbf{E}[Y]$
\item If $\varphi:\mathbb{R}\rightarrow \mathbb{R}$, then $$\mathbf{E}[\varphi(X)]= \begin{cases}
\sum\limits_{t}\varphi(t)f(t) & \mbox{ if $f$ is a pmf}. \\
\int_{-\infty}^{+\infty}\varphi(t)f(t) dt & \mbox{ if $f$ is a pdf}.
\end{cases}
$$
\item More generally, if $(X_{1},\ldots ,X_{n})$ has joint pdf $f(t_{1},\ldots ,t_{n})$ and $V=T(X_{1},\ldots ,X_{n})$ (here $T:\mathbb{R}^{n}\rightarrow \mathbb{R}$), then $\mathbf{E}[V]=\int_{-\infty}^{\infty}\ldots \int_{-\infty}^{\infty}T(x_{1},\ldots ,x_{n})f(x_{1},\ldots ,x_{n})dx_{1}\ldots dx_{n}$. 
\end{enumerate}
For random variables on a discrete probability space (then they have  pmf), we have essentially proved all these properties (or you can easily do so). For random variables with pmf, a proper proof require a bit of work. So we shall just take these for granted.
%and just illustrate the first one\footnote{You may skip this.}.
%
%\para{Proof of linearity of expectation} Let $(X,Y)$ have joint density $f(x,y)$. The marginal densities of $X$ and $Y$ are $g(x)=\int f(x,y) dy$ and $h(y)=\int f(x,y) dx$ respectively. Then,
%$$
%\mathbf{E}[
%$$ 
We state one more property of expectations, its relationship to independence.
\begin{lemma} Let $X,Y$ be random variables on a common probability space. If $X$ and $Y$ are independent, then $\mathbf{E}[H_{1}(X)H_{2}(Y)]=\mathbf{E}[H_{1}(X)]\mathbf{E}[H_{2}(Y)]$ for any functions $H_{1},H_{2}:\mathbb{R}\rightarrow \mathbb{R}$ (for which the expectations make sense). In particular, $\mathbf{E}[XY]=\mathbf{E}[X]\mathbf{E}[Y]$. 
\end{lemma}
\begin{proof} Independence means that the joint density (analogous statements for pmf omitted) of $(X,Y)$ is f the form $f(t,s)=g(t)h(s)$ where $g(t)$ is the density of $X$ and $h(s)$ is the density of $Y$. Hence, 
$$
\mathbf{E}[H_{1}(X)H_{2}(Y)]=\intt_{-\infty}^{\infty}\!\!\intt_{-\infty}^{\infty} H_{1}(t)H_{2}(s) f(t,s)dt ds = \left(\int\limits_{-\infty}^{\infty} H_{1}(t)g(t) dt \right)\left(\int\limits_{-\infty}^{\infty} H_{2}(s)g(s) ds \right)
$$
which is precisely $\mathbf{E}[H_{1}(X)]\mathbf{E}[H_{2}(Y)]$.
\end{proof}
Expectation is a very important quantity. Using it, we can define several other quantities of interest.

\para{Discussion} For simplicity let us take random variables to have densities in this discussion. You may adapt the remarks to the case of pmf easily.  The density has all the information we need about a random variable. However, it is a function, which means that we have to know $f(t)$ for every $t$. In real life often we have random variables whose pdf is unknown or impossible to determine. It would be better to summarize the main features of the distribution (i.e., the density) in a few numbers. That is what the quantities defined below try to do. 

\para{Mean} Mean is another term for expected value.

\para{Quantiles} Let us assume that the CDF $F$ of $X$ is strictly increasing and continuous. Then $F^{-1}(t)$ is well defined for every $t\in (0,1)$. For each $t\in (0,1)$, the number $Q_{t}=F^{-1}(t)$ is called the $t$-quantile. For example, the $1/2$-quantile, also called {\em median} is the number $x$ such that $F(x)=\frac{1}{2}$ (unique when the CDF is strictly increasing and continuous). Similarly one defines $1/4$-quantile and $3/4$-quantile and these are sometimes called quartiles.\footnote{Another familiar quantity is the percentile, frequently used in reporting performance in competitive exams. For each $x$, the $x$-percentile is nothing but $F(x)$. For exam scores, it  tells the proportion of exam-takers who scored less than or equal to $x$.}

\para{Moments} The quantity $\mathbf{E}[X^{k}]$ (if it exists) is called the $k^{\mbox{th}}$ {\em moment} of $X$. 

\para{Variance} Let $\mu=\mathbf{E}[X]$ and define ${\sigma}^{2}:=\mathbf{E}\left[ (X-\mu)^{2}\right]$. This is called the {\em variance} of $X$, also denoted by $\mbox{Var}(X)$. It can be written in other forms. For example,
\begin{align*}
{\sigma}^{2} &= \mathbf{E}[X^{2}+\mu^{2}-2\mu X] \qquad (\mbox{by expanding the square})\\
 &= \mathbf{E}[X^{2}]+\mu^{2} - 2\mu \mathbf{E}[X] \qquad (\mbox{by property (1) above})\\
 &= \mathbf{E}[X^{2}]-\mu^{2}.
\end{align*}
That is $\mbox{Var}(X)=\mathbf{E}[X^{2}]-(\mathbf{E}[X])^{2}$.

\para{Standard deviation} The standard deviation of $X$ is defined as $\mbox{s.d.}(X):=\sqrt{\mbox{Var}(X)}$.

\para{Mean absolute deviation} The mean absolute deviation of $X$ is defined as the $\mathbf{E}[|X-\mbox{med}(X)|]$.

\para{Coefficient of variation} The coefficient of variation of $X$ is defined as $\mbox{c.v.}(X)=\frac{\mbox{s.d.}(X)}{|{\bf E}[X]|}$.




\para{Covariance} Let $X,Y$ be random variables on a common probability space. The {\em covariance} of $X$ and $Y$ is defined as $\mbox{Cov}(X,Y)=\mathbf{E}[(X-\mathbf{E}[X])(Y-\mathbf{E}[Y])]$. It can also be written as $\mbox{Cov}(X,Y)=\mathbf{E}[XY]-\mathbf{E}[X]\mathbf{E}[Y]$.

\para{Correlation} Let $X,Y$ be random variables on a common probability space. Their {\em correlation} is defined as $\mbox{Corr}(X,Y)=\frac{\mbox{Cov}(X,Y)}{\sqrt{\mbox{Var}(X)}\sqrt{\mbox{Var}(Y)}}$.

\para{Entropy} The entropy of a random variable $X$ is defined as 
$$
\mbox{Ent}(X) = \begin{cases} -\sum_{i} f(t) \log(f(t_{i})) & \mbox{ if }X\mbox{ has pmf }f.\\
-\int\limits f(t) \log(f(t)) & \mbox{ if }X \mbox{ has pdf} f. \end{cases}
$$
If ${\bf X}=(X_{1},\ldots ,X_{n})$ is a random vector, we can define its entropy exactly by the same expressions, except that we use the joint pmf or pdf of ${\bf X}$ and the sum or integral is over points in $\mathbb{R}^{n}$.




\para{Discussion} What do these quantities mean? 

\parag{Measures of central tendency} Mean and median  try to summarize the distribution of $X$ by a single number. Of course one number cannot capture the whole distribution, so there are many densities and mass functions that have the same mean or median. Which is better - mean or median? This question has no unambiguous answer. Mean has excellent mathematical properties (mainly linearity) which the median lacks ($\mbox{med}(X+Y)$ bears no general relationship to $\mbox{med}(X)+\mbox{med}(Y)$). In contrast, mean is sensitive to outliers, while the median is far less so. For example, if the average income in a village of 50 people is $1000$ Rs. per month, the immigration of multi-millionaire to the village will change the mean drastically but the median remains about the same. This is good, if by giving one number we are hoping to express the state of a typical individual in the population.

\para{Measures of dispersion} Suppose the average height of people in a city is 160 cm. This could be because everyone is 160 cm exactly or because half the people are 100 cm. while the other half are 220 cm., or alternately the heights could be uniformly spread over 150-170 cm., etc. How widely the distribution is  spread is measured by standard deviation and mean absolute deviation. Since we want deviation from mean, $\mathbf{E}[X-\mathbf{E}[X]]$ looks natural, but this is zero because of cancellation of positive and negative deviations. To prevent cancellation, we may put absolute values (getting to the $\mbox{m.a.d}$, but that is usually taken around the median) or we may square the deviations before taking expectation (giving the variance, and then the standard deviation). Variance and standard deviation have much better mathematical properties (as we shall see) and hence are usually preferred.

The standard deviation has the same units as the quantity. Fo example, if mean height is 160cm  measured in centimeters with a standard deviation of 10cm, and the mean weight is 55kg with a  standard deviation of 5kg, then we cannot say which of the two is less variable. To make such a comparison we need a dimension free quantity (a pure number). Coefficient of variation is such a quantity, as it measure the standard deviation per mean. For the height and weight data just described, the coefficients of variation are 1/16 and 1/11, respectively. Hence we may say that height is less variable than weight in this example. 


\para{Measures of association} The marginal distributions do not determine the joint distribution. For example, if $(X,Y)$ is a point chosen at random from the unit square (with vertices $(0,0),(1,0),(0,1),(1,1)$) then $X,Y$ both have marginal distribution that is uniform on $[0,1]$. If $(U,V)$ is a point picked at random from the diagonal line (the line segment from $(0,0)$ to $(1,1)$, then again $U$ and $V$ have marginals that are uniform on $[0,1]$. 
But the two joint distributions are completely different. In particular, giving the means and standard deviations of $X$ and $Y$ does not tell anything about possible relationships between the two. 

Covariance is the quantity that is used to measure the ``association'' of $Y$ and $X$. Correlation is a dimension free quantity that measures the same. For example, we shall see that if $Y=X$, then $\mbox{Corr}(X,Y)=+1$, if $Y=-X$ then $\mbox{Corr}(X,Y)=-1$. Further, if $X$ and $Y$ are independent, then $\mbox{Corr}(X,Y)=0$. In general, if an increase in $X$ is likely to mean an increase in $Y$, then the correlation is positive and if an increase in $X$ is likely to mean a decrease in $Y$ then the correlation is negative.



\begin{example} Let $X\sim N(\mu,{\sigma}^{2})$. Recall that its density is $\frac{1}{{\sigma} \sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2{\sigma}^{2}}}$. We can compute
\begin{align*}
\mathbf{E}[X] &=\frac{1}{{\sigma} \sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}xe^{-\frac{(x-\mu)^{2}}{2{\sigma}^{2}}} dx =\mu.
% \mathbf{E}[(X-\mu)^{2}] &=\frac{1}{{\sigma} \sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}(x-\mu)^{2}e^{-\frac{(x-\mu)^{2}}{2{\sigma}^{2}}} dx  ={\sigma}^{2}. \\
\end{align*}
On the other hand
\begin{align*}
\mbox{Var}(X) &= \frac{1}{{\sigma} \sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}(x-\mu)^{2}e^{-\frac{(x-\mu)^{2}}{2{\sigma}^{2}}} dx \; = \; {\sigma}^{2}\frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{+\infty}u^{2}e^{-\frac{u^{2}}{2}} du \qquad (\mbox{substitute }x  = \mu+{\sigma} u) \\
&= {\sigma}^{2}\frac{2}{\sqrt{2\pi}} \int\limits_{0}^{+\infty}u^{2}e^{-\frac{u^{2}}{2}} du \; = \; {\sigma}^{2}\frac{2\sqrt{2}}{\sqrt{2\pi}} \int\limits_{0}^{+\infty}\sqrt{t}e^{-t} dt \qquad (\mbox{substitute }t=u^{2}/2) \\
&= {\sigma}^{2}\frac{2\sqrt{2}}{\sqrt{2\pi}}\Gamma(3/2) \; =\; {\sigma}^{2}.
\end{align*}
To get the last line, observe that $\Gamma(3/2)=\frac{1}{2}\Gamma(1/2)$ and $\Gamma(1/2)=\sqrt{\pi}$. Thus we now have a meaning for the parameters $\mu$ and ${\sigma}^{2}$ - they are the mean and variance of the $N(\mu,{\sigma}^{2})$ distribution. Again note that the mean is the same for all $N(0,{\sigma}^{2})$ distributions but the variances are different, capturing the spread of the distribution.
\end{example}
\begin{exercise} Let $X\sim N(0,1)$. Show that $\mathbf{E}[X^{n}]=0$ if $n$ is odd and if $n$ is even then $\mathbf{E}[X^{n}]=(n-1)(n-3)\ldots (3)(1)$ (product of all odd numbers up to and including $n-1$). What happens if $X\sim N(0,{\sigma}^{2})$?
\end{exercise}



\begin{exercise} Calculate the mean and variance for the following distributions. 
\begin{enumerate}\setlength\itemsep{6pt}
\item $X\sim \mbox{Geo}(p)$. $\mathbf{E}[X]=\frac{1}{p}$ and $\mbox{Var}(X)=\frac{q}{p^{2}}$.
\item $X\sim \mbox{Bin}(n,p)$. $\mathbf{E}[X]=np$ and $\mbox{Var}(X)=npq$.
\item $X\sim \mbox{Pois}(\lambda)$. $\mathbf{E}[X]=\lambda$ and $\mbox{Var}(X)=\lambda$.
\item $X\sim \mbox{Hypergeo}(N_{1},N_{2},m)$. $\mathbf{E}[X]=\frac{mN_{1}}{N_{1}+N_{2}}$ and $\mbox{Var}(X)=??$.
\end{enumerate}
\end{exercise}

\begin{exercise} Calculate the mean and variance for the following distributions. 
\begin{enumerate}\setlength\itemsep{6pt}
\item $X\sim \mbox{Exp}(\lambda)$. $\mathbf{E}[X]=\frac{1}{\lambda}$ and $\mbox{Var}(X)=\frac{1}{\lambda^{2}}$.
\item $X\sim \mbox{Gamma}(\nu,\lambda)$. $\mathbf{E}[X]=\frac{\nu}{\lambda}$ and $\mbox{Var}(X)=\frac{\nu}{\lambda^{2}}$.
\item $X\sim \mbox{Unif}[0,1]$. $\mathbf{E}[X]=\frac{1}{2}$ and $\mbox{Var}(X)=\frac{1}{12}$.
\item $X\sim \mbox{Beta}(p,q)$. $\mathbf{E}[X]=\frac{p}{p+q}$ and $\mbox{Var}(X)=\frac{pq}{(p+q)^{2}(p+q+1)}$.
\end{enumerate}
\end{exercise}

\para{Properties of covariance and variance}  Let $X,Y,X_{i},Y_{i}$ be random variables on a common probability space. Small letters $a,b,c$ etc will denote scalars.

\begin{enumerate}\setlength\itemsep{6pt}
\item (Bilinearity): $\mbox{\textrm Cov}(aX_{1}+bX_{2},Y)=a\mbox{\textrm Cov}(X_{1},Y)+b\mbox{\textrm Cov}(X_{2},Y)$ and $\mbox{\textrm Cov}(Y,aX_{1}+bX_{2})=a\mbox{\textrm Cov}(Y,X_{1})+b\mbox{\textrm Cov}(Y,X_{2})$
\item (Symmetry): $\mbox{\textrm Cov}(X,Y)=\mbox{\textrm Cov}(Y,X)$.
\item (Positivity): $\mbox{\textrm Cov}(X,X)\ge 0$ with equality if and only if $X$ is a constant random variable. Indeed, $\mbox{\textrm Cov}(X,X)=\mbox{Var}(X)$.
\end{enumerate}

\begin{exercise} Show that $\mbox{Var}(cX)=c^{2}\mbox{Var}(X)$ (hence $\mbox{sd}(cX)=|c|\mbox{sd}(X)$). Further, if $X$ and $Y$ are independent, then $\mbox{Var}(X+Y)=\mbox{Var}(X)+\mbox{Var}(Y)$.
\end{exercise}

Note that the properties of ovariance are very much like properties of inner-products in vector spaces. In particular, we have the following analogue of the well-known inequality for vectors $(\u\cdot \v)^{2}\le(\u\cdot \u)(\v\cdot \v)$.

\para{Cauchy-Schwarz inequality}  If $X$ and $Y$ are random variables with finite variances, then $(\mbox{\textrm Cov}(X,Y))^{2}\le \mbox{Var}(X)\mbox{Var}(Y)$ with equality if and only if $Y=aX+b$ for some scalars $a,b$. 

If not convinced, follow the proof of Cauchy-Schwarz inequality that you have seen for vectors. This just means that $\mbox{Var}(X+tY)\ge 0$ for any scalar $t$ and choose an appropriate $t$ to get the Cauchy-Schwarz's inequality.


\section{Makov's and Chebyshev's inequalities}
Let $X$ be a non-negative integer valued random variable with pmf $f(k)$, $k=0,1,2,\ldots$.  Fix any number $m$, say $m=10$. Then
$$
\mathbf{E}[X]=\sum_{k=1}^{\infty}k f(k) \ge \sum_{k=10}^{\infty}kf(k)\ge \sum_{k=10}^{\infty}10f(k) = 10 \mathbf{P}\{X\ge 10\}.
$$
More generally $m\mathbf{P}\{X\ge m\}\le \mathbf{E}[X]$. This shows that if the expected value is finite This idea is captured in general by the following inequality.

\para{Markov's inequality} Let $X$ be a non-negative random variable with finite expectation. Then, for any $t>0$, we have $\mathbf{P}\{X\ge t\}\le \frac{1}{t}\mathbf{E}[X]$.

\begin{proof} Fix $t>0$ and let $Y=X{\mathbf 1}_{X< t}$ and $Z=X{\mathbf 1}_{X\ge t}$ so that $X=Y+Z$. Both $Y$ and $Z$ are non-negative random variable and hence $\mathbf{E}[X]=\mathbf{E}[Y]+\mathbf{E}[Z]\ge \mathbf{E}[Z]$. On the other hand, $Z\ge t {\mathbf 1}_{X\ge t}$ (why?). Therefore $\mathbf{E}[Z]\ge t\mathbf{E}[{\mathbf 1}_{X\ge t}]=t\mathbf{P}\{X\ge t\}$.
 Putting these together we get $\mathbf{E}[X]\ge t\mathbf{P}\{X\ge t\}$ as desired to show.
\end{proof}
Markov's inequality is simple but surprisingly useful. Firstly, one can apply it to functions of our random variable and get many inequalities. Here are some.

\para{Variants of Markov's inequality}
\begin{enumerate}\setlength\itemsep{6pt}
\item If $X$ is a non-negative random variable with finite $p^{\mbox{\tiny th}}$ moment, then $\mathbf{P}\{X\ge t\}\le t^{-p}\mathbf{E}[X^{p}]$ for any $t>0$.
\item If $X$ is a random variable with finite second moment, then $\mathbf{E}[|X-\mu|\ge t]\le \frac{1}{t^{2}}\mbox{Var}(X)$. [{\em Chebyshev's inequality}]
\item IF $X$ is a random variable with finite exponential moments, then $\mathbf{P}(X>t)\le e^{-\lambda t}\mathbf{E}[e^{\lambda X}]$ for any $\lambda>0$.
\end{enumerate}
Thus, if we only know that $X$ has finite mean, the tail probability $\mathbf{P}(X>t)$ must decay at least as fast as $1/t$. But if we knew that the second moment was finite we could assert that the decay must be at least as fast as $1/t^{2}$, which is better. If $\mathbf{E}[e^{\lambda X}]<\infty$, then we get much faster decay of the tail, like $e^{-\lambda t}$.

Chebyshev's inequality captures again the intuitive notion that variance measures the spread of the distribution about the mean. The smaller the variance, lesser the spread. An alternate way to write Chebyshev's inequality is 
$$
\mathbf{P}(|X-\mu|>r\sigma)\le \frac{1}{r^{2}}
$$
where ${\sigma}=\mbox{s.d.}(X)$. This measures the deviations in multiples of the standard deviation. This is a very general inequality. In specific cases we can get better bounds than $1/r^{2}$ (just like Markov inequality can be improved using higher moments, when they exist).

One more useful inequality we have already seen is the Cauchy-Schwarz inequality: $(\mathbf{E}[XY])^{2}\le \mathbf{E}[X^{2}]\mathbf{E}[Y^{2}]$ or $(\mbox{\textrm Cov}(X,Y))^{2}\le \mbox{Var}(X)\mbox{Var}(Y)$.

%\newpage
\section{Weak law of large numbers}
Let $X_{1},X_{2},\ldots$ be i.i.d random variables (independent random variables each having the same marginal distribution). Assume that the second moment of $X_{1}$ is finite. Then, $\mu=\mathbf{E}[X_{1}]$ and ${\sigma}^{2}=\mbox{Var}(X_{1})$ are well-defined. 

Let $S_{n}=X_{1}+\ldots +X_{n}$ (partial sums) and $\overline{X}_{n}=\frac{S_{n}}{n}=\frac{X_{1}+\ldots +X_{n}}{n}$ ({\em sample mean}). Then, by the properties of expectation and variance, we have
$$
\mathbf{E}[S_{n}]=n\mu, \;\; 
\mbox{Var}(S_{n})=n{\sigma}_{1}^{2}, \qquad \mathbf{E}[\overline{X}_{n}]=\mu, \;\; \mbox{Var}(\overline{X}_{n})=\frac{{\sigma}^{2}}{n}.
$$
In particular, $\mbox{s.d.}(\overline{X}_{n})={\sigma}/\sqrt{n}$ decreases with $n$. If we apply Chebyshev's inequality to $\overline{X}_{n}$, we get for any $\delta>0$ that
$$
\mathbf{P}\{|\overline{X}_{n}-\mu|\ge \delta\}\le \frac{{\sigma}^{2}}{\delta^{2}n}.
$$
This goes to zero as $n\rightarrow \infty$ (with $\delta>0$ being fixed). This means that for large $n$ the sample mean is unlikely to be far from $\mu$ (sometimes called ``population mean''). This is consistent with our intuitive idea that if we toss a $p$-coin many times, we can get a better guess of what the value of $p$ is.

\para{Weak law of large numbers (Jacob Bernoulli)}  With the above notations, for any $\delta>0$, we have
$$
\mathbf{P}\{|\overline{X}_{n}-\mu|\ge \delta\}\le \frac{{\sigma}^{2}}{\delta^{2}n}\rightarrow 0 \;\; \mbox{ as }n\rightarrow \infty.
$$

\medskip
This is very general, in that we only assume the existence of variance. If $X_{k}$ are assumed to have more moments, one can get better bounds. For example,  when $X_{k}$ are i.i.d. $\mbox{Ber}(p)$, we have the following theorem.

\para{Hoeffding's inequality} Let $X_{1},\ldots ,X_{n}$ be i.i.d. $\mbox{Ber}(p)$. Then 
$$
\mathbf{P}\{|\overline{X}_{n}-p|\ge \delta\} \le 2e^{-n\delta^{2}/2}.
$$

%\medskip
%To compare the two, let $X_{k}$ be $\mbox{Ber}(1/2)$ and take $\delta=k/4$. The weak law asserts that $\mathbf{P}\{|\overline{X}_{n}-\frac{1}{2}|\ge n^{-0.49}\}\le 0.25n^{-0.02}$. Hoeffdings gives $\mathbf{P}\{|\overline{X}_{n}-\frac{1}{2}|\ge n^{-0.49}\}\le 2e^{-n^{0.02}/2}$ which is much smaller for large $n$. For $n=10^{23}$, the two bounds are 



 %Let us take the simple example of $X_{k}$ having $\mbox{Ber}(p)$ distribution where $0<p<1$. Then $\overline{X}_{n}-p=\frac{1}{n}\sum_{k=1}^{n}(X_{k}-p)$. If we take the fourth power instead of the second, we get
%\bes
%\mathbf{E}[|\overline{X}_{n}-p|^{4}] &=& \frac{1}{n^{4}}\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}\sum_{\ell=1}^{n}\mathbf{E}[(X_{i}-p)(X_{j}-p)(X_{k}-p)(X_{\ell}-p)] \\
%&=& \frac{1}{n^{4}}\sum_{i=1}^{n}\mathbf{E}[(X_{i}-p)^{4}] + \frac{3}{n^{4}}\sum_{i=1}^{n}\sum_{\stackrel{j=1}{j\not=i}}^{n}\mathbf{E}[(X_{i}-p)^{2}]\mathbf{E}[(X_{j}-p)^{2}].
%\ees
%This is because  $\mathbf{E}[(X_{i}-p)(X_{j}-p)(X_{k}-p)(X_{\ell}-p)]$ vanishes if one of $i,j,k,\ell$ occurs only once (since expectation factors when we have independence). Either all four are must be equal, or they must be equal in pairs. For $X\sim \mbox{Ber}(p)$, it is easy to see that $\mathbf{E}[(X-p)^{4}]=pq(p^{3}+q^{3})$. Thus, 
%$$
%\mathbf{E}[|\overline{X}_{n}-p|^{4}] = \frac{1}{n^{3}}pq(p^{3}+q^{3}) + \frac{3(n-1)}{n^{3}}p^{2}q^{2} \le \frac{4pq}{n^{2}}.
%$$
%Hence, $\mathbf{P}\{|\overline{X}_{n}-p|^{4}\ge \delta\}\le \frac{1}{\delta^{4}}\mathbf{E}[|\overline{X}_{n}-p|^{4}] \le \frac{4pq}{\delta^{2}n^{2}}$ which decays faster as $n\rightarrow \infty$.


\section{Monte-Carlo integration} In this section we give  a simple application of WLLN. Let $\varphi:[0,1]\rightarrow \mathbb{R}$ be a continuous function. We would like to compute $I=\int_{0}^{1}\varphi(x)dx$. Most often we cannot compute the integral explicitly and for an approximate value we resort to numerical methods. Here is an idea to use random numbers. 

Let $U_{1},U_{2},\ldots ,U_{n}$ be i.i.d. $\mbox{Unif}[0,1]$ random variables and let $X_{1}=\varphi(U_{1}),\ldots ,X_{n}=\varphi(U_{n})$. Then, $X_{k}$ are i.i.d. random variables with common mean and variance
$$
\mu=\int\limits_{0}^{1}\varphi(x)dx = I, \qquad {\sigma}^{2}:=\mbox{Var}(X_{1})=\int\limits_{0}^{1}(\varphi(x)-I)^{2}dx.
$$
This gives the following method of finding $I$. Fix a large number $N$ appropriately and pick $N$ uniform random numbers $U_{k}$, $1\le k\le N$. Then define $\hat{I}_{N}:=\frac{1}{N}\sum_{k=1}^{N}\varphi(U_{k})$. Present $\hat{I}_{N}$ as an approximate value of $I$.

In what sense is this an approximation of $I$ and why? Indeed, by WLLN $\mathbf{P}\{|\hat{I}_{n}-I|\ge \delta \}\rightarrow 0$ and hence we expect $\hat{I}_{n}$ to be close to $I$. How large should $n$ be? For this, we fix two numbers $\epsilon=0.01$ and $\delta=0.001$ (you may change the numbers). By Chebyshev's inequality, observe that $\mathbf{P}\{|\hat{I}_{n}-I|\ge \delta \}\rightarrow {\sigma}^{2}/N\delta^{2}$.

 First find $N$ so that ${\sigma}^{2}/N\delta^{2}<\epsilon$, i.e., $N=\lceil \frac{{\sigma}^{2}}{\delta^{2}}\rceil$. Then, the random variable $\hat{I}_{N}$ is within $\delta$ of $I$ with probability greater than $1-\epsilon$. This is a probabilistic method, hence there is a possibility of large error, but with a small probability. Observe that $N$ grows proportional to {\em square} of $1/\delta$. To increase the accuracy by $10$, you must increase the number of samples by a factor of $100$. 
 
 One last point. To find $N$ we need ${\sigma}^{2}$ which involves computing another integral involving $\varphi$ which we do not know how to compute!  Here we do not need the exact value of the integral. For example,  if our functions satisfies $-M\le \varphi(x)\le M$ for all $x\in [0,1]$, then also $-M\le I\le M$ and hence $(\varphi(x)-\I)^{2}\le 4M^{2}$. This means that ${\sigma}^{2}\le 4M^{2}$. Therefore, if we take $N=\lceil \frac{4M^{2}}{\delta^{2}}\rceil$ then the value of $N$ is larger than required for the desired accuracy. We can work with this $N$. Note that the dependence of  of $N$ on $\delta$ does not change.
 
\begin{exercise} We know that $\int_{0}^{1}\frac{1}{1+x^{2}}dx=\frac{\pi}{4}$. Based on this, devise a method to find an approximate value of $\pi$. Use any software you like to implement your method and see how many sample you need to get an approximation to $1$, $2$ and $3$ decimal places consistently (consistently means with a large enough probability, say $0.9$). 
\end{exercise} 
 
\begin{exercise} Devise a method to approximate $e$ and $\pi$ (there are many possible integrals). 
\end{exercise} 

This method can be used to evaluate integrals over any interval. For instance, how would you find $\int_{a}^{b}\varphi(t) dt$ or $\int_{0}^{\infty}\varphi(t) e^{-t}dt$ or $\int_{-\infty}^{\infty}\varphi(t)e^{-t^{2}}dt$ where  $\varphi$ is a function on the appropriate interval? It can also be used to evaluate multiple integrals (and consequently to find the areas and volumes of sets). The only condition is that it should be possible to evaluate the given function $\varphi$ at a point $x$ on the computer. To illustrate, consider the problem of finding the area of a region $\{(x,y){\; : \;} 0\le x,y, \le1, \ 2x^{3}y^{2}\ge 1, \ x^{2}+2y^{2}\le 2.3 \}$. It is complicated to work with such regions analytically, but given a point $(x,y)$, it is easy to check on a computer whether all the constraints given are satisfied. 

As a last remark, how do Monte-Carlo methods compare with the usual numerical methods? In the latter, usually a number $N$ and a set of points $x_{1},\ldots ,x_{N}$ are fixed along with some weights $w_{1},\ldots ,w_{N}$ that sum to $1$. Then one presents $\tilde{I}:=\sum_{k=1}^{N}w_{k}\varphi(x_{k})$ as the approximate value of $I$. Lagrange's method, Gauss quadrature etc are of this type. Under certain assumptions on $\varphi$, the accuracy of these integrals can be like $1/N$ as opposed to $1/\sqrt{N}$ in Monte-Carlo. But when those assumptions are not satisfied, $\tilde{I}$ can be way off $I$. One may regard this as a game of strategy as follows.

I present a function $\varphi$ (say bounded between $-1$ and $1$)  and you are expected to give an approximation to $\varphi$. Quadrature methods do a good job generically, but if I knew the procedure you use, then I can give a function for which your result is entirely wrong (for example, I pick a function $\varphi$ which vanishes at each of the quadrature points!). However, with Monte-Carlo methods, even if I know the procedure, there is no way to prevent you from getting an approximation of accuracy $1/\sqrt{N}$. This is because neither of us know where the points $U_{k}$ will fall!


%\newpage
\section{Central limit theorem}
Let $X_{1},X_{2},\ldots$ be i.i.d. random variables with expectation $\mu$ and variance ${\sigma}^{2}$. We saw that $\overline{X}_{n}$ has mean $\mu$ and standard deviation ${\sigma}/\sqrt{n}$. 

This roughly means that $\overline{X}_{n}$ is close to $\mu$, within a few multiples of ${\sigma}/\sqrt{n}$ (as shown by Chebyshev's inequality). Now we look at $\overline{X}_{n}$ with a finer microscope. In other words, we ask for the probability that $\overline{X}_{n}$ is within the tiny interval $[\mu+\frac{a}{\sqrt{n}},\mu+\frac{b}{\sqrt{n}}]$ for any $a<b$. The answer turns out to be surprising and remarkable!

\para{Central limit theorem} Let $X_{1},X_{2},\ldots$ be i.i.d. random variables with expectation $\mu$ and variance ${\sigma}^{2}$. We assume that $0<{\sigma}^{2}<\infty$. Then, for any $a<b$, we have
$$
\mathbf{P}\left\{ \mu+a\frac{{\sigma}}{\sqrt{n}}\le \overline{X}_{n}\le \mu+b\frac{{\sigma}}{\sqrt{n}}\right\} \rightarrow \Phi(b)-\Phi(a) = \frac{1}{\sqrt{2\pi}}\int\limits_{a}^{b}e^{-t^{2}/2}dt.
$$

\medskip
What is remarkable about this? The end result does not depend on the distribution of $X_{i}$s at all! Only the mean and variance of the distribution were used! As this is one of the most important theorems in all of probability theory, we restate it in several forms, all equivalent to the above.

\para{Restatements of central limit theorem} Let $X_{k}$ be as above. Let $S_{n}=X_{1}+\ldots +X_{n}$. Let $Z$ be a $N(0,1)$ random variable. Then of course $\mathbf{P}\{a<Z<b\}=\Phi(b)-\Phi(a)$.
\begin{enumerate}\setlength\itemsep{6pt}
\item $\mathbf{P}\{a<\frac{\sqrt{n}}{{\sigma}}(\overline{X}_{n}-\mu)\le b\} \rightarrow \Phi(b)-\Phi(a) = \mathbf{P}\{a<Z<b\}$. Put another way, this says that for large $n$, the random variable $\frac{\sqrt{n}(\overline{X}_{n}-\mu)}{{\sigma}}$ has $N(0,1)$ distribution, approximately. Equivalently, $\sqrt{n}(\overline{X}_{n}-\mu)$ has $N(0,{\sigma}^{2})$ distribution, approximately.

\item Yet another way to say the same is that $S_{n}$ has approximately normal distribution with mean $n\mu$ and variance $n{\sigma}^{2}$. That is,
$$
\mathbf{P}\left\{a\le \frac{S_{n}-n\mu}{{\sigma}\sqrt{n}} \le b\right\} \rightarrow \mathbf{P}\{a<Z<b\}.
$$
\end{enumerate}

The central limit theorem so deep and surprising and useful. The following example gives a hint as to why.
\begin{example} Let $U_{1},\ldots ,U_{n}$ be i.i.d. Uniform($[-1,1]$) random variables. Let $S_{n}=U_{1}+\ldots +U_{n}$, let $\overline{U}_{n}=S_{n}/n$ (sample mean) and let $Y_{n}=S_{n}/\sqrt{n}$. Consider the problem of finding the distribution of any of these. Since they are got from each other by scaling, finding the distribution of one is the same as finding that of any other. For uniform $[-1,1]$, we know that $\mu=0$ and ${\sigma}^{2}=1/3$. Hence, CLT tells us that
$$
\mathbf{P}\left\{\frac{a}{\sqrt{3}}<Y_{n}<\frac{b}{\sqrt{3}}\right\}\rightarrow \Phi(b)-\Phi(a).
$$
or equivalently, $\mathbf{P}\{a<Y_{n}<b\}\rightarrow \Phi(b\sqrt{3})-\Phi(a\sqrt{3})$. For large $n$ (practically, $n=50$ is large enough) we may use this limit as a good aproximation to the probability we want.

Why is this surprising? The way to find the distribution of $Y_{n}$ would be this. Using the convolution formula $n$ times successively, one can find the density of $S_{n}=U_{1}+\ldots +U_{n}$ (in principle! the actual integration may be intractable!). Then we can find the density of $Y_{n}$ by another change of variable (in one dimension). Having got the density of $Y_{n}$, we integrate it from $a$ to $b$ to get $\mathbf{P}\{a<Y_{n}<b\}$.  This is clearly a daunting task (if you don't feel so, just try it for $n=5$). 

The CLT cuts short all this and directly gives an approximate answer! And what is even more surprising is that the original distribution does not matter - we only need to know the mean and variance of the original distribution!
\end{example}
%
%Of these $Y_{n}$ is the better one to look at. To see why, observe that $\mbox{Var}(S_{n})=\frac{n}{12}$, $\mbox{Var}(\overline{U}_{n})=\frac{1}{12n}$ and $\mbox{Var}(Y_{n})=\frac{1}{12}$. Thus the variance of $S_{n}$ goes to infinity (implying that the distribution of $S_{n}$ gets spread out over larger and larger intervals), the variance of $\overline{U}_{n}$ goes to zero (so it becomes almost a constant, indeed the weak law says that $\overline{U}_{n}$ is very close to the population mean which is $0$ in this case). Put another way, what happens is that for any fixed $a<b$, the probabilities $\mathbf{P}\{a<S_{n}<b\}$ will go to $0$, while $\mathbf{P}\{a<\overline{U}_{n}<b\}$ goes to $0$ (if $a<0<b$) or $1$ (if $a$ and $b$ are both positive or both negative) etc. 

%However, the variance of $Y_{n}$ stays away from $0$ and $\infty$

%
%\end{example}


We shall not prove the central limit theorem in general. But we indicate how it is done when $X_{k}$ come from $\mbox{Exp}(\lambda)$ distribution. This is optional and may be skipped.

\begin{proof}[CLT for Exponetials] Let $X_{k}$ be i.i.d. $\mbox{Exp}(1)$ random variables. They have mean $\mu=1$ and variance ${\sigma}^{2}=1$. We know that (this was an exercise), $S_{n}=X_{1}+\ldots +X_{n}$ has $\mbox{Gamma}(n,1)$ distribution. Its density is given by $f_{n}(t)=e^{-t}t^{n-1}/(n-1)!$ for $t>0$.

Now let $Y_{n}=\frac{S_{n}-n\mu}{{\sigma}\sqrt{n}}=\frac{S_{n}-n}{\sqrt{n}}$. By a change of variable (in one-dimension) we see that the density of $Y_{n}$ is given by $g_{n}(t)=\sqrt{n}f_{n}(n+t\sqrt{n})$. Let us analyse this.
\begin{align*}
g_{n}(t) &= \sqrt{n}\ \frac{1}{(n-1)!}e^{-(n+t\sqrt{n})}(n+t\sqrt{n})^{n-1}  \\
&= \sqrt{n}\ \frac{n^{n-1}}{(n-1)!}e^{-n-t\sqrt{n}}\left(1+\frac{t}{\sqrt{n}}\right)^{n-1} \\
&\approx \sqrt{n}\ \frac{n^{n-1}}{\sqrt{2\pi}(n-1)^{n-\frac{1}{2}}e^{-n+1}}e^{-n-t\sqrt{n}}\left(1+\frac{t}{\sqrt{n}}\right)^{n-1} \hspace{2mm}(\mbox{by Stirling's formula})\\
&= \frac{1}{\sqrt{2\pi}(1-\frac{1}{n})^{n-\frac{1}{2}}e^{1}}e^{-t\sqrt{n}}\left(1+\frac{t}{\sqrt{n}}\right)^{n-1}.
\end{align*}
To find the limit of this, first observe that $(1-\frac{1}{n})^{n-\frac{1}{2}}\rightarrow e^{-1}$. It remains to find the limit of $w_{n}:=e^{-t\sqrt{n}}\left(1+\frac{t}{\sqrt{n}}\right)^{n-1}$. Easiest to do this by taking logarithms. Recall that $\log(1+t)=t-\frac{t^{2}}{2}+\frac{t^{3}}{3}-\ldots$. Hence
\begin{align*}
\log w_{n} &= -t\sqrt{n} + (n-1)\log\left(1+\frac{t}{\sqrt{n}}\right) \\
&= -t\sqrt{n} + (n-1)\left[ \frac{t}{\sqrt{n}}-\frac{t^{2}}{2n}+\frac{t^{3}}{3n^{3/2}}-\ldots\right] \\
&= -\frac{t^{2}}{2}+[\ldots]
\end{align*}
where in $[\ldots]$ we have put all terms which go to zero as $n\rightarrow \infty$. Since there are infinitely many, we should argue that even after adding all of them, the total goes to zero as $n\rightarrow \infty$. Let us skip this step and simply conclude that $\log w_{n}\rightarrow -t^{2}/2$. Therefore, $g_{n}(t) \rightarrow \varphi(t):=\frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}$ which is the standard normal density.

What we wanted was $\mathbf{P}\{a<Y_{n}<b\}=\int\limits_{a}^{b}g_{n}(t) dt$. Since $g_{n}(t)\rightarrow \varphi(t)$ for each $t$, it is believable that $\int\limits_{a}^{b}g_{n}(t) dt\rightarrow \int\limits_{a}^{b}\varphi(t) dt$. This too needs justification but we skip it. Thus,
$$
\mathbf{P}\{a<Y_{n}<b\} \rightarrow \int\limits_{a}^{b}\varphi(t) dt = \Phi(b)-\Phi(a).
$$
This proves CLT for the case of exponential random variables.
\end{proof}
%
%$$
%\mathbf{P}\left\{a\le \frac{S_{n}-n\mu}{{\sigma}\sqrt{n}}\le b\right\} =\mathbf{P}\{np+a\sqrt{npq}\le S_{n}\le np+b\sqrt{npq}\} = \sum\limits_{k=A_{n}}^{B_{n}}\binom{n}{k}p^{k}q^{n-k}.
%$$
%where $A_{n}=\lceil np+a\sqrt{npq} \rceil$ and $B_{n}=\lfloor np+b\sqrt{npq} \rfloor$. Since $\sqrt{n}$ is much smaller than $n$, we see that $A_{n},B_{n}\rightarrow \infty$. Therefore, for $k$ between $A_{n}$ and $B_{n}$, we may use Stirling's approximation and write
%$$
%\binom{n}{k}p^{k}q^{n-k} = \frac{n!}{k!(n-k)!}p^{k}q^{n-k} \sim \frac{\sqrt{2\pi}e^{-n}n^{n+\frac{1}{2}}}{\sqrt{2\pi}e^{-k}k^{k+\frac{1}{2}}\sqrt{2\pi}e^{-(n-k)}(n-k)^{n-k+\frac{1}{2}}}
%$$
%\end{proof}




%

%%\newpage
%\section{Expectation}

%%%%\newpage
%\section{Variance, covariance, moments}

%%%%\newpage
%\section{Inequalities - Markov, Chebyshev, Cauchy-Schwarz}

%%%%\newpage
%\section{Weak law of large numbers}

%%%%\newpage
%\section{Normal approximation to the binomial distribution}

%%%%\newpage
%\section{Poisson approximation to the binomial distribution}

%%%%\newpage
%\section{Central limit theorem - a discussion}

%%%%\newpage
%\section{Random walks}

%%%%\newpage
%\section{Branching processes}

%%%%\newpage
%\section{P\'{o}lya's urn scheme}

%%%%\newpage
%\section{Coupon collector problem}

%%%\newpage

\newpage
\section{Poisson limit for rare events}
Let $X_{k}\sim \mbox{Ber}(p)$ be independent random variables. Central limit theorem says that if $p$ is fixed and $n$ is large, the distribution of $(X_{n}-np)/\sqrt{np(1-p)}$ is close to the  $N(0,1)$ distribution. 

Now we consider a slightly different situation. Let $X_{1},\ldots ,X_{n}$ have $\mbox{Ber}(n,p_{n})$ distribution where $p_{n}=\frac{\lambda}{n}$, where $\lambda>0$ is fixed. Then, we shall show that the distribution of $X_{1}+\ldots +X_{n}$ is close to that of $\mbox{Pois}(\lambda)$. Note that the distribution of $X_{1}$ changes with $n$ and hence it would be more correct to write $X_{n,1}, \ldots ,X_{n,n}$. 

\begin{theorem} Let $\lambda>0$ be fixed and let $X_{n,1},\ldots ,X_{n,n}$ be i.i.d. $\mbox{Ber}(\lambda/n)$. Let $S_{n}=X_{n,1}+\ldots +X_{n,n}$. Then, for every $k\ge 0$
$$
\mathbf{P}\{S_{n}=k\}\rightarrow e^{-\lambda}\frac{\lambda^{k}}{k!}.
$$
\end{theorem}
\begin{proof} Fix $k$ and observe that
\begin{align*}
\mathbf{P}\{S_{n}=k\} &= \binom{n}{k}\left(\frac{\lambda}{n}\right)^{k}\left(1-\frac{\lambda}{n}\right)^{n-k} \\
 &= \frac{n(n-1)\ldots (n-k+1)}{k!}\frac{\lambda^{k}}{n^{k}}\left(1-\frac{\lambda}{n}\right)^{n-k}.
 \end{align*}
Note that $\frac{n(n-1)\ldots(n-k+1)}{n^{k}}\rightarrow 1$ as $n\rightarrow \infty$ (since $k$ is fixed). Also, $(1-\frac{\lambda}{n})^{n-k}\rightarrow e^{-\lambda}$ (if not clear, note that $(1-\frac{\lambda}{n})^{n}\rightarrow e^{-\lambda}$ and $(1-\frac{\lambda}{n})^{-k}\rightarrow 1$). Hence, the right hand side above converges to $e^{-\lambda}\frac{\lambda^{k}}{k!}$ which is what we wanted to show.
\end{proof}
What is the meaning of this? Bernoulli random variables may be thought of as indicators of events, i.e., think of $X_{n,1}$ as ${\mathbf 1}_{A_{1}}$ etc. The theorem considers $n$ events which are independent and each of them is ``rare'' (since the probability of it occurring is $\lambda/n$ which becomes small as $n$ increases). The number of events increases but the chance of each events decreases in such a way that the expected number of events that occur stays constant. Then, the total number of events that actually occur has an approximately Poisson distribution.
\begin{example} (A physical example). A large amount of custard is made in the hostel mess to serve $100$ students. The cook adds $300$ raisins and mixes the custard so that on an average they get $3$ raisins per student. But the number of raisins that a given student gets is random and the above theorem says that it has approximately $\mbox{Pois}(3)$ distribution. How so? Let $X_{k}$ be the indicator of the event that the $k$th raisin ends up in your cup. Since there are $100$ cups, the chance of this happening is $1/100$. The number of raisins in your cup is precisely $X_{1}+X_{2}+\ldots +X_{300}$. Appy the theorem (take $n=100$ and $\lambda=3$).
\end{example}

\begin{example} Place $r$ balls in $m$ bins at random. If $m=1000$  and $r=500$, then the number of balls in the first bin has approximately $\mbox{Pois}(1/2)$ distribution. Work out how this comes from the theorem.
\end{example}

The Poisson limit is a much more general phenomenon than what the theorem above captures. For example, consider the problem of a psychic guessing a deck of cards. If $X$ is the number  of correct guesses, we saw (by direct calculation and approximation) that $\mathbf{P}\{X=k\}$ is close to $e^{-1}/k!$. In other words $X$ has approximately $\mbox{Pois}(1)$ distribution. Does it follows from the theorem above. Let us try.

Set $X_{k}$ to be the indicator of the event that the $k$th guess is correct. Then $X_{k}\sim \mbox{Ber}(1/52)$ and $X=X_{1}+\ldots +X_{52}$. It looks like the theorem tells us that $X$ should have $\mbox{Pois}(1)$ distribution (by taking $n=52$ and $\lambda=1$). But note that $X_{i}$ are not independent random variables and hence the theorem does not strictly apply. The theorem should be thought of as one of many theorems that capture the theme {\em ``in a large collection of rare events that are nearly independent, the actual number of events that occur is approximately Poisson''}.


{\color{magenta}
\section{Entropy, Gibbs distribution}
In this section we talk about entropy, a concept of fundamental importance in physics, mathematics and information theory.\footnote{This section was not covered in class and may be safely omitted}
\begin{definition} Let $X$ be a random variable that takes values in ${\mathcal A}=\{a_{1},\ldots ,a_{k}\}$ such that $\mathbf{P}(X=a_{i})=p_{i}$. The entropy of $X$ is defined as 
$$
H(X) := -\sum\limits_{i=1}^{k} p_{i}\log p_{i}.
$$ 
If $X$ is a real-valued random variable with density $f$, its entropy is defined
$$
H(X):= -\int f(t)\log f(t) dt.
$$ 
\end{definition}
\begin{example} Let $X\sim\mbox{Ber}(p)$. Then $H(X)=p\log(1/p) +(1-p)\log(1/(1-p))$.
\end{example}

\begin{example} Let $X\sim \mbox{Geo}(p)$. Then $H(X)=-\sum\limits_{k=0}^{\infty}(\log p+k\log q) pq^{k} = -\log p -q^{2}\log q$. 
\end{example}

\begin{example} Let $X\sim \mbox{Exp}(\lambda)$. Then $H(X)=\int_{0}^{\infty} (\log \lambda -t)\lambda e^{-\lambda t}dt=\log \lambda -\frac{1}{\lambda}$.
\end{example}

\begin{example} Let $X\sim N(\mu,{\sigma}^{2})$
\end{example}

Entropy is a measure of the randomness. For example, among the $\mbox{Ber}(p)$ distributions, the entropy is maximized at $p=1/2$ and minimized at $p=0\mbox{ or }1$. It quantifies the intuitive feeling that $\mbox{Ber}(1/2)$ is {\em more random} than $\mbox{Ber}(1/4)$. 

\begin{lemma}
\begin{enumerate}\setlength\itemsep{6pt}
\item If $|{\mathcal A}|=k$, then $0\le H(X)\le \log k$. $H(X)=0$ if and only if $X$ is degenerate and $H(X)=\log k$ if and only if $X\sim \mbox{Unif}({\mathcal A})$.
\item Let $f:{\mathcal A}\rightarrow {\mathcal B}$ and let $Y=f(X)$. Then $H(Y)\le H(X)$.
\item Let $X$ take values in ${\mathcal A}$ and $Y$ take values in ${\mathcal B}$ and let $Z=(X,Y)$. Then $H(Z)\le H(X)+H(Y)$ with equality if and only if $X$ and $Y$ are independent.
\end{enumerate}
\end{lemma}

\para{Gibbs measures} Let ${\mathcal A}$ be a countable set and let ${\mathcal H}:{\mathcal A}\rightarrow \mathbb{R}$ be a given function. For any $E\in \mathbb{R}$, consider the set of ${\mathcal P}_{E}$ of all probability mass functions on $\Omega$ under which ${\mathcal H}$ has expected value $E$. In other words,
$$
{\mathcal P}_{E}:=\{{\bf p}=\left(p_{i}\right)_{i\in {\mathcal A}}{\; : \;} \sum_{i\in {\mathcal A}} p(i){\mathcal H}(i)=E\}.
$$
${\mathcal P}_{E}$ is non-empty if and only if ${\mathcal H}_{\min}\le E\le {\mathcal H}_{\max}$.
\begin{lemma} Assume that ${\mathcal H}_{\min}\le E\le {\mathcal H}_{\max}$. Then, there is a unique pmf in ${\mathcal P}_{E}$ with maximal entropy and it is given by
$$
p_{\beta}(i)=\frac{1}{Z_{\beta}}e^{-\beta {\mathcal H}(i)}
$$ 
where $Z_{\beta}=\sum\limits_{i\in {\mathcal A}}e^{-\beta {\mathcal H}(i)}$ and the value of $\beta$ is chosen to satisfy 
$\frac{1}{Z_{\beta}}\frac{\partial Z_{\beta}}{\partial \beta}=E$.
\end{lemma}
This minimizing pmf is called the {\em Boltzmann-Gibbs} distribution. An analogous theorem holds for densities.

\begin{example} Let ${\mathcal A}=\{1,2,\ldots,n\}$ and ${\mathcal H}(i)=1$ for all $i$. Let $E=1$ so that ${\mathcal P}_{E}$ is the same as all pmfs on ${\mathcal A}$. Clearly $p_{\beta}(i)=\frac{1}{n}$ for all $i\le n$. Indeed, we know that the maximal entropy is attained by the uniform distribution.
\end{example}

\begin{example} Let ${\mathcal A}=\{0,1,2,\ldots\}$ and let ${\mathcal H}(i)=i$ for all $i$. Fix any $E>0$. The Boltzmann-Gibbs distribution is given by $p_{\beta}(i)=\frac{1}{Z_{\beta}}e^{-\beta i}$. This is just the Geometric distribution with parameter chosen to have mean $E$. 
\end{example}

\begin{example} Let us blindly apply the lemma to densities.
\begin{enumerate}\setlength\itemsep{6pt}
\item ${\mathcal A}=\mathbb{R}_{+}$ and ${\mathcal H}(x)=\lambda x$
\item ${\mathcal A}=\mathbb{R}$ and ${\mathcal H}(x)=x^{2}$. 
\end{enumerate}
\end{example}

}

\newpage
\vspace*{\fill}
\begin{center}
\Huge {\bf Statistics}
\end{center}
\vspace*{\fill}
\newpage
%\setcounter{section}{0}
\section{Introduction}
In statistics we are faced with data, which could be measurements in an experiment, responses in a survey etc. There will be some randomness, which may be inherent in the problem or due to errors in measurement etc. The problem in statistics is to make various kinds of inferences about the underlying distribution, from realizations of the random variables.  We shall consider a few basic types of problems encountered in statistics. We shall mostly deal with examples, but sufficiently many that the general ideas should become clear too. It may be remarked that we stay with the simplest ``textbook type problems'' but we shall also see some real data. Unfortunately we shall not touch upon the problems of current interest, which typically involve very huge data sets etc. Here are the kinds of problems we study.

\para{General setting} We shall have data (measurements perhaps), usually of the form $X_{1},\ldots ,X_{n}$ which are realizations of independent random variables  from a common distribution. The underlying distribution is not known. In the problems we consider, typically the distribution is known, except for the values of a few parameters. Thus, we may write the data as $X_{1},\ldots ,X_{n}$ i.i.d. $f_{\theta}(x)$ where $f_{\theta}(x)$ is a pdf or pmf for each value of the parameter(s) $\theta$. For example, the density could be of $N(\mu,{\sigma}^{2})$ (two unknown parameters $\mu$ and ${\sigma}^{2}$) or of $\mbox{Pois}(\lambda)$ (one unknown parameter $\lambda$). 

\para{(1) Estimation} Here, the question is to guess the value of the unknown $\theta$ from the sample $X_{1},\ldots ,X_{n}$. For example, if $X_{i}$ are i.i.d. from $\mbox{Ber}(p)$ distribution ($p$ is unknown), then a reasonable guess for $\theta$ would be the sample mean $\overline{X}_{n}$ (an {\em estimator}). Is this the only one? Is it the ``best'' one? Such questions are addressed in estimation.
 
\para{(2) Confidence intervals} Here again the problem is of estimating the value of a parameter, but instead of giving one value as a guess, we instead give an interval and quantify  how sure we are that the interval will contain the unknown parameter. For example, a coin with unknown probability $p$ of turning up head, is tossed $n$ times. Then, a confidence interval for $p$ could be of the form \[\begin{aligned} \left[\overline{X}_{n}-\frac{3}{\sqrt{n}}\sqrt{\overline{X}_{n}(1-\overline{X}_{n})},\overline{X}_{n}+\frac{3}{\sqrt{n}}\sqrt{\overline{X}_{n}(1-\overline{X}_{n})}\right] \end{aligned}\] where $\overline{X}_{n}$ is the proportion of heads in $n$ tosses. The reason for such an interval will come later. It turns out that if $n$ is large, one can say that with probability $0.99$ (``confidence level''), this interval will contain the true value of the parameter. 

\para{(3) Hypothesis testing} In this type of problem we are required to decide between two competing choices (``hypotheses''). For example, it is claimed that one batch of students is better than a second batch of students in mathematics. One way to check this is to give the same exam to students in both exams and record the scores. Based on the scores, we have to decide whether the first batch is better than the second (one hypothesis) or whether there is not much difference between the two (the other hypothesis). One can imagine that this can be done by comparing the sample means etc., but that will come later. 

A good analogy for testing problems is from law, where the judge has to decide whether an accused is guilty or not guilty. Evidence presented by lawyers take the role of data (but of course one does not really compute any probabilities quantitatively here!).

\para{(4) Regression} Consider two measurements, such as height and weight. It is reasonable to say that weight and height are positively correlated (if the height is larger, the weight tends to be larger too), but is there a more quantitative relationship? Can we predict the weight (roughly) from the height?  One could try to see if a linear function fits: $\mbox{wt.}=a\ \mbox{ht.}+b$ for some $a,b$. Or perhaps a more complicated fit such as $\mbox{wt.}=a\ \mbox{ht.}+b\ \mbox{ht.}^{2}+c$, etc. To see if this is a good fit, and to know what values of $a,b,c$ to take, we need data. Thus, the problem is that we have some data $(H_{i},W_{i})$, $i=1,2,\ldots ,n$, and based on this data we try to find the best linear fit (or the best quadratic fit) etc. 

As another example, consider the approximate law that the resistivity of a  material is proportional to the temperature. What is the constant of proportionality (for a given material). Here we have a law that says $R=aT$ where $a$ is not known. By taking many measurements at various temperatures we get data $(T_{i},R_{i})$, $i=1,2,\ldots ,n$. From this we must find the best possible $a$ (if all the data points were to lie on a line $y=ax$, there would be no problem. In reality they never will, and that is why the choice is an issue!).


%\section{Various things to include}

%\para{Data sets}
%\begin{enumerate}\setlength\itemsep{6pt}
%\item {\em Pie chart} \href{http://en.wikipedia.org/wiki/File:U.S._Distribution_of_Wealth,_2007.jpg}{Data on wealth distribution in the USA}
%\item {\em Correlation} \href{http://lib.stat.cmu.edu/DASL/Datafiles/SmokingandCancer.html}{Smoking and cancer}
%\item {\em Correlation} \href{http://lib.stat.cmu.edu/DASL/Datafiles/Brainsize.html}{Brain size and intelligence}
%\item {\em Mean or median?} \href{http://lib.stat.cmu.edu/DASL/Datafiles/Cavendish.html}{Density of earth}. There is an outlier, so summarizing with median is better.
%\item {\em Mean or median? Confidence interval} \href{http://lib.stat.cmu.edu/DASL/Datafiles/SpeedofLight.html}{Newcomb's data on speed of light}. The data given is the (t-24.8)*1000 milliseconds, the time taken for light to travel 7443.73 meters.
%\item {\em Two sample t-test} \href{http://lib.stat.cmu.edu/DASL/Datafiles/DRPScores.html}{Date on reading ability v/s classroom activities}
%\item {\em Linear regression} \href{http://lib.stat.cmu.edu/DASL/Datafiles/Hubble.html}{Hubble's data on receding galaxies}. An important point is that there is no $\alpha$, only $\beta$!
%\end{enumerate}

%\newpage
\section{Estimation problems}
Consider the following examples.
\begin{enumerate}\setlength\itemsep{6pt}
\item A coin has an unknown probability $p$ of turning up head. We wish to determine the value of $p$. For this, we toss the coin $100$ times and observe the outcomes. How to give a guess for the value of $p$ based on the data?
\item A factory manufacture light bulbs whose lifetimes may be assumed to be exponential random variables with a mean life-time $\mu$. We take a sample of $50$ bulbs at random and measure their life-times $X_{1},\ldots ,X_{50}$. Based on this data, how can we present a reasonable guess for $\mu$? We may want to do this so that the specifications can be printed on the product when sold. 
\item Can we guess the average height $\mu$ of all people in India by taking a random sample of $100$ people and measuring their heights?
\end{enumerate}
In such questions, there is an unknown parameter $\mu$ (there could be more than one unknown parameter too) whose value we are trying to guess based on the data. The data consists of i.i.d. random variables from a family of distributions. We assume that the family of distributions is known and the only unknown is (are) the value of the parameter(s). 
 Rather than present the ideas in abstract let us see a few examples.
 
 \begin{example}Let $X_{1},\ldots ,X_{n}$ be i.i.d. random variables with Exponential density $f_{\mu}(x)=\frac{1}{\mu}e^{-x/\mu}$ (fro $x>0$) where the value of $\mu>0$ is unknown. How to  {\em estimate} it using the data $X=(X_{1},\ldots ,X_{n})$?


This is the framework in which we would study the second example above, namely the lie-time distribution of light bulbs. Observe that we have parameterized the exponential family of distributions differently from usual. We could equivalently have considered $g_{\lambda}(x)=\lambda e^{-\lambda x}$ but the interest is then in estimating $1/\lambda$ (which is the expected value) rather than $\lambda$. Here are two methods.

\para{Method of moments} We observe that $\mu=\mathbf{E}_{\mu}[X_{1}]$, the mean of the distribution (also called {\em population mean}). Hence it seems reasonable to take the sample mean $\overline{X}_{n}$ as an estimate. On second thought, we realize that $\mathbf{E}_{\mu}[X_{1}^{2}]=2\mu^{2}$ and hence $\mu=\sqrt{\frac{1}{2}\mathbf{E}_{\mu}[X_{1}^{2}]}$. Therefore it also seems reasonable to take the corresponding sample quantity, $T_{n}:=\sqrt{\frac{1}{2n}(X_{1}^{2}+\ldots +X_{n}^{2})}$ as an estimate for $\mu$. One can go further and write $\mu$ in various ways as $\mu=\sqrt{\mbox{Var}_{\mu}(X_{1})}$, $\mu=\sqrt[3]{\frac{1}{6}\mathbf{E}_{\mu}[X_{1}^{3}]}$ etc. Each such expression motivates an estimate, just by substituting sample moments for population moments.

This is called estimating by the {\em method of moments} because we are equating the sample moments to population moments to obtain the estimate.

We can also use other features of the distribution, such as quantiles (we may call this  the ``method of quantiles''). In other words, obtain estimates by equating the sample quantiles to population quantiles. For example, the median of $X_{1}$ is $\mu\log 2$, hence a reasonable estimate for $\mu$ is $M_{n}/\log 2$, where $M_{n}$ is a sample median. Alternately, the $25\%$ quantile of $\mbox{Exponential}(1/\mu)$ distribution is $\mu\log(4/3)$ and hence another estimate for $\mu$ is $Q_{n}/\log(4/3)$ where $Q_{n}$ is a $25\%$ sample quantile.

\para{Maximum likelihood method} The joint density of $X_{1},\ldots ,X_{n}$ is $$g_{\mu}(x_{1},\ldots ,x_{n})=\mu^{-n}e^{-\mu(x_{1}+\ldots +x_{n})} \qquad \mbox{ if all }x_{i}>0$$ (since $X_{i}$ are independent, the joint density is a product). We evaluate the joint density at the observed data values. This is called the likelihood function. In other words, define,
$$
L_{X}(\mu) := \mu^{-n}e^{-\frac{1}{\mu}\sum_{i=1}^{n}X_{i}}.
$$
Two points: This is the joint density of $X_{1},\ldots ,X_{n}$, evaluated at the observed data. Further, we like to think of it as a function of $\mu$ with $X:=(X_{1},\ldots ,X_{n})$ being fixed. 

When $\mu$ is the actual value, then $L_{X}(\mu)$ is the ``likelihood'' of seeing the data that we have actually observed. The {\em maximum likelihood estimate} is that value of $\mu$ that maximizes the likelihood function. In our case, by differentiating and setting equal to zero we get,
$$
0 =\frac{d}{d\mu}L_{X}(\mu) = -n\mu^{-n-1}e^{-\frac{1}{\mu}\sum_{i=1}^{n}X_{i}}+\mu^{-n}\left(\frac{1}{\mu^{2}}\sum_{i=1}^{n}X_{i}\right)e^{-\frac{1}{\mu}\sum_{i=1}^{n}X_{i}}
$$
which is satisfied when $\mu=\frac{1}{n}\sum_{i=1}^{n}X_{i}=\overline{X}_{n}$. To distinguish this from the true value of $\mu$ which is unknown, it is customary to put a hat on the leter $\mu$. We write $\hat{\mu}_{MLE}=\overline{X}_{n}$. We should really verify whether $L(\mu)$ is maximized or minimized (or neither) at this point, but we leave it to you to do the checking (eg., by looking at the second derivative).
\end{example}

Let us see the same methods at work in two more examples.
\begin{example} Let $X_{1},\ldots ,X_{n}$ be i.i.d. Ber($p$) random variables where the value of $p$ is unknown. How to  {\em estimate} it using the data $X=(X_{1},\ldots ,X_{n})$?

\para{Method of moments} We observe that $p=\mathbf{E}_{p}[X_{1}]$, the mean of the distribution (also called {\em population mean}). Hence, a method of moments estimator would be   the sample mean $\overline{X}_{n}$. In this case, $\mathbf{E}_{p}[X_{1}^{2}]=p$ again but we don't get any new estimate because $X_{k}^{2}=X_{k}$ (as $X_{k}$ is $0$ or $1$)

\para{Maximum likelihood method} Now we have a probability mass function instead of density. The joint pmf of of $X_{1},\ldots ,X_{n}$ is $f_{p}(x_{1},\ldots ,x_{n}=p^{\sum_{i=1}^{n}x_{i}}(1-p)^{n-\sum_{i=1}^{n}x_{i}}$ when each $x_{i}$ is $0$ or $1$. The likelihood function is
$$
L_{X}(p) := p^{\sum_{i=1}^{n}x_{i}}(1-p)^{n-\sum_{i=1}^{n}x_{i}} = p^{n\overline{X}_{n}}(1-p)^{n(1-\overline{X}_{n})}.
$$
We need to find the value of $p$ that maximizes $L_{X}(p)$. Here is a trick that almost always simplifies calculations (try it in the previous example too!). Instead of maximizing $L_{X}(p)$, maximize $\ell_{X}(p)=\log L_{X}(p)$ (called the {\em log-likelihood function}). Since ``$\log$'' is an increasing function, the maximizer will remain the same. In our case, 
$$
\ell_{X}(p)=\overline{X}_{n}\log p + n(1-\overline{X}_{n})\log (1-p).
$$
 Differentiating and setting equal to $0$, we get $\hat{p}_{MLE}=\overline{X}_{n}$. Again the sample mean is the maximum likelihood estimate.
\end{example}

A last example.
\begin{example} Consider the two-parameter Laplace-density $f_{\theta,\alpha}(x)=\frac{1}{2\alpha}e^{-\frac{|x-\theta|}{\alpha}}$ for all $x\in \mathbb{R}$.  Check that $f_{\theta,\alpha}$ is indeed a density for all $\theta\in \mathbb{R}$ and $\alpha>0$.

Now suppose we have data $X_{1},\ldots ,X_{n}$ i.i.d. from $f_{\theta,\alpha}$ where we do not know the values of $\theta$ and $\alpha$. How to estimate the parameters?

\para{Method of moments} We compute
\begin{align*}
\mathbf{E}_{\theta,\alpha}[X_{1}]&=\frac{1}{2\alpha}\int\limits_{-\infty}^{+\infty}te^{-\frac{|t-\theta|}{\alpha}}dt = \frac{1}{2}\int\limits_{-\infty}^{+\infty}(\alpha s+\theta) e^{-|s|}ds = \theta.\\
\mathbf{E}_{\theta,\alpha}[X_{1}^{2}]&=\frac{1}{2\alpha}\int\limits_{-\infty}^{+\infty}t^{2}e^{-\frac{|t-\theta|}{\alpha}}dt = \frac{1}{2}\int\limits_{-\infty}^{+\infty}(\alpha s+\theta)^{2} e^{-|s|}ds = 2\alpha^{2}+\theta^{2}.
\end{align*}
Thus the variance is $\mbox{Var}_{\theta,\alpha}(X_{1})=2\alpha^{2}$. Based on this, we can take the method of moments estimate to be $\hat{\theta}_{n}=\overline{X}_{n}$ (sample mean) and $\hat{\alpha}_{n}=\frac{1}{\sqrt{2}}s_{n}$ where $s_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$. At the moment the ideas of defining sample variance as $s_{n}^{2}$ may look strange and it might be more natural to take $V_{n}:=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$ as an estimate for the population variance. As we shall see later, $s_{n}^{2}$ has some desirable properties that $V_{n}$ lacks. Whenever we say sample variance, we mean $s_{n}^{2}$, unless stated otherwise.

\para{Maximum likelihood method} The likelihood function of the data is
$$
L_{X}(\theta,\alpha)=\prod\limits_{k=1}^{n}\frac{1}{2\alpha}\exp\left\{-\frac{|X_{k}-\theta|}{\alpha}\right\}= 2^{-n}\alpha^{-n}\exp\left\{-\sum_{k=1}^{n}\frac{|X_{k}-\theta|}{\alpha}\right\}.
$$
The log-likelihood function is $$\ell_{X}(\theta,\alpha)=\log L(\theta,\alpha)=-n\log 2 - n\log \alpha -\frac{1}{\alpha}\sum_{k=1}^{n}|X_{k}-\theta|.$$ We know that\footnote{If you do not know here is an argument. Let $x_{1}<x_{2}<\ldots <x_{n}$ be $n$ distinct real numbers and let $a\in \mathbb{R}$. Rewrite $\sum_{k=1}^{n}|x_{k}-a|$ as $(|x_{1}-a|+|x_{n}-a|)+(|x_{2}-a|+|x_{n-1}-a|)+\ldots$. By triangle inequality, we see that $$|x_{1}-a|+|x_{n}-a|\ge x_{n}-x_{1}, \;\;\; |x_{2}-a|+|x_{n-1}-a|\ge x_{n-1}-x_{2}, \;\;\; |x_{3}-a|+|x_{n-2}-a|\ge x_{n-2}-x_{3}\ldots.
$$
Further the first inequality is an equality if and only if $x_{1}\le a\le x_{n}$, the second inequality is an equality if and only if $x_{2}\le a\le x_{n-1}$ etc. In particular, if $a$ is a median, then all these inequalities become equalities and shows that a median minimizes the given sum.
} for fixed $X_{1},\ldots ,X_{n}$, the value of $\sum_{k=1}^{n}|X_{k}-\theta|$ is minimized when $\theta=M_{n}$, the median of $X_{1},\ldots ,X_{n}$ (strictly speaking the median may have several choices, all of them are equally good). Thus we fix $\hat{\theta}=M_{n}$ and then we maximize $\ell(\hat{\theta},\alpha)$ over $\alpha$ by differentiating. We get $\hat{\alpha}=\frac{1}{n}\sum_{k=1}^{n}|X_{k}-\theta|$ (the sample mean-absolute deviation about the median). Thus the MLE of $(\theta,\alpha)$ is $(\hat{\theta},\hat{\alpha})$.
\end{example}

In homeworks and tutorials you will see several other estimation problems which we list in the exercise below.
\begin{exercise} Find an estimate for the unknown parameters by the method of moments and the maximum likelihood method.
 \begin{enumerate}\setlength\itemsep{6pt}
\item $X_{1},\ldots, X_{n}$ are i.i.d. $N(\mu,1)$. Estimate $\mu$. How do your estimates change if the distribution is $N(\mu,2)$?
\item $X_{1},\ldots, X_{n}$ are i.i.d. $N(0,{\sigma}^{2})$. Estimate ${\sigma}^{2}$. How do your estimates change if the distribution is $N(7,{\sigma}^{2})$?
\item $X_{1},\ldots, X_{n}$ are i.i.d. $N(\mu,{\sigma}^{2})$. Estimate $\mu$ and ${\sigma}^{2}$.
\end{enumerate}
[{\bf Note:} The first case is when ${\sigma}^{2}$ is known and $\mu$ is unknown. Then the known value of ${\sigma}^{2}$ may be used to estimate $\mu$. In the second case it is similar, now $\mu$ is known and ${\sigma}^{2}$ is not known. In the third case, both are unknown].
\end{exercise}

\begin{exercise} $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Geo}(p)$ Estimate $\mu=1/p$.
\end{exercise}

\begin{exercise} $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Pois}(\lambda)$ Estimate $\lambda$.
\end{exercise}

\begin{exercise} $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Beta}(a,b)$ Estimate $a,b$.
\end{exercise}

The following exercise is approachable by the same methods but requires you to think a little.
\begin{exercise} $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Uniform}[a,b]$ Estimate $a,b$.
\end{exercise}

\section{Properties of estimates}
We have seen that there may be several competing estimates that can be used to estimate a parameter. How can one choose between these estimates? In this section we present some properties that may be considered desirable in an estimator. However, having these properties does not lead to an unambiguous choice of one estimate as the best for a problem. 

\para{The setting} Let $X_{1},\ldots ,X_{n}$ be i.i.d random variables with a common density $f_{\theta}(x)$. The parameter $\theta$ is unknown and the goal is to estimate it. Let $T_{n}$ be an estimator for $\theta$, this just means that $T_{n}$ is a function of $X_{1},\ldots ,X_{n}$ (in words, if we have the data at hand, we should be able to compute the value of $T_{n}$). 

\para{Bias} Define the {\em bias} of the estimator as $\mbox{bias}_{T_{n}}(\theta):=\mathbf{E}_{\theta}[T_{n}]-\theta$. If $\mbox{Bias}_{T_{n}}(\theta)=0$ for all values of the parameter $\theta$ then we say that $T_{n}$ is  {\em unbiased} for $\theta$. Here we write $\theta$ in the subscript of $\mathbf{E}_{\theta}$ to remind ourself that in computing the expectation we use the density $f_{\theta}$. However we shall often omit the subscript for simplicity. 

\para{Mean-squared error} The {\em mean squared error} of $T_{n}$ is defined as $\mbox{m.s.e.}_{T_{n}}(\theta)=\mathbf{E}_{\theta}[(T_{n}-\theta)^{2}]$. This is a function of $\theta$. Smaller it is, better our estimate.

In computing mean squared error, it is useful to observe the formula
$$
\mbox{m.s.e.}_{T_{n}}(\theta) = \mbox{Var}_{T_{n}}(\theta) + \left(\mbox{Bias}_{T_{n}}(\theta)\right)^{2}.
$$
To prove this, consider and random variable $Y$ with mean $\mu$ and observe that for any real number $a$ we have
\begin{align*}
\mathbf{E}[(Y-a)^{2}] &=\mathbf{E}[(Y-\mu+\mu-a)^{2}] = \mathbf{E}[(Y-\mu)^{2}]+(\mu-a)^{2}+2(\mu-a)\mathbf{E}[Y-\mu] \\
&= \mathbf{E}[(Y-\mu)^{2}]+(\mu-a)^{2} = \mbox{Var}(Y) + (\mu-a)^{2}.
\end{align*}
Use this identity with $T_{n}$ in place of $Y$ and $\theta$ in place of $a$.

\begin{remark} An analogy. Consider shooting with a rifle having a telescopic sight. A given target can be missed for two reasons. One, the marksman may be unskilled and shoot all over the place, sometimes a meter to the right of the target, sometimes a meter to the left, etc. In this case, the shots have a large variance. Another person may consistently hit a point 20 cm. to the right of the target. Perhaps the telescopic sight is not set right, and this caused the systematic error. This is the bias. Both bias and variance contribute to missing the target. 
\end{remark}


\begin{example} Let $X_{1},\ldots ,X_{n}$ be i.i.d. $N(\mu,{\sigma}^{2})$. Let $V_{n}=\frac{1}{n}\sum_{k=1}^{n}(X_{k}-\overline{X}_{n})^{2}$ be an estimate for ${\sigma}^{2}$. By expanding the squares we get
$$
V_{n}=\overline{X}_{n}^{2}+\frac{1}{n}\sum_{k=1}^{n}X_{k}^{2} -\frac{2}{n}\overline{X}_{n}\sum_{k=1}^{n}X_{k} = \left(\frac{1}{n}\sum_{k=1}^{n}X_{k}^{2} \right)-\overline{X}_{n}^{2}.
$$ 
It is given that $\mathbf{E}[X_{k}]=\mu$ and $\mbox{Var}(X_{k})={\sigma}^{2}$. Hence $\mathbf{E}[X_{k}^{2}]=\mu^{2}+{\sigma}^{2}$. We have seen before that $\mbox{Var}(\overline{X}_{n})={\sigma}^{2}$ and $\mathbf{E}[\overline{X}_{n}]=\mu$.  Hence $\mathbf{E}[\overline{X}_{n}^{2}]=\mu^{2}+\frac{{\sigma}^{2}}{n}$. Putting all this together, we get
$$
\mathbf{E}\left[V_{n} \right] = \left( \frac{1}{n}\sum_{k=1}^{n}\mu^{2}+{\sigma}^{2} \right) - \left(\mu^{2}+\frac{{\sigma}^{2}}{n}\right)  = \frac{n-1}{n}{\sigma}^{2}.
$$
Thus, the bias of $V_{n}$ is $\frac{n-1}{n}{\sigma}^{2}-{\sigma}^{2}=-\frac{1}{n}{\sigma}^{2}$.
\end{example}

\begin{example} For the same setting as the previous example, suppose $W_{n}=\frac{1}{n}\sum_{k=1}^{n}(X_{k}-\mu)^{2}$. Then it is easy to see that $\mathbf{E}[W_{n}]={\sigma}^{2}$. Can we say that $W_{n}$ is an unbiased estimate for ${\sigma}^{2}$? There is a hitch!

If the value of $\mu$ is unknown, then $W_{n}$ is {\em not} an estimate (cannot compute it using $X_{1},\ldots ,X_{n}$!). However if $\mu$ is known, then it is an unbiased estimate. For example, if we knew that $\mu=0$, then $W_{n}=\frac{1}{n}\sum_{k=1}^{n}X_{k}^{2}$ is an unbiased estimate for ${\sigma}^{2}$. 

When $\mu$ is unknown, we define $s_{n}^{2}=\frac{1}{n-1}\sum_{k=1}^{n}(X_{k}-\overline{X}_{n})^{2}$. Clearly $s_{n}^{2}=\frac{n}{n-1}V_{n}$ and hence $\mathbf{E}[s_{n}^{2}]=\frac{n}{n-1}\mathbf{E}[V_{n}]= {\sigma}^{2}$. Thus, $s_{n}^{2}$ is an unbiased estimate for ${\sigma}^{2}$. Note that $s_{n}^{2}$ depends only on the data and hence it is an estimate, whether $\mu$ is known or unknown.
\end{example}
All the remarks in the above two examples apply for any distribution, i.e., 
\begin{enumerate}\setlength\itemsep{6pt}
\item The sample mean is unbiased for the population mean.
\item The sample variance $s_{n}^{2}=\frac{1}{n-1}\sum_{k=1}^{n}(X_{k}-\overline{X}_{n})^{2}$ is unbiased for the population variance. But $V_{n}=\frac{1}{n}\sum_{k=1}^{n}(X_{k}-\overline{X}_{n})^{2}$ is not, in fact $\mathbf{E}[V_{n}]=\frac{n-1}{n}{\sigma}^{2}$.
\end{enumerate}
 It appears that $s_{n}^{2}$ is better, but the following remark says that one should be cautious in making such a statement.
\begin{remark} In case of $N(\mu,{\sigma}^{2})$ data, it turns out that although $s_{n}^{2}$ is unbiased and $V_{n}$ is biased, the mean squared error of $V_{n}$ is smaller! Further $V_{n}$ is the maximum likelihood estimate of ${\sigma}^{2}$! Overall, unbiasedness is not so important as having smaller mean squared error, but for estimating variance (when the mean is not known), we always use $s_{n}^{2}$. The computation of the m.s.e is a bit tedious, so we skip it here.
\end{remark}

\begin{example} Let $X_{1},\ldots ,X_{n}$ be i.i.d. $\mbox{Ber}(p)$. Then $\overline{X}_{n}$ is an estimate for $p$. It is unbiased since $\mathbf{E}[\overline{X}_{n}]=p$. Hence, the m.s.e of $\overline{X}_{n}$ is just the variance which is equal to $p(1-p)/n$.
\end{example}

\para{A puzzle} A coin $C_{1}$ has probability $p$ of turning up head and a coin $C_{2}$ has probability $2p$ of turning up head. All we know is that $0<p<\frac{1}{2}$. You are given $20$ tosses. You can choose all tosses from $C_{1}$ or all tosses from $C_{2}$ or some tosses from each (the total is $20$). If the objective is to estimate $p$, what do you do?

\para{Solution} If we choose to have all $n=20$ tosses from $C_{1}$, then we get $X_{1},\ldots ,X_{n}$ that are i.i.d. $\mbox{Ber}(p)$. An estimate for $p$ is $\overline{X}_{n}$ which is unbiased and hence $\mbox{MSE}_{\overline{X}_{n}}(p)=\mbox{Var}(\overline{X}_{n})=p(1-p)/n$. On the other hand if we choose to have all $20$ tosses from $C_{2}$, then we get $Y_{1},\ldots ,Y_{n}$ that are i.i.d. $\mbox{Ber}(2p)$. The estimate for $p$ is now $\overline{Y}_{n}/2$ which is also unbiased and has $\mbox{MSE}_{\overline{Y}_{n}/2}(p)=\mbox{Var}(\overline{Y}_{n})=2p(1-2p)/4 = p(1-2p)/2$. It is not hard to see that for all $p<1/2$, $\mbox{MSE}_{\overline{Y}_{n}/2}(p)<\mbox{MSE}_{\overline{X}_{n}}(p)$ and hence choosing $C_{2}$ is better, at least by mean-squared criterion! It can be checked that if we choose to have $k$ tosses from $C_{1}$ and the rest from $C_{2}$, the MSE of the corresponding estimate will be between the two MSEs found above and hence not better than $\overline{Y}_{n}/2$.




\para{Another puzzle} A factory produces light bulbs having an exponential distribution with mean $\mu$. Another factory produces light bulbs having an exponential distribution with mean $2\mu$. Your goal is to estimate $\mu$. You are allowed to choose a total of $50$ light bulbs (all from the first or all from the second or some from each factory). What do you do?

\para{Solution} If we pick all $n=50$ bulbs from the first factory, we see $X_{1},\ldots ,X_{n}$ i.i.d. $\mbox{Exp}(1/\mu)$. The estimate for $\mu$ is $\overline{X}_{n}$ which has $\mbox{MSE}_{\overline{X}_{n}}(\mu)=\mbox{Var}(\overline{X}_{n})=\mu^{2}/n$. If we choose  all bulbs from factory $2$ we get $Y_{1},\ldots ,Y_{n}$ i.i.d. $\mbox{Exp}(1/2\mu)$. The estimate for $\mu$ is $\overline{Y}_{n}/2$. But $\mbox{MSE}_{\overline{Y}_{n}/2}(\mu)=\mbox{Var}(\overline{Y}_{n}/2)=(2\mu)^{2}/4n=\mu^{2}/n$. The two mean-squared errors are exactly the same!

\para{Probabilistic thinking} Is there any calculation-free explanation why the answers to the two puzzles are as above? Yes, and it is illustrative of what may be called probabilistic thinking. Take the second puzzle. Why are the two estimates same by mean-squared error? Is one better by some other criterion? 

Recall that if $X\sim \mbox{Exp}(1/\mu)$ then $X/2\sim \mbox{Exp}(1/2\mu)$ and vice versa. Therefore, if we have data from $\mbox{Exp}(1/\mu)$ distribution, then we can divided all the numbers by $2$ and convert it into data from $\mbox{Exp}(1/2\mu)$ distribution. Conversely if we have data from $\mbox{Exp}(1/2\mu)$ distribution, then we can convert it into data from $\mbox{Exp}(1/\mu)$ distribution by multiplying each number by $2$. Hence there should be no advantage in choosing either factory. We leave it for you to think in analogous ways why in the first puzzle $C_{2}$ is better than $C_{1}$.


\section{Confidence intervals}
 So far, in estimating of an unknown parameter, we give a single number as our guess for the known parameter. It would be better to give an interval and say with what confidence we expect the true parameter to lie within it.  As a very simple example, suppose we have one random variable $X$ with $N(\mu,1)$ distribution. How do we estimate $\mu$? Suppose the observed value of $X$ is $2.7$. Going by any method, the guess for $\mu$ would be $2.7$ itself. But of course $\mu$ is not equal to $X$, so we would like to give an interval in which $\mu$ lies. How about $[X-1,X+1]$? Or $[X-2,X+2]$? Using normal tables, we see that $\mathbf{P}(X-1<\mu<X+1)=\mathbf{P}(-1<(X-\mu)<1)=\mathbf{P}(-1<Z<1) \approx 0.68$ and similarly $\mathbf{P}(X-2<\mu<X+2)\approx 0.95$. Thus, by making the interval longer we can be more confident that the true parameter lies within. But the accuracy of our statement goes down (if you want to know the average height of people in India, and the answer you give is ``between 100cm and 200cm'', it is very probably correct, but of little use!). The probability with which our CI contains the unknown parameter is called the level of confidence. Usually we fix the level of confidence, say as $0.90$ and find an interval {\em as short as possible} but subject to the condition that it should have a confidence level of $0.90$.
 
 In this section we consider the problem of confidence intervals in Normal population. In the next we see a few other examples.
 
\para{The setting} Let $X_{1},\ldots ,X_{n}$ be i.i.d. $N(\mu,{\sigma}^{2})$ random variables. We consider four situations.
\begin{enumerate}\setlength\itemsep{6pt}
\item Confidence interval for $\mu$ when ${\sigma}^{2}$ is known.
\item Confidence interval for ${\sigma}^{2}$ when $\mu$ is known.
\item Confidence interval for $\mu$ when ${\sigma}^{2}$ is unknown.
\item Confidence interval for ${\sigma}^{2}$ when $\mu$ is unknown.
\end{enumerate}

A starting point in finding a confidence interval for a parameter is to first start with an estimate for the parameter. For example, in finding a CI for $\mu$, we may start with $\overline{X}_{n}$ and enlarge it to an interval $[\overline{X}_{n}-a,\overline{X}_{n}+a]$. Similarly, in finding a CI for ${\sigma}^{2}$ we use the estimate $s_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$ if $\mu$ is unknown and $W_{n}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2}$ if the value of $\mu$ is known.

\subsection{Estimating $\mu$ when ${\sigma}^{2}$ is known} We look for a confidence interval of the form $I_{n}=[\overline{X}_{n}-a,\overline{X}_{n}+a]$. Then,
$$
\mathbf{P}\left(I_{n}\ni \mu\right) = \mathbf{P}\left(-a\le \overline{X}_{n}-\mu\le a\right) =\mathbf{P}\left(-\frac{a\sqrt{n}}{{\sigma}}\le \frac{\sqrt{n}(\overline{X}_{n}-\mu)}{{\sigma}} \le \frac{a\sqrt{n}}{{\sigma}}\right)
$$
Now we use two facts about normal distribution that we have seen before. 
\begin{enumerate}\setlength\itemsep{6pt}
\item If $Y\sim N(\mu,{\sigma}^{2})$ then $aX+b\sim N(a\mu+b,a^{2}{\sigma}^{2})$. 
\item If $Y_{1}\sim N(\mu,{\sigma}^{2})$ and $Y_{2}\sim N(\nu,\tau^{2})$ and they are independent, then $X+Y\sim N(\mu+\nu,{\sigma}^{2}+\tau^{2})$. 
\end{enumerate}
Consequently, $\overline{X}_{n}\sim N(0,{\sigma}^{2}/n)$ and $\frac{\sqrt{n}(\overline{X}_{n}-\mu)}{{\sigma}}\sim N(0,1)$. 
Therefore, 
$$
 \mathbf{P}\left(I_{n}\ni \mu\right)
 = \mathbf{P}(-\frac{a\sqrt{n}}{{\sigma}}\le Z\le -\frac{a\sqrt{n}}{{\sigma}})
$$
 where $Z\sim N(0,1)$. Fix any $0<\alpha<1$ and denote by $z_{\alpha}$ the number such that $\mathbf{P}(Z>z_{\alpha})=\alpha$ (in other words, $z_{\alpha}$ is the $(1-\alpha)$-quantile of the standard normal distribution). For example, from normal tables we find that $z_{0.05}\approx1.65$ and $z_{0.005}\approx 2.58$ etc.
 
  If we set $a=z_{\alpha/2}{\sigma}/\sqrt{n}$, we get 
$$
\mathbf{P}\left(\left[\overline{X}_{n}-\frac{{\sigma}}{\sqrt{n}}z_{\alpha/2},\overline{X}_{n}+\frac{{\sigma}}{\sqrt{n}}z_{\alpha/2}\right]\ni \mu\right)=1-\alpha.
$$
This is our confidence interval.

\subsection{Estimating ${\sigma}^{2}$ when $\mu$ is known}
Since $\mu$ is known, we use $W_{n}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2}$ to estimate ${\sigma}^{2}$. Here is an exercise.
\begin{exercise} Let $Z_{1},\ldots ,Z_{n}$ be i.i.d. $N(0,1)$ random variables. Then, $Z_{1}^{2}+\ldots +Z_{n}^{2}\sim \mbox{Gamma}(n/2,1/2)$. 
\end{exercise}
\noindent{\bf Solution:} For $t>0$ we have
\begin{align*}
\mathbf{P}\{Z_{1}^{2}\le t\} &= \mathbf{P}\{-\sqrt{t}\le Z_{1}\le \sqrt{t}\} 
= 2\int\limits_{0}^{\sqrt{t}}\frac{1}{\sqrt{2\pi}}e^{-u^{2}/2}du = \frac{1}{\sqrt{2\pi}}\int\limits_{0}^{t}e^{-s/2}s^{-1/2}ds.
\end{align*}
Differentiate w.r.t $t$ to see that the density of $Z_{1}^{2}$ is $h(t)=\frac{1}{\sqrt{\pi}}e^{-t/2}t^{-1/2}\sqrt{(1/2)}$, which is just the $\mbox{Gamma}(\frac{1}{2},\frac{1}{2})$ density. 

Now, each $Z_{k}^{2}$ has the same $\mbox{Gamma}(\frac{1}{2},\frac{1}{2})$ density, and they are independent. Earlier we have seen that when we add independent Gamma random variables with the same scale parameter, the sum has a Gamma distribution with the same scale but whose shape parameter is the sum of the shape parameters of the individual summands. Therefore, $Z_{1}^{2}+\ldots +Z_{n}^{2}$ has $\mbox{Gamma}(n/2,1/2)$ distribution. This completes the solution to the exercise.

\medskip


In statistics, the distribution $\mbox{Gamma}(1/2,1/2)$ is usually called the {\em chi-squared distribution with $n$ degrees of freedom}. Let $\chi_{n}^{2}\left(\alpha\right)$ denote the $1-\alpha$ quantile of this distribution. Similarly, $\chi_{n}^{2}\left(1-\alpha\right)$ is the $\alpha$ quantile (i.e., the probability for the chi-squared random variable to fall below $\chi_{n}^{2}\left(1-\alpha\right)$ is exactly $\alpha$).

When $X_{i}$ are i.i.d. $N(\mu,{\sigma}^{2})$, we know that $(X_{i}-\mu)/{\sigma}$ are i.i.d. $N(0,1)$. Hence, by the above fact, we see that
$$
 \frac{nW_{n}}{{\sigma}^{2}}=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{{\sigma}}\right)^{2} 
$$
has chi-squared distribution with $n$ degrees of freedom. Hence
\begin{align*}
\mathbf{P}\left\{   \frac{nW_{n}}{\chi_{n}^{2}\left(\frac{\alpha}{2}\right)} \le {\sigma}^{2}\le \frac{nW_{n}}{\chi_{n}^{2}\left(1-\frac{\alpha}{2}\right)}\right\}&=\mathbf{P}\left\{ \chi_{n}^{2}\left(1-\frac{\alpha}{2}\right) \le \frac{nW_{n}}{{\sigma}^{2}} \le \chi_{n}^{2}\left(\frac{\alpha}{2}\right)\right\}=1-\alpha.
\end{align*}
 Thus, $\left[\frac{ns_{n}^{2}}{\chi_{n-1}^{2}\left(\frac{\alpha}{2}\right)},\frac{ns_{n}^{2}}{\chi_{n-1}^{2}\left(1-\frac{\alpha}{2}\right)}\right]$ is a $(1-\alpha)$-confidence interval for ${\sigma}^{2}$.

\para{An important result} Before going to the next two confidence interval problems, let us try to understand the two examples already covered. In both cases, we came up with a random variable ($\sqrt{n}(\overline{X}_{n}-\mu)/{\sigma}$ and $W_{n}/{\sigma}^{2}$, respectively) which involved the data and the unknown parameter  whose distributions we knew (standard normal and $\chi^{2}_{n}$, respectively) and these distributions do not depend on any parameters. This is generally the key step in any confidence interval problem. For the next two problems, we cannot use the same two random variables as above as they depend on the other unknown parameter too (i.e.,  $\sqrt{n}(\overline{X}_{n}-\mu)/{\sigma}$ uses ${\sigma}$ which will be unknown and $W_{n}/{\sigma}^{2}$ uses $\mu$ which will be unknown). Hence, we need a new result that we state without proof.

\begin{theorem}\label{thm:indepofsamplemeanandvar} Let $Z_{1},\ldots ,Z_{n}$ be i.i.d. $N(\mu,{\sigma}^{2})$ random variables. Let $\overline{Z}_{n}$ and $s_{n}^{2}$ be the sample mean and the sample variance, respectively. Then, 
$$
\overline{Z}_{n}\sim N(\mu,\frac{{\sigma}^{2}}{n}), \;\;  \frac{(n-1)s_{n}^{2}}{{\sigma}^{2}}\sim \chi^{2}_{n-1}, 
$$
and the two are independent.
\end{theorem}
This is not too hard to prove (a muscle-flexing exercise in change of variable formula) but we skip the proof. Note two important features. First, the surprising independence of the sample mean and the sample variance. Second, the sample variance (appropriately scaled) has $\chi^{2}$ distribution, just like $W_{n}$ in the previous example, but the degree of freedom is reduced by $1$. Now we use this theorem in computing confidence intervals.


\subsection{Estimating ${\sigma}^{2}$ when $\mu$ is unknown}
The estimate $s_{n}^{2}$ must be used as $W_{n}$ depends on $\mu$ which is unknown. Theorem~{thm:indepofsamplemeanandvar} tells us that $\frac{(n-1)s_{n}^{2}}{{\sigma}^{2}}\sim \chi^{2}_{n-1}$. Hence, by the same logic as before we get
\begin{align*}
\mathbf{P}\left\{   \frac{(n-1)s_{n}^{2}}{\chi_{n-1}^{2}\left(\frac{\alpha}{2}\right)} \le {\sigma}^{2}\le \frac{(n-1)s_{n}^{2}}{\chi_{n-1}^{2}\left(1-\frac{\alpha}{2}\right)}\right\}&=\mathbf{P}\left\{ \chi_{n-1}^{2}\left(1-\frac{\alpha}{2}\right) \le \frac{(n-1)s_{n}^{2}}{{\sigma}^{2}} \le \chi_{n-1}^{2}\left(\frac{\alpha}{2}\right)\right\} \\
&=1-\alpha.
\end{align*}
 Thus, $\left[\frac{(n-1)s_{n}^{2}}{\chi_{n-1}^{2}\left(\frac{\alpha}{2}\right)} ,\frac{(n-1)s_{n}^{2}}{\chi_{n-1}^{2}\left(1-\frac{\alpha}{2}\right)}\right]$ is a $(1-\alpha)$-confidence interval for ${\sigma}^{2}$.

If $\mu$ is known, we could use the earlier confidence interval using $W_{n}$, or simply ignore the knowledge of $\mu$ and use the above confidence interval using $s_{n}^{2}$. What is the difference? The cost of ignoring the knowledge of $\mu$ is that the second confidence interval will be typically larger, although for large $n$ the difference is slight. On the other hand, if our knowledge of $\mu$ was inaccurate, then the first confidence interval is invalid (we have no idea what its level of confidence is!) which is more serious. In realistic situations it is unlikely that we will know one of the parameters but not the other - hence, most often one just uses the confidence interval based on $s_{n}^{2}$.


\subsection{Estimating $\mu$ when ${\sigma}^{2}$ is unknown} The earlier confidence interval We look for a confidence interval $[\overline{X}_{n}-\frac{{\sigma}}{\sqrt{n}}z_{\alpha/2},\overline{X}_{n}+\frac{{\sigma}}{\sqrt{n}}z_{\alpha/2}]$ cannot be used as we do not know the value of ${\sigma}$.

A natural idea would be to use the estimate $s_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$ in place of ${\sigma}^{2}$. However, recall that the earlier confidence interval (in particular,  the cut-off values $z_{\alpha/2}$ in the CI)  was an outcome of the fact that
$$
\frac{\sqrt{n}(\overline{X}_{n}-\mu)}{{\sigma}}\sim N(0,1).
$$
Is it true if ${\sigma}$ is replaced by $s_{n}$? Actually no, but we have a different distribution called {\em Student's $t$-distribution}.

\begin{exercise} Let $Z\sim N(0,1)$ and $S^{2}\sim \chi^{2}_{n}$ be independent. Then, the density of $\frac{Z}{S/\sqrt{n}}$ is given by 
$$
\frac{1}{\sqrt{n-1}\mbox{Beta}(\frac{1}{2},\frac{n-1}{2})}\frac{1}{\left(1+\frac{t^{2}}{n-1}\right)^{\frac{n}{2}}}
$$
for all $t\in \mathbb{R}$. This is known as {\em Student's $t$-distribution}.
\end{exercise}
The exact density of  $t$-distribution is not important to remember, so the above exercise is optional. The point is that it can be computed from the change of variable formula and that by numerical integration its CDF can be tabulated.  

How does this help us? From Theorem~\ref{thm:indepofsamplemeanandvar} we know that $\frac{\sqrt{n}(\overline{X}_{n}-\mu)}{{\sigma}}\sim N(0,1)$, $\frac{(n-1)s_{n}^{2}}{{\sigma}^{2}}\sim \chi^{2}_{n-1}$, and the two are independent. Take these random variables in the above exercise to conclude that $\frac{\sqrt{n}(\overline{X}_{n}-\mu)}{s_{n}}$ has $t_{n-1}$ distribution. 


The $t$-distribution is symmetric about zero (the density at $t$ and at $-t$ are the same). Further, as the number of degrees of freedom goes to infinity, the $t$-density converges to the standard normal density. What we need to know is that there are tables from which we can read off specific quantiles of the distribution. In particular, by $t_{n}(\alpha)$ we mean the $1-\alpha$ quantile of the $t$-distribution with $n$ degrees of freedom. Then of course, the $\alpha$ quantile is $-t_{n}(\alpha)$.

Returning to the problem of the confidence interval, from the fact stated above, we see that (use $T_{n}$ to indicate a random variable having $t$-distribution with $n$ degrees of freedom).
\begin{align*}
& \mathbf{P}\left(\overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}t_{n-1}\left(\frac{\alpha}{2}\right)\le \mu \le\overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}t_{n-1}\left(\frac{\alpha}{2}\right) \right) \\
&=
\mathbf{P}\left(-t_{n-1}\left(\frac{\alpha}{2}\right)\le \frac{\sqrt{n}(\overline{X}_{n}-\mu)}{s_{n}}\le t_{n-1}\left(\frac{\alpha}{2}\right)\right) \\
&= \mathbf{P}\left(-t_{n-1}\left(\frac{\alpha}{2}\right)\le T_{n-1}\le t_{n-1}\left(\frac{\alpha}{2}\right)\right) \\
&= 1-\alpha.
\end{align*}
Hence, our $(1-\alpha)$-confidence interval is $\left[\overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}t_{n-1}\left(\frac{\alpha}{2}\right),\overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}t_{n-1}\left(\frac{\alpha}{2}\right)\right]$.

\begin{remark} We remarked earlier that as $n\rightarrow \infty$, the $t_{n-1}$ density approaches the standard normal density. Hence, $t_{n-1}(\alpha)$ approaches $z_{\alpha}$ for any $\alpha$ (this can be seen by looking at the $t$-table for large degree of freedom). Therefore, when $n$ is large, we may as well use
$$
\left[\overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}z_{\alpha/2},\overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}z_{\alpha/2}\right].
$$
Strictly speaking the level of confidence is smaller than for the one with $t_{n-1}(\alpha/2)$. However for $n$ large the level of confidence is quite close to $1-\alpha$.
\end{remark}
 
 
\section{Confidence interval for the mean}
Now suppose $X_{1},\ldots ,X_{n}$ are i.i.d. random variables from some distribution with mean $\mu$ and variance ${\sigma}^{2}$, both unknown. How can we construct a confidence interval for $\mu$?

In case of normal distribution, recall that the $(1-\alpha)$-CI that we gave was
$$
\left[\overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}t_{n-1}\left(\frac{\alpha}{2}\right),\overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}t_{n-1}\left(\frac{\alpha}{2}\right)\right] or \left[\overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}z_{\alpha/2},\overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}z_{\alpha/2}\right]
$$
 Is this a valid confidence interval in general? The answer is ``No'' for both. If $X_{i}$ are from some general distribution then the distributions of $\sqrt{n}(\overline{X}_{n}-\mu)/s_{n}$ and $\sqrt{n}(\overline{X}_{n}-\mu)/{\sigma}$ are very complicated to find. Even if $X_{i}$ come from binomial or exponential family, these distributions will depend on the parameters in a complex way (in particular, the distributions are not free from the parameters, which is important in constructing confidence intervals). 
 
 But suppose $n$ is large. Then the sample variance is close to population variance and hence $s_{n}\approx {\sigma}
$. Further, by CLT, we know that $\sqrt{n}(\overline{X}_{n}-\mu)/{\sigma}$ has approximately    $N(0,1)$ distribution. Hence, we see that
$$
\mathbf{P}\left\{-z_{\alpha/2}\le  \frac{\sqrt{n}(\overline{X}_{n}-\mu)}{s_{n}} \le z_{\alpha/2}\right\} \approx \Phi(z_{\alpha/2})-\Phi(-z_{\alpha/2}) =1-\alpha.
$$
Consequently, we may say that 
$$
\mathbf{P}\left\{ \overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}z_{\alpha/2} \le \mu \le \overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}z_{\alpha/2}\right\} \approx 1-\alpha.
$$
Thus, $\left[\overline{X}_{n}-\frac{s_{n}}{\sqrt{n}}z_{\alpha/2}, \overline{X}_{n}+\frac{s_{n}}{\sqrt{n}}z_{\alpha/2} \right]$ is an approximate $(1-\alpha)$-confidence interval. Further, when $n$ is large, the difference between $V_{n}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$ and $V_{n}:=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}$ is small (indeed, $s_{n}^{2}=(n/(n-1))V_{n}$). Hence it is also okay to use $\left[\overline{X}_{n}-\frac{\sqrt{V_{n}}}{\sqrt{n}}z_{\alpha/2}, \overline{X}_{n}+\frac{\sqrt{V_{n}}}{\sqrt{n}}z_{\alpha/2} \right]$ as an approximate $(1-\alpha)$-confidence interval.
%%%\newpage

\begin{example} Let $X_{1},\ldots ,X_{n}$ be i.i.d. $\mbox{Ber}(p)$. Consider the problem of finding a confidence interval for $p$. Since each $X_{i}$ is $0$ or $1$, observe that 
$$
\hat{s}_{n}^{2}= \frac{1}{n}\sum_{i=1}^{n}X_{i}^{2} - \overline{X}_{n}^{2} = \overline{X}_{n}-(\overline{X}_{n})^{2} = \overline{X}_{n}(1-\overline{X}_{n}).
$$
\end{example}
Hence, an approximate $(1-\alpha)$-CI for $p$ is given by 
$$
\left[\overline{X}_{n}-z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}, \overline{X}_{n}+z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}\right].
$$

\section{Actual confidence by simulation}
Suppose we have a candidate confidence interval whose confidence we do not know. For example, let us take the confidence interval $$
\left[\overline{X}_{n}-z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}, \overline{X}_{n}+z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}\right].
$$ for the parameter $p$ of i.i.d. $\mbox{Ber}(p)$ samples. We saw that for large $n$ this has approximately $(1-\alpha)$ confidence. But how large is large? One way to check this is by simulation. We explain how.

Take $p=0.3$ and $n=10$. Simulate $n=10$ independent $\mbox{Ber}(p)$ random variables and compute the confidence interval given above. Check whether it contains the true value of $p$ (i.e., $0.3)$ or not. Repeat this exercise $10000$ times and see what proportion of times it contains $0.3$. That proportion is the true confidence, as opposed to $1-\alpha$ (which is valid only for large $n$). Repeat this experiment with $n=20$, $n=30$ etc. See how close the actual confidence is to $1-\alpha$. Repeat this experiment with different value of $p$. The $n$ you need to get close to $1-\alpha$ will depend on $p$ (in particular, on how close $p$ is to $1/2$).

This was about checking the validity of a confidence interval that was specified. In a real situation, it may be that we can only get $n=20$ samples. Then what can we do? If we have an idea of the approximate value of $p$, we can first simulate $\mbox{Ber}(p)$ random numbers on a computer. We compute the sample mean each time, and repeat  $10000$ times to get so many values of the sample mean. Note that the histogram of these $10000$ values tells us (approximately) the actual distribution of $\overline{X}_{n}$. Then we can find $t$ (numerically) such that $[\overline{X}_{n}-t,\overline{X}_{n}+t]$ contains the true value of $p$ in $(1-\alpha)$-proportion of the $10000$ trials. Then, $[\overline{X}_{n}-t,\overline{X}_{n}+t]$ is a $(1-\alpha)$-CI for $p$. Alternately, we may try a CI of the form $$
\left[\overline{X}_{n}-t\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}, \overline{X}_{n}+t\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}\right].
$$
where we choose $t$ numerically to get $(1-\alpha)$ confidence.

\para{Summary} The gist of this discussion is this. In the neatly worked out examples of the previous sections, we got explicit confidence intervals. But we assumed that we knew the data came from $N(\mu,{\sigma}^{2})$ distribution. What if that is not quite right? What if it is not any of the nicely studied distributions? The results also become invalid in such cases. For large $n$, using law of large numbers and CLT we could overcome this issue. But for small $n$? The point is that using simulations we can calculate probabilities, distributions, etc, numerically and approximately. That is often better, since it is more robust to assumptions.

%%%%
%%%%\section{Estimation problems - second example}
%%%%We now consider a case with densities rather than mass functions and go over different methods of estimation.

%%%%
%%%%\begin{question}Let $X_{1},\ldots ,X_{n}$ be i.i.d. random variables with Laplace density $f_{\theta}(x)=\frac{1}{2}e^{-|x-\theta|}$ where the value of $\theta\in \mathbb{R}$ is unknown. How to  {\em estimate} it using the data $X=(X_{1},\ldots ,X_{n})$?
%%%%\end{question}
%%%%In this case, a statistic is a functions $T:\mathbb{R}^{n}\rightarrow \mathbb{R}$. What is a reasonable estimate?

%%%%\begin{enumerate}\setlength\itemsep{6pt}
%%%%\item {\em Maximum likelihood estimate:} The {\em likelihood function} is computed using density in place of mass functions. It is
%%%%\begin{align*}
%%%%\ell(\theta;X) &= \prod_{k=1}^{n}\frac{1}{2}e^{-|X_{k}-\theta|}
%%%%= 2^{-n}\exp\left\{-\sum_{k=1}^{n}|X_{k}-\theta|\right\}.
%%%%\end{align*}
%%%%The MLE $\hat{\theta}$ is by definition the maximizer of  $\ell(\theta;X)$ for the given data $X$. This is equivalent to finding the minimizer of $\sum_{k=1}^{n}|X_{k}-\theta|$ for given $X_{k}$. The answer is $\hat{\theta}=\mbox{median}$\footnote{Recall that for $w_{1}<w_{2}<\ldots <w_{n}$, the median is $w_{(n+1)/2}$ if $n$ is odd and any number in $[w_{n/2},w_{(n+2)/2}]$ if $n$ is even.} of $X_{1},\ldots ,X_{n}$.

%%%%\begin{lemma}\label{lem:medianoptimal} Let $w_{1}<w_{2}<\ldots <w_{n}$ be real numbers. Let $f(t)=\sum_{k=1}^{n}|w_{k}-t|$ is minimized when $t$ is equal to a median of $w_{1},\ldots ,w_{n}$.
%%%%\end{lemma}
%%%%\begin{proof} If $t\in [w_{i},w_{i+1}]$, we can write $f(t)=\sum_{k=1}^{i}(t-w_{i})+\sum_{k=i+1}^{n}(w_{i+1}-t)$. If $t,t+h$ are both in $(w_{i},w_{i+1})$, then $f(t+h)-f(t)=hi-h(n-i)=h(n-2i)$. Thus, when $2i<n$, it is advantageous to move $t$ upwards, and when $n>2i$, it is advantageous to move it downwards. Thus, $f$ is minimized when $t$ is a median. Note that if $n$ is even, $f(t)$ stays constant when $2i=n$, hence all medians are minimizing.
%%%%\end{proof}

%%%%\item {\em Uniformly minimum variance unbiased estimate:} $\mathcal U$ consists of statistics $T$ such that $\mathbf{E}_{\theta}[T(X)]=\theta$ for all $\theta\in \mathbb{R}$. Examples are $T_{1}(X)=X_{1}$, $T_{2}(X)=(X_{1}+X_{2})/2$, $T_{3}(X)=\overline{X}_{n}$, $T_{4}(X)=\mbox{median}_{*}(X)$, where $\mbox{median}_{*}(X)$ is defined as the central median when there is more than one median. For the first three, one must notice that $\mathbf{E}_{\theta}[X_{1}]=\theta$. 

%%%%Let us compute the loss functions for just these statistics. Recall that for the squared loss function, ${\mathbb L}_{T}(\theta)=\mbox{Var}_{\theta}(T(X))$ for any unbiased statistic $T$. Thus,
%%%%\begin{align*}
%%%%{\mathbb L}_{T_{1}}(\theta) &= \mbox{Var}_{\theta}(X_{1})= \frac{1}{2} \int_{-\infty}^{\infty}u^{2}e^{-|u|}du = 2. \\
%%%%{\mathbb L}_{T_{2}}(\theta) &= \frac{1}{4}(\mbox{Var}_{\theta}(X_{1})+\mbox{Var}_{\theta}(X_{2}))=1. \\
%%%%{\mathbb L}_{T_{3}}(\theta) &= \frac{2}{n}.
%%%%\end{align*}
%%%%The variance of the median is more complicated. Let us take $n=2m+1$ and recall that $T_{4}(X)$ has the density $(2m+1)\binom{2m}{m}f(t)F(t)^{m}(1-F(t))^{m}$ where $f$ is the density and $F$ is the CDF. As $\mathbf{E}[T_{4}(X)]=0$ by symmetry, we get
%%%%\begin{align*}
%%%%\mbox{Var}_{\theta}(T_{4}(X)) &= 2(2m+1)\binom{2m}{m}\int_{0}^{\infty}t^{2}\frac{1}{2}e^{-t}\left(\frac{1}{2}e^{-t}\right)^{m}(1-\frac{1}{2}e^{-t})^{m} dt \\
%%%%&= 2^{-m}(2m+1)\binom{2m}{m}\int_{0}^{\infty}t^{2}e^{-(m+1)t}\left(1-\frac{1}{2}e^{-t}\right)^{m} dt. 
%%%%\end{align*}
%%%%Among these, it \marginpar{\tiny{Need to compute and compare the median. Anyway which is the UMVUE here?}}
%%%%\begin{theorem} For i.i.d samples from a coin toss, $\overline{X}_{n}$ is the UMVUE for $p$.
%%%%\end{theorem}
%%%%\begin{remark} Observe that the loss functions here were constant, not depending on $\theta$ at all!
%%%%\end{remark}
%%%%\item {\em Minimax estimate:} The problem with comparing loss functions was that two functions need not be comparable to each other. If we could associate one real number to a statistic (its ``loss''), then this problem would go away. For example, define ${\mathbb L}_{\max}(T)=\max_{p\in [0,1]}{\mathbb L}_{T}(p)$, which is the worst case performance of $T$ under the loss function $L$. If $T_{*}$ is a staistic such that ${\mathbb L}_{\max}(T_{*})\le {\mathbb L}_{\max}(T)$ for any other statistic $T$, then we say that $T_{*}$ is a {\em minimax estimator} of $p$.

%%%%\begin{theorem} For i.i.d samples from a coin toss, $\overline{X}_{n}$ is the minimax estimator of $p$.
%%%%\end{theorem}
%%%% 
%%%%\item {\em Method of moments:} The mean of the distribution with density $\frac{1}{2}e^{-|x-\theta|}$ is $\theta$. Hence the method of moment estimate is $\hat\theta=\overline{X}_{n}$.
%%%%\end{enumerate}

%%%%

%%%%%%%%\newpage
%%%%\section{Estimation problems - first example}
%%%%We start with an example. 

%%%%
%%%%\begin{question}Let $X_{1},\ldots ,X_{n}$ be i.i.d. Ber($p$) random variables where the value of $p$ is unknown. How to  {\em estimate} it using the data $X=(X_{1},\ldots ,X_{n})$?
%%%%\end{question}
%%%%Any function of the data, in this case $T:\{0,1\}^{n}\rightarrow [0,1]$, is called  a {\em statistic} or {\em estimate}. When the problem is of estimating $p$, our goal is to find a statistic $T$ such that $T(X)$ is close to $p$. But close in what sense? And for what values of $p$? Depending on the criterion, we may get different answers. Intuitively in the given case, $\overline{X}_{n}:=\frac{1}{n}(X_{1}+\ldots +X_{n})$ appears to be the best guess for $p$, and indeed it is, by some of the criteria we shall see.
%%%%\begin{enumerate}\setlength\itemsep{6pt}
%%%%\item {\em Maximum likelihood estimate:} We look at the {\em likelihood function} of the data, namely
%%%%\begin{align*}
%%%%\ell(p;X) &= \prod_{k=1}^{n}p^{X_{k}}(1-p)^{1-X_{k}} \\
%%%%&= \exp\{n[\overline{X}_{n}\log p + (1-\overline{X}_{n})\log (1-p)]\}.
%%%%\end{align*}
%%%%The {\em maximum likelihood estimate} (MLE) of $p$ is the statistic $\hat{p}=\hat{p}(X)$ that maximizes $\ell(p;X)$ for the given data $X$. If there is more than one maximizer, we can take any of them.

%%%%In our setting, the maximizer is the solution to $\frac{\overline{X}_{n}}{p}=\frac{1-\overline{X}_{n}}{1-p}$ which is in fact $\hat{p}(X)=\overline{X}_{n}$.

%%%%\item {\em Loss minimizing estimate:} We fix a loss function $L:[0,1]\times [0,1]\rightarrow \mathbb{R}_{+}$, say $L(a,b)=(a-b)^{2}$ for definiteness. This measures the loss if the true answer is $a$ and our guess is $b$. Then, for any statistic $T(X)$, we can compute its {\em loss function} $\mathcal L_{T}(p):=\mathbf{E}_{p}[L(T(X),p)]=\mathbf{E}_{p}[(T(X)-p)^{2}]$. 

%%%%If $T_{1}$ and $T_{2}$ are two estimators such that ${\mathbb L}_{T_{1}}(p)\le {\mathbb L}_{T_{2}}(p)$ for all $p\in [0,1]$. Then it is clear that $T_{1}$ is better than $T_{2}$ and no one in his or her right mind would use $T_{2}$. But what if ${\mathbb L}_{T_{1}}$ and ${\mathbb L}_{T_{2}}$ are as in the figure below? \marginpar{\tiny{Add a figure with three graphs of loss functions here. Two comparable, not the third.}} 

%%%%If there is one statistic $T_{*}(X)$ such that ${\mathbb L}_{T_{*}}(p)\le {\mathbb L}_{T}(p)$ for all $p$ and for all other statistics $T$, then we could unambiguously declare $T_{*}$ to be the champion. Unfortunately, such a $T_{*}$ does not exist in any interesting example. To see this, consider $T_{1}(X)=1/2$ and $T_{2}(X)=1/4$ (constant estimates). Then, ${\mathbb L}_{T_{1}}(1/2)=0$ while ${\mathbb L}_{T_{2}}(1/4)=0$. Thus, $T_{*}$, if it exists, must satisfy ${\mathbb L}_{T_{*}}(1/2)=0$ and ${\mathbb L}_{T_{*}}(1/4)=0$. However, such a statistic clearly does not exist! We could break this impasse in two ways.

%%%%\item {\em Uniformly minimum variance unbiased estimate:} Call an estimate $T$ unbiased if $\mathbf{E}_{p}[T(X)]=p$ for every $p$. Let $\mathcal U$ be the set of all unbiased estimates of $T$. For $T\in \mathcal U$, observe that ${\mathbb L}_{T}(p)=\mbox{Var}_{p}(T)$ (for the squared error loss function).

%%%%We could ask for $T_{*}\in \mathcal U$ such that ${\mathbb L}_{T_{*}}(p)\le {\mathbb L}_{T}(p)$ for all $p$ and for all other statistics $T\in \mathcal U$. Any such $T_{*}$ is called a {\em uniformly minimum variance unbiased estimate}.

%%%%The previous objections do not hold here, for instance, the $T_{1}$ and $T_{2}$ that we had were not unbiased. There are some (special) example, where a unique UMVUE does exist. In particular
%%%%\begin{theorem} For i.i.d samples from a coin toss, $\overline{X}_{n}$ is the UMVU estimate for $p$.
%%%%\end{theorem}

%%%%\item {\em Minimax estimate:} The problem with comparing loss functions was that two functions need not be comparable to each other. If we could associate one real number to a statistic (its ``loss''), then this problem would go away. For example, define ${\mathbb L}_{\max}(T)=\max_{p\in [0,1]}{\mathbb L}_{T}(p)$, which is the worst case performance of $T$ under the loss function $L$. If $T_{*}$ is a staistic such that ${\mathbb L}_{\max}(T_{*})\le {\mathbb L}_{\max}(T)$ for any other statistic $T$, then we say that $T_{*}$ is a {\em minimax estimator} of $p$.

%%%%\begin{theorem} For i.i.d samples from a coin toss, $\overline{X}_{n}$ is the minimax estimator of $p$.
%%%%\end{theorem}
%%%% 
%%%%\item {\em Method of moments:} In this method, we consider the empirical distribution, namely the probability mass function that puts mass $1/n$ at each $X_{k}$. Then we find the value of $p$ for which the first moment of the population mean is equal to the first moment of the empirical distribution. If we had two parameters to estimate, we would equate the first two moments of the population with the first two moments of the sample and solve the equations to get our estimates. This method is naive but practically sometimes useful and easy to find.

%%%%In the case at hand, the expected value of Ber($p$) is $p$, and the sample mean is $\overline{X}_{n}$, hence the MOM estimator is simply $\hat{p}=\overline{X}_{n}$.
%%%%\end{enumerate}

%%%%%%%\newpage
%%%%\section{Estimation - generalities}
%%%%We now abstract out the essence of the last two examples. 

%%%%\vspace{4mm}\marginpar{\tiny{Should we use $P_{\theta}$ or $F_{\theta}$? Have earlier identified measures with CDFs!}}
%%%%\noindent{\bf The setting:} Let $P_{\theta}$ be a family of probability distributions on $\mathbb{R}^{d}$ indexed by $\theta\in {\mathcal I}$. We shall assume that  all $P_{\theta}$ have p.m.f or all have p.d.f,  either of which we shall denote by $f_{\theta}(x)$, $\theta\in {\mathcal I}$, $x\in \mathbb{R}^{d}$. 

%%%%The parameter $\theta$ will be deemed unknown. Note that ${\mathcal I}$ could be a subset in $\mathbb{R}^{2}$ in which case there are really two parameters, etc. What we observe is $X=(X_{1},\ldots ,X_{n})$, where $X_{k}$ are i.i.d. from the distribution $P_{\theta}$. The objective is to estimate $\theta$. Sometimes we may have a function $g:{\mathcal I}\rightarrow \mathbb{R}$ and be interested in estimating $g(\theta)$ (another parameter) only.

%%%%\vspace{4mm}
%%%%\noindent{\bf Statistic or estimate:} A function of the data is called a statistic. When it takes values in the parameter space, we called it an {\em estimate}. In other words, $T:(\mathbb{R}^{d})^{n}\rightarrow {\mathcal I}$ is an estimate for $\theta$ while in estimating $g(\theta)$ estimates are $\mbox{Range}(g)$-valued.

%%%%\vspace{4mm}
%%%%\noindent{\bf Loss function:} A function $L:{\mathcal I}\times {\mathcal I} \rightarrow \mathbb{R}_{+}$ is called a loss function. If ${\mathcal I}$ is a subset of $\mathbb{R}$, popular choices are $L_{\alpha}(\theta,\varphi)=\|\theta-\varphi\|^{\alpha}$, in particular, the squared-loss function with $\alpha=2$. If ${\mathcal I}$ is a discrete set, a popular choice is the zero-one loss function $L(\theta,\varphi)={\mathbf 1}_{\theta=\varphi}$.

%%%%\vspace{4mm}
%%%%\noindent{\bf Risk function of a statistic:} Fix a loss function $L$. For any estimate $T$, its risk function is defined as ${\mathbb L}_{T}(\theta)=\mathbf{E}_{\theta}[L(\theta,T(X))]$.

%%%%\vspace{4mm}
%%%%\noindent{\bf Bias and variance:} We say that a statistic $T(X)$ is {\em unbiased} for $g(\theta)$ if $\mathbf{E}_{\theta}[T(X)]=g(\theta)$ for all $\theta\in {\mathcal I}$. In general, the {\em bias} of $T$ is the function $b_{T}(\theta)=\mathbf{E}_{\theta}[T(X)]-g(\theta)$. The variance of $T$ is of course the function $V_{T}(\theta)=\mbox{Var}_{\theta}(T(X))$. The {\em mean squared error} of $T$ is $\mbox{M.S.E}_{T}(\theta):=\mathbf{E}[(T(X)-g(\theta))^{2}]$. This is just the risk function associated to the squared error loss. It is a simple matter to check that $\mbox{M.S.E}_{T}(\theta)=b_{T}(\theta)^{2}+Var_{\theta}(T)$. Note that it makes sense to talk of bias and variance only if the parameter is real-valued.
%%%%\begin{enumerate}\setlength\itemsep{6pt}
%%%%\item {\em Maximum likelihood estimate:} The likelihood function is defined as $\ell(\theta;X):=\prod_{k=1}^{n}f_{\theta}(X_{k})$. The MLE of $\theta$ is the statistic $\hat{\theta}(X):=\arg\max\limits_{\theta\in {\mathcal I}}\ell(\theta;X)$. This is well-defined if the maximizer is unique. If not, we must make up some rule to choose among the maximizers (so that $\hat{\theta}$ is a function) and each such rule is an MLE. 
%%%%\item {\em Minimax estimate:} $T_{*}$ is called a minimax estimate of $\theta$ if $\max_{\theta\in {\mathcal I}}{\mathbb L}_{T_{*}}(\theta)\le \max_{\theta\in {\mathcal I}}{\mathbb L}_{T}(\theta)$. Minimax estimate need not be unique, but the maximal risk of all minimax estimates must be equal.
%%%%\item {\em Uniformly minimum variance unbiased estimate:} Let $\mathcal U$ be the set of all unbiased estimators of $g(\theta)$. We say that a statistic $T_{*}$ is the UMVUE of $g(\theta)$ if $\mbox{Var}_{T_{*}}(\theta)\le \mbox{Var}_{T}(\theta)$ for all $\theta\in {\mathcal I}$. In general, it may not exist or if it exists, need not be unique.
%%%%\item {\em Method of moments estimates:} Define the parameters $g_{p}(\theta):=\mathbf{E}_{\theta}[X_{1}^{p}]$. In typical situations, there is some $p_{0}$ such that $\theta\rightarrow (g_{1}(\theta),\ldots ,g_{p_{0}}(\theta))$ is a one-one function (for example, if ${\mathcal I}$ is an open set in $\mathbb{R}^{m}$, then we expect $p_{0}=m$). In such cases, we consider the equations
%%%%$$
%%%%\frac{1}{n}\sum_{k=1}^{n}X_{k}^{p} = g_{p}(\theta), \;\; 1\le p\le p_{0}.
%%%%$$
%%%%If there is some $\hat{\theta}$ that solves these equations, then it is unique. That is called the method of moments estimate.
%%%%\end{enumerate}

%%%%\begin{exercise} Write down the parameter space, a few reasonable estimates for the parameters and discuss the estimation problem in light of the above criteria.
%%%%\begin{enumerate}\setlength\itemsep{6pt}
%%%%\item $P_{\mu}$ is the $N(\mu,1)$ distribution.
%%%%\item $P_{{\sigma}^{2}}$ is the $N(0,{\sigma}^{2})$ distribution.
%%%%\item $P_{\mu,{\sigma}^{2}}$ is the $N(\mu,{\sigma}^{2})$ distribution.
%%%%\item $P_{\lambda}$ is the Exponential($\lambda$) distribution. Let $\theta=1/\lambda$. Discuss the estimation of $\theta$. (There are some interesting aspects of estimation of $\lambda$, for example, there is no unbiased estimate of $\lambda$!).
%%%%\item Let $P_{a,b}$ be Uniform($[a,b]$) distribution. 
%%%%\item Let $P_{p}$ be Geometric($p$) distribution. Again, consider the parameter $1/p$ instead of $p$.
%%%%\item Let $P_{\lambda}$ be the Poisson($\lambda$) distribution. (In this case, it is a fact that $1/\lambda$ does not have any unbiased estimates!).
%%%%\end{enumerate}
%%%%\end{exercise}

%%%%\begin{remark} Let $\varphi=h(\theta)$ where $h$ is a one-one function. Then intuitively we would consider the estimation of $\theta$ or the estimation of $\varphi$ as equivalent problems. In other words, if $\hat{\theta}$ is our estimate of $\theta$, then $\hat{\varphi}=h(\hat{\theta})$ ought to be our estimate for $\varphi$ and {\em vice versa}. This is indeed true of the MLE, but not for the other estimates above. For example, note that if $T(X)$ is unbiased for $\theta$, it is in general {\em not} the case that $h(T(X))$ is unbiased for $\varphi$. Thus, UMVUE etc. depend on the way in which we parameterize the family of probability distributions and parameterization should not be thought of as an arbitrary labeling of the family of distributions at hand - some parameterizations are better than others.
%%%%\end{remark}

\section{Hypothesis testing - first examples}
Earlier in the course we discussed the problem of how to test whether a ``psychic'' can make predictions better than a random guesser. This is a prototype of what are called {\em testing problems}. We start with this simple example and introduce various general terms and notions in the context of this problem.


%\begin{question} A $p$-coin is tossed $n$ times with outcomes $X_{1},\ldots ,X_{n}$. Suppose we are told that the value of $p$ is either $1/2$ or $3/4$. How to decide which, using the data $X=(X_{1},\ldots ,X_{n})$?

% For example, one can imagine a new drug released by a pharmaceutical company to cure a disease $\mathcal D$. The company claims that it is effective in 75\% of the cases while there is a contrary opinion that it is effective in only 50\% of the cases. Somewhat more realistic situations are discussed later.
%\end{question}

\begin{question} A ``psychic'' claims to guess the order of cards in a deck. We shuffle a deck of cards, ask her to guess and count the number of correct guesses, say $X$. 

\medskip
One hypotheses (we call it the {\em null hypothesis} and denote it by $H_{0}$) is that the psychic is guessing randomly. The {\em alternate hypothesis} (denoted $H_{1}$) is that his/her guesses are better than random guessing (in itself this does not imply existence of psychic powers. It could be that he/she has managed to see some of the cards etc.). Can we decide between the two hypotheses based on $X$?
\end{question}

What we need is a rule for deciding which hypothesis is true. A rule for deciding between the hypotheses is called a {\em test}. For example, the following are examples of rules (the only condition is that the rule must depend only on the data at hand).

\begin{example} We present three possible rules.
\begin{enumerate}\setlength\itemsep{6pt}
\item If $X$ is an even number declare that $H_{1}$ is true. Else declare that $H_{1}$ is false.
\item If $X\ge 5$, then accept $H_{1}$, else reject $H_{1}$.
\item If $X\ge 8$, then accept $H_{1}$, else reject $H_{1}$.
\end{enumerate}
The first rule does not make much sense as the parity (evenness or oddness) has little to do with either hypothesis. On the other hand, the other two rules make some sense. They rely on the fact that if $H_{1}$ is true then we expect $X$ to be larger than if $H_{0}$ is true. But the question still remains, should we draw the line at $5$ or at $8$ or somewhere else?
\end{example}
In testing problems  there is only one objective, to avoid the following two possible types of mistakes.
\begin{align*}
\mbox{Type-I error:} & \; H_{0} \mbox{ is true but our rule concludes }H_{1}. \\
\mbox{Type-II error:} & \; H_{1} \mbox{ is true but our rule concludes }H_{0}.
\end{align*}
The probability of type-I error is called the {\em significance level} of the test and usually denote by $\alpha$. That is, $\alpha=\mathbf{P}_{H_{0}}\{\mbox{the test accepts }H_{1}\}$ where we write $\mathbf{P}_{H_{0}}$ to mean that the probability is calculated under the assumption that $H_{0}$ is true. Similarly one define the {\em power} of the test as $\beta=\mathbf{P}_{H_{1}}\{\mbox{the test accepts }H_{1}\}$. Note that $\beta$ is the probability of not making type-II error, and hence we would like it to be close to $1$. Given two tests with the same level of significance, the one with higher power is better. Ideally we would like both to be small, but that is not always achievable.

We fix the desired level of significance, usually $\alpha=0.05$ or $0.1$ and only consider tests whose probability of type-I error is at most $\alpha$. It may seem surprising that we take $\alpha$ to be so small. Indeed the  two hypotheses are not treated equally. Usually $H_{0}$ is the default option, representing traditional belief and $H_{1}$ is a claim that must prove itself. As such, the burden of proof is on $H_{1}$. 

To use analogy with law, when a person is convicted, there are two hypotheses, one that he is guilty and the other that he is not guilty. According to the maxim ``innocent till proved guilty'', one is not required to prove his/her innocence. On the other hand guilt must be proved. Thus the null hypothesis is ``not guilty'' and the alternative hypothesis is ``guilty''. 


In our example of card-guessing, assuming random guessing, we have calculated the distribution of $X$ long ago. Let $p_{k}=\mathbf{P}\{X=k\}$ for $k=0,1,\ldots ,52$.  Now consider a test of the form ``Accept $H_{1}$ if $X\ge k_{0}$ and reject otherwise''. Its level of significance is
$$
\mathbf{P}_{H_{0}}\{\mbox{accept }H_{1}\} = \mathbf{P}_{H_{0}}\{X\ge k_{0}\} = \sum_{i=k_{0}}^{52}p_{i}.
$$
For $k_{0}=0$, the right side is $1$ while for $k_{0}=52$ it is $1/52!$ which is tiny. As we increase $k_{0}$ there is a first time where it becomes less than or equal to $\alpha$. We take that $k_{0}$ to be the threshold for cut-off.

 In the same example of card-guessing, let $\alpha=0.01$. Let us also assume that Poisson approximation holds. This means that $p_{j}\approx e^{-1}/j! $ for each $j$. Then, we are looking for the smallest $k_{0}$ such that $\sum_{j=k_{0}}^{\infty}e^{-1}/j! \le 0.01$. For $k_{0}=4$, this sum is about $0.019$ while for $k_{0}=5$ this sum is $0.004$. Hence, we take $k_{0}=5$. In other words, accept $H_{1}$ if $X\ge 5$ and reject if $X<5$. If we took $\alpha=0.0001$ we would get $k_{0}=7$ and so on.

\para{Strength of evidence} Rather than merely say that we accepted $H_{1}$ or rejected it would be better to say how strong the evidence is in favour of the alternative hypothesis. This is captured by the {\em $p$-value}, a central concept of decision making. It is defined as {\em the probability that data drawn from the null hypothesis would show closer agreement with the alternative hypothesis than the data we have at hand} (read it five times!).

Before we compute it in our example, let us return to the analogy with law. Suppose  a man is convicted for murder. Recall that $H_{0}$ is that he is not guilty and $H_{1}$ is that he is guilty. Suppose his fingerprints were found in the house of the murdered person. Does it prove his guilt? It is some evidence in favour of it, but not necessarily strong. For example, if the convict was a friend of the murdered person, then he might be innocent but have left his fingerprints on his visits to his friend. However if the convict is a total stranger, then one wonders why, if he was innocent, his finger prints were found there. The evidence is stronger for guilt. If bloodstains are found on his shirt, the evidence would be even stronger! In saying this, we are  asking ourselves questions like ``if he was innocent, how likely is it that his shirt is blood-stained?''. That is $p$-value. Smaller the $p$-value, stronger the evidence for the alternate hypothesis.

Now we return to our example. Suppose the observed value is $X_{\mbox{\tiny obs}}=4$. Then the $p$-value is $\mathbf{P}\{X\ge 4\}=p_{4}+\ldots +p_{52}\approx 0.019$. If the observed value was $X_{\mbox{\tiny obs}}=6$, then the $p$-value would be $p_{6}+\ldots +p_{52}\approx 0.00059$. Note that the computation of $p$-value does not depend on the level of significance. It just depends on the given hypotheses and the chosen test. 


\section{Testing for the mean of a normal population}
 Let $X_{1},\ldots ,X_{n}$ be i.i.d. $N(\mu,{\sigma}^{2})$. We shall consider the following  hypothesis testing problems.

\begin{enumerate}\setlength\itemsep{6pt}
\item One sided test for the mean. $H_{0}:\; \mu=\mu_{0}$ versus $H_{1}: \; \mu>\mu_{0}$.
\item Two sided test for the mean.  $H_{0}:\; \mu=\mu_{0}$ versus $H_{1}: \; \mu\not=\mu_{0}$.
\end{enumerate}
%The sense is that in the first case we do not have to worry about the possibility that $\mu<\mu_{0}$
This kind of problem arises in many situations in comparing the effect of a treatment as follows. 
\begin{example} Consider a drug claimed to reduce blood pressure. How do we check if it actually does? We take a random sample of $n$ patients, measure their blood pressures $Y_{1},\ldots ,Y_{n}$. We administer the drug to each of them and again measure the blood pressures $Y_{1}',\ldots ,Y_{n}'$, respectively.   Then, the question is whether the mean blood pressure decreases upon giving the treatment. To this effect, we define $X_{i}=Y_{i}-Y_{i}'$ and wish to test the hypothesis that the mean of $X_{i}$s is strictly positive. If $X_{i}$ are indeed normally distributed, this is exactly the one-sided test above. 
\end{example}
\begin{example} The same applies to test the efficacy of a fertilizer to increase yield, a proposed drug to decrease weight, a particular educational method to improve a skill, or a particular course such as the current {\em probability and statistics course} in increasing subject knowledge. To make a policy decision on such  matters, we can conduct an experiment as in the above example. 

For example, a bunch of students are tested on probability and statistics and their scores are noted. Then they are subjected to the course for a semester. They are tested again after the course (for the same marks, and at the same level of difficulty) and the scores are again noted. Take differences of the scores before and after, and test whether the mean of these differences is positive (or negative, depending on how you take the difference). This is a  one-sided tests for the mean. Note that in these examples, we are taking the null hypothesis to be that there is no effect. In other words, the burden of proof is on the new drug or fertilizer or the instructor of the course.
\end{example}

\para{The test} Now we present the test. We shall use the statistic $\mathcal T:=\frac{\sqrt{n}(\overline{X}-\mu_{0})}{s}$ where $\overline{X}$ and $s$ are the sample mean and sample standard deviation. 
\begin{enumerate}\setlength\itemsep{6pt}
\item In the one-sided test, we accept the alternative hypothesis if $\mathcal T>t_{n-1}(\alpha)$.
\item In the two sided-test, accept the alternative hypothesis if $\mathcal T>t_{n-1}(\alpha/2)$ or $\mathcal T<-t_{n-1}(\alpha/2)$.
\end{enumerate}

\para{The rationale behind the tests} If $\overline{X}$ is much larger than $\mu_{0}$ then the greater is the evidence that the true mean $\mu$ is greater than $\mu_{0}$. However, the magnitude depends on the standard deviation and hence we divide by $s$ (if we knew ${\sigma}$ we would divide by that). Another way to see that this is reasonable is that $\mathcal T$ does not depend on the units in which you measure $X_{i}$s (whether $X_{i}$ are measured in meters or centimeters, the value of $\mathcal T$ does not change). 

\para{The significance level is $\alpha$} The question is where to draw the threshold. We have seen before that {\em under the null hypothesis}  $\mathcal T$ has a $t_{n-1}$ distribution. Recall that this is because, if the null hypothesis is true, then  $\frac{\sqrt{n}(\overline{X}-\mu_{0})}{{\sigma}}\sim N(0,1)$, $(n-1)s^{2}/{\sigma}^{2} \sim \chi^{2}_{n-1}$ and the two are independent.  Thus, the given tests have significance level $\alpha$ for the two problems.

\begin{remark} Earlier we considered the problem of constructing a $(1-\alpha)$-CI for $\mu$ when ${\sigma}^{2}$ is unknown. The two sided test abovecan be simply stated as follows: Accept the alternative at level $\alpha$ if the corresponding $(1-\alpha)$-CI does not contain $\mu_{0}$. Conversely, if we had dealt with testing problems first, we could define a confidence interval as the set of all those $\mu_{0}$ for which the corresponding test rejects the alternative.

Thus, confidence intervals and testing are closely related. This is true in some greater generality. For example, we did not construct confidence interval for $\mu$, but you should do so and check that it is closely related to the one-sided tests above.
\end{remark}

\section{Testing for the difference between means of two normal populations}
Let $X_{1},\ldots ,X_{n}$ be i.i.d. $N(\mu_{1},{\sigma}_{1}^{2})$ and let $Y_{1},\ldots ,Y_{m}$ be i.i.d. $N(\mu_{2},{\sigma}_{2}^{2})$. We shall consider the following  hypothesis testing problems.

\begin{enumerate}\setlength\itemsep{6pt}
\item One sided test for the difference in means. $H_{0}:\; \mu_{1}=\mu_{2}$ versus $H_{1}: \; \mu_{1}>\mu_{2}$.
\item Two sided test for the mean.  $H_{0}:\; \mu_{1}=\mu_{2}$ versus $H_{1}: \; \mu_{1}\not=\mu_{2}$.
\end{enumerate}


This kind of problem arises in many situations in comparing two different populations or the effect of two different treatments etc. Actual data sets of such questions can be found in the homework.
\begin{example} Suppose a new drug to reduce blood pressure is introduced by a pharmaceutical company.  There is already an existing drug in the market which is working reasonably alright. But it is claimed by the company that the new drug is better. How to test this claim?

We take a random sample of $n+m$ patients and break them into two groups of $n$ and of $m$ patients. The first group is administered the new drug while the second group is administered the old drug. Let $X_{1},\ldots ,X_{n}$ be the {\em decrease in blood pressures} in the first group. Let $Y_{1},\ldots ,Y_{m}$ be the {\em decrease} in blood pressures in the second group. The claim is that one average $X_{i}$s are larger than $Y_{i}$s.

Note that it does not make sense to subtract $X_{i}-Y_{i}$ and reduce to a one sample test as in the previous section (here $X_{i}$ is a measurement on one person and $Y_{i}$ is a measurement on a completely different person! Even the number of persons in the two groups may differ). This is an example of a two-sample test as formulated above. 
\end{example}
\begin{example} The same applies to many studies of comparision. If someone claims that Americans are taller than Indians on average, or if it is claimed that cycling a lot leads to increase in height, or if it is claimed that Chinese have higher IQ than Europeans, or if it is claimed that {\em Honda Activa} gives better mileage than {\em Suzuki Access}, etc., etc., the claims can be reduced to the two-sample testing problem as introduced above.
\end{example}

\parag{BIG ASSUMPTION:} We shall assume that ${\sigma}_{1}^{2}={\sigma}_{2}^{2}={\sigma}^{2}$ (yet unknown). This assumption is not made because it is natural or because it is often observed, but because it leads to mathematical simplification. Without this assumption, no exact level-$\alpha$ test has been found!

\para{The test} Let $\overline{X},\overline{Y}$ denote the sample means of $X$ and $Y$ and let $s_{X}, s_{Y}$ denote the corresponding sample standard deviations. Since ${\sigma}^{2}$ is the assumed to be the same for both populations, $s_{X}^{2}$ and $s_{Y}^{2}$ can be combined to define 
$$
S^{2}:=\frac{(n-1)s_{X}^{2}+(m-1)s_{Y}^{2}}{m+n-2}
$$
which is a better estimate for ${\sigma}^{2}$ than just $s_{X}^{2}$ or $s_{Y}^{2}$ (this $S^{2}$ is better than simply taking $(s_{X}^{2}+s_{Y}^{2})/2$ because it gives greater weight to the larger sample). 

Now define $\mathcal T =\sqrt{\frac{1}{n}+\frac{1}{m}}\left(\frac{\overline{X}-\overline{Y}}{S}\right)$.  The following tests hav significance level $\alpha$.
\begin{enumerate}\setlength\itemsep{6pt}
\item For the one-sided test, accept the alternative if $\mathcal T>t_{n+m-2}(\alpha)$.
\item For the one-sided test, accept the alternative if $\mathcal T>t_{n+m-2}(\alpha/2)$ or $\mathcal T<-t_{n+m-2}(\alpha/2)$.
\end{enumerate}

\para{The rationale behind the tests} If $\overline{X}$ is much larger than $\overline{Y}$ then the greater is the evidence that the true mean $\mu_{1}$ is greater than $\mu_{2}$. But again we need to standardize by dividing this by an estimate of ${\sigma}$, namely $S$. The resulting statistic $\mathcal T$ has a $t_{m+n-2}$ distribution as explained below.

\para{The significance level is $\alpha$} The question is where to draw the threshold. From the facts we know,
\begin{align*}
\overline{X}&\sim N(\mu_{1},{\sigma}_{1}^{2}/n), \\
\overline{Y}&\sim N(\mu_{2},{\sigma}_{2}^{2}/m), \\
\frac{(n-1)}{{\sigma}^{2}}s_{X}^{2}&\sim \chi_{n-1}^{2}, \\
\frac{(m-1)}{{\sigma}^{2}}s_{Y}^{2}&\sim \chi_{m-1}^{2}
\end{align*}
and the four random variables are independent. From this, it follows that $(m+n-2)S^{2}$ has $\chi_{n+m-2}^{2}$ distribution. {\em Under the null hypothesis} $\frac{1}{{\sigma}}\sqrt{\frac{1}{n}+\frac{1}{m}}(\overline{X}-\overline{Y})$ has $N(0,1)$ distribution and is independent of $S$. Taking ratios, we see that $\mathcal T$ has $t_{m+n-2}$ distribution (under the null hypothesis).


\section{Testing for the mean in absence of normality} Suppose $X_{1},\ldots ,X_{n}$ are i.i.d. $\mbox{Ber}(p)$. Consider the test
$$
H_{0}: \; p=p_{0} \;\;\; \mbox{ versus }\;\;\; H_{1}: \; p\not=p_{0}.
$$
One can also consider the one-sided test. Just as in the confidence interval problem, we can give a solution when $n$ is large, using the approximation provided by the central limit theorem. Recall that an approximate $(1-\alpha)$-CI is 
 $$
\left[\overline{X}_{n}-z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}, \overline{X}_{n}+z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}\right].
$$
Inverting this confidence interval, we see that a reasonable test is:

Reject the alternative if $p_{0}$ belongs to the above CI. That is, accept the alternative if
$$
\overline{X}_{n}-z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}\le p_{0}\le \overline{X}_{n}+z_{\alpha/2}\sqrt{\frac{\overline{X}_{n}(1-\overline{X}_{n})}{n}}
$$
This test has (approximately) significance level $\alpha$.

\medskip
More generally, if we have data $X_{1},\ldots ,X_{n}$ from a population with mean $\mu$ and variance ${\sigma}^{2}$, then consider the test
$$
H_{0}: \; \mu=\mu_{0} \;\;\; \mbox{ versus }\;\;\; H_{1}: \; \mu\not=\mu_{0}.
$$
A test with approximate significance level $\alpha$ is given by: Reject the alternative if
$$
\overline{X}_{n}-z_{\alpha/2}\frac{s_{n}}{\sqrt{n}}\le \mu_{0}\le \overline{X}_{n}+z_{\alpha/2}\frac{s_{n}}{\sqrt{n}}.
$$
Just as with confidence intervals, we can find the actual level of significance (if $n$ is not large enough) by simulating data on a computer.

%\section{Testing for the difference in means of two normal populations}
%The mathematical set-up is as follows. Let $X_{1},\ldots ,X_{n}$ be i.i.d. samples from $N(\mu_{1},{\sigma}_{1}^{2})$ distribution and let $Y_{1},\ldots ,Y_{m}$ be i.i.d. samples from $N(\mu_{2},{\sigma}_{2}^{2})$. We assume that $X_{i}$s are independent of $Y_{j}$s. Our objective is to test
%$$
%H_{0}: \; \mu_{1}=\mu_{2}\;\;\;  \mbox{ versus} \;\;\; H_{1}: \; \mu_{1}>\mu_{2}.
%$$

%This is a very common question that arises in comparing two populations. For example, imagine a new fertilizer that claims to give better yields than an existing one. To test this, we conduct an experiment where we divide a large tract of land into $m+n$ equal areas. To the first $n$ tracts the new fertilizer is applied and to the first $m$ plots the old fertilizer is applied. The yields at the end of the season the first set of tracts are $X_{1},\ldots ,X_{n}$ and in the second set of tracts are $Y_{1},\ldots ,Y_{m}$. Based on this data we must decide whether the second fertilizer is indeed better as claimed. Note that we have taken the null hypothesis to be ``$\mu_{1}=\mu_{2}$'', indicating that the burden of proof is on the new fertilizer. 

%Many similar problems can be considered.
%\begin{itemize}\setlength\itemsep{3pt}
%\item A pharmaceutical company releases a new drug. Test if it is better than the old one.
%\item A new method of teaching is claimed to be better than an existing one.
%\item It is claimed that one race of people have higher IQ than another.
%\end{itemize}
%Since comparing things is an ever present obsession of humans, you can add any number of other examples on your own! It is clear that the above testing problem captures all these situations except for one serious issue, the assumption of normality. We will only say that if $m$ and $n$ are both large, then like in the one sample test for the mean, we can use law of large numbers and central limit theorems to obtain approximate level $\alpha$ tests.

%Now we return to the mathematical setting. We have $X_{1},\ldots ,X_{n}$,  i.i.d. samples from $N(\mu_{1},{\sigma}_{1}^{2})$ distribution and $Y_{1},\ldots ,Y_{m}$,   i.i.d. samples from $N(\mu_{2},{\sigma}_{2}^{2})$ distribution. We assume that $X_{i}$s are independent of $Y_{j}$s. Our objective is to test
%$$
%H_{0}: \; \mu_{1}=\mu_{2}\;\;\;  \mbox{ versus} \;\;\; H_{1}: \; \mu_{1}>\mu_{2}.
%$$
%From the facts we know about sample mean and sample variance, we know that 
%\begin{align*}
%\overline{X}\sim N(\mu_{1},{\sigma}_{1}^{2}/n), & \hspace{5mm}   \frac{(n-1)}{{\sigma}_{1}^{2}}s_{X}^{2}\sim \chi^{2}_{n-1}, \\
%\overline{Y}\sim N(\mu_{2},{\sigma}_{2}^{2}/m), & \hspace{5mm} \frac{(m-1)}{{\sigma}_{2}^{2}}s_{Y}^{2}\sim \chi^{2}_{m-1}.
%\end{align*}
%Further, all these four random variables are mutually independent. Consequently, we see that
%\begin{align*}
%\overline{X}-\overline{Y}&\sim N\left(\mu_{1}-\mu_{2},\frac{{\sigma}_{1}^{2}}{n}+\frac{{\sigma}_{2}^{2}}{m}\right) \\
% \frac{(n-1)}{{\sigma}_{1}^{2}}s_{X}^{2}+\frac{(m-1)}{{\sigma}_{2}^{2}}s_{Y}^{2}&\sim \chi^{2}_{m+n-2}.
%\end{align*}

%As with testing for the mean when the variance was unknown, we want to make a random variable that depends on the data and the difference in means (since we are testing whether $\mu_{1}-\mu_{2}$ is zero or positive) but not on the unknown variance. Unfortunately it is known how to do this (you can try manipulating the above random variables!). But we can do it, under a simplifying assumption.

%\para{Assumption} The population variances are unknown but equal. That is, ${\sigma}_{1}^{2}={\sigma}_{2}^{2}={\sigma}^{2}$ (not known). In that case, 
%\begin{align*}
%\frac{(\overline{X}-\overline{Y})-(\mu_{1}-\mu_{2})}{{\sigma}\sqrt{\frac{1}{n}+\frac{1}{m}}}&\sim N\left(0,1\right) \\
% \frac{1}{{\sigma}^{2}}\{(n-1)s_{X}^{2}+(m-1)s_{Y}^{2}\}&\sim \chi^{2}_{m+n-2}.
%\end{align*}
%Further, the two random variables are independent. Therefore,
%$$
%\frac{(\overline{X}-\overline{Y})-(\mu_{1}-\mu_{2})}{\sqrt{\frac{1}{n}+\frac{1}{m}}\sqrt{\frac{(n-1)s_{X}^{2}+(m-1)s_{Y}^{2}}{n+m-2}}}\sim T_{n+m-2}.
%$$
%If the null hypothesis was true, the  left hand side become the statistic 
%$$
%\mathcal T_{m,n}:=\frac{\overline{X}-\overline{Y}}{\sqrt{\frac{1}{n}+\frac{1}{m}}\sqrt{\frac{(n-1)s_{X}^{2}+(m-1)s_{Y}^{2}}{n+m-2}}}
%$$
% Hence, we can use this to test for $\mu_{1}-\mu_{2}$ (or equivalently, to construct a confidence interval for it). 

%\para{Testing rule} If $\mathcal T_{m,n}>t_{m+n-2}(\alpha)$, then accept the alternate hypothesis. Else, reject it.

%
%\para{Without the normality assumption} For large $m,n$ we can still say that $\mathcal T_{m,n}$ has approximately standard normal distribution (under the null hypothesis). Hence, we can use it to test for $\mu_{1}-\mu_{2}$. 


\section{Chi-squared test for goodness of fit}
At various times we have made statements such as ``heights follow normal distribution'', ``lifetimes of bulbs follow exponential distribution'' etc. Where do such claims come from? Over years of analysing data, of course. This leads to an interesting question. Can we test whether  lifetimes of bulbs do follow exponential distribution? 

We start with a simple example of testing whether a die is fair.  The hypotheses are  $H_{0}:$ the die is fair, versus $H_{1}:$ the die is unfair\footnote{You may feel that the null and alternative hypotheses are reversed. Is not independence a special property that should prove itself. Yes and no. Here we are imagining a situation where we have some reason to think that the die is fair. For example perhaps the die looks symmetric.}.

We throw the die $n$ times and record the observations $X_{1},\ldots ,X_{n}$. For $j\le 6$, let $O_{j}$ be the number of times we observe the face $j$ turn up. In symbols $O_{j}=\sum_{i=1}^{n}{\mathbf 1}_{X_{i}=j}$. Let $E_{j}=\mathbf{E}[O_{j}]=\frac{n}{6}$ be the expected number of times we see the face $j$ (under the null hypothesis). Common sense says that if $H_{0}$ is true then $O_{j}$ and $E_{j}$ must be rather close for each $j$. How to measure the closeness? Karl Pearson introduced the test statistic
$$
 T:=\sum_{j=1}^{6}\frac{(O_{j}-E_{j})^{2}}{E_{j}}.
$$
If the desired level of significance is $\alpha$, then the Pearson $\chi^{2}$-test says ``Reject $H_{0}$ if $T\ge \chi^{2}_{5}(\alpha)$''. The number of degrees of freedom is $5$ here. In general, it is one less than the number of bins (i.e., how many terms you are summing to get $T$). 

\para{Some practical points} The $\chi^{2}$ test is really an asymptotic statement. For large $n$, the level of significance is approximately $1-\alpha$. There is no assurance for small $n$. Further, in performing the test, it is recommended that each bin must have at least $5$ observations (i.e., $O_{j}\ge 5$). Otherwise we club together bins with fewer entries. The number $5$ is a rule of thumb, the more the better.

\para{Fitting the Poisson distribution} We consider the famous data collected by Rutherford, Chadwick and Ellis on the number of radioactive disintegrations. For details see the book of Feller's book (section VI.7) or \href{http://galton.uchicago.edu/~lalley/Courses/312/PoissonProcesses.pdf}{this website}. 

The data consists of $X_{1},\ldots ,X_{2608}$ (where $X_{k}$ is the number of particles detected by the counter in the $k^{\mbox{\tiny th}}$ time interval. The hypotheses are
 $$
 H_{0}: \; F \mbox{ is a Poisson distribution}. \qquad H_{1}: \; F \mbox{ is not Poisson}.
 $$ 
The physical theories predict that the distribution ought to be Poisson and hence we have taken it as the null hypothesis\footnote{When a new theory is proposed, it should prove itself and is put in the alterntive hypotheis, but here we take it as null.}

We define $O_{j}$ as the number of time intervals in which we see exactly $j$ particles. Thus $O_{j}=\sum_{i=1}^{2608}{\mathbf 1}_{X_{i}=j}$. How do we find the expected numbers? If the null hypothesis had said that $F$ has Poisson(1) distribution, we could use that to find the expected numbers. But $H_{0}$ only says Poisson($\lambda$) for an unspecified $\lambda$? This brings in a new feature.

First estimate $\lambda$, for example $\hat{\lambda}=\overline{X}_{n}$ is an MLE as well as method of moments estimate. Then we use this to calculate Poisson probabilities and the expected numbers. In other words, $E_{j}=e^{-\hat{\lambda}}\frac{\hat{\lambda}^{j}}{j!}$. For the given data we find that $\hat{\lambda}=3.87$. The table is as follows.

\begin{center}
\begin{tabular}{||r|c|c|c|c|c|c|c|c|c|c|c||}
\hline
$j$  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & $\ge$ 10 \\
\hline
$O_{j}$  & 57 & 203 & 383 & 525 & 532 & 408 & 273 & 139 & 45 & 27 & 16 \\
\hline
$E_{j}$ & 54.4 & 210.5 & 407.4 & 525.4 & 508.4 & 393.5 & 253.8 & 140.3 & 67.9 & 29.2 & 17.1 \\
\hline
\end{tabular}
\end{center}
Two remarks: The original data would have consisted of several more bins for $j=11,12\ldots$. These have been clubbed together to perform the $\chi^{2}$ test (instead of a minimum of $5$ per bin, they may have ensured that there are at least $10$ per bin). Also, the estimate $\hat{\lambda}=3.87$ was obtained before clubbing these bins. Indeed, if the data is merely presented as the above table, there will be some ambiguity in how to find $\hat{\lambda}$ as one of the bins says ``$\ge 10$''. 

Then we compute
$$
 T=\sum_{j=0}^{10}\frac{(O_{j}-E_{j})^{2}}{E_{j}} = 14.7.
$$
Where should we look up in the $\chi^{2}$ table? Earlier we said that the degrees of freedom is one less than the number of bins. Here we give the more general rule.
$$
\mbox{Degrees of freedom of the }\chi^{2} = \mbox{ No. of bins }-1-\mbox{No. of parameters estimated from data}.
$$
In our case we estimated one parameter, $\lambda$ hence the d.f. of the $\chi^{2}$ is $11-1-1=9$. Looking at $\chi_{9}^{2}$ table one can see that the $p$-value is $0.10$. This is the probability that a $\chi_{9}^{2}$ random variable is greater than $14.7$. (Caution: Elsewhere I see that the $p$-value for this experiment is reported as $0.17$, please check my calculations!). This means that at $5\%$ level, we would not reject the null hypothesis. If the $p$-value was $0.17$, we would not reject the null hypothesis even at $10\%$ level.  

\para{Fitting a continuous distribution} Chi-squared test can be used to test goodness of fit for continuous distributions too. We need some modifications. We must make bins of appropriate size, like $[a,a+h],[a+h,a+2h],\ldots ,[a+h(k-1),a+hk]$ for a suitable $h$ and $k$. Then we find the expected numbers in each bin using the null hypothesis (first estimating some parameters if necessary) and then proceed to compute $T$ in the same way as before. Then check against the $\chi^{2}$ table with the appropriate degrees of freedom. We omit details.


\para{The probability theorem behind the $\chi^{2}$-test for goodness of fit} Let $(W_{1},\ldots ,W_{k})$ have multinomial distribution with parameters $n,m,(p_{1},\ldots ,p_{k})$. (In other words, place $n$ balls at random into $m$ bins, but each ball goes into the $i^{\mbox{th}}$ bin with probability $p_{i}$ and distinct balls are assigned independently of each other). The following proposition is the mathematics behind Pearson's test.

\para{Proposition [Pearson]} Fix $k,p_{1},\ldots,p_{k}$. Let $T_{n}=\sum_{i=1}^{k}\frac{(W_{i}-np_{i})^{2}}{np_{i}}$. Then  $T_{n}$ converges to a $\chi_{k-1}^{2}$ distribution in the sense that $\mathbf{P}\{T_{n}\le x\}\rightarrow \int_{0}^{x}f_{k-1}(u)du$ where $f_{k-1}$ is the density of $\chi_{k-1}^{2}$ distribution.

\medskip
How does this help? Suppose $X_{1},\ldots ,X_{n}$ are i.i.d. random variables taking $k$ values (does not matter what the values are, say $t_{1},t_{2},\ldots ,t_{k}$) with probabilities $p_{1},\ldots ,p_{k}$. Then, let $W_{i}$ be the number of $X_{i}$s whose value is $t_{i}$. Clearly, $(W_{1},\ldots ,W_{k})$ has a multinomial distribution.  Therefore, for large $n$, the random variable $T_{n}$ defined above (which is in fact the $\chi^{2}$-statistic of Pearson) has approximately $\chi_{k-1}^{2}$ distribution. This explains the test.

\para{Sketch of proof of the proposition} Start with the case $k=2$. Then, $W_{1}\sim \mbox{Bin}(n,p_{1})$ and $W_{2}=r-W_{1}$. Thus, $T_{n}=\frac{(W_{1}-np_{1})^{2}}{np_{1}p_{2}}$ (recall that $p_{1}+p_{2}=1$ and check this!). We know that $(W_{1}-np_{1})/\sqrt{np_{1}q_{1}}$ is approximately a $N(0,1)$ random variable, where $q_{i}=1-p_{i}$). Its square has (approximately$\chi_{1}^{2}$ distribution. Thus the proposition is proved for $k=2$.

When $k>2$, what happens is that the random variables $\xi_{i}:=(W_{i}-np_{i})/\sqrt{np_{i}q_{i}}$ are approximately $N(0,1)$, but not independent. In fact the correlation between $\xi_{i}$ and $\xi_{j}$ is  close to $-\sqrt{p_{i}p_{j}/q_{i}q_{j}}$. The sum of squares of $\xi_{i}$s  gives the $\chi^{2}$ statistic. On the other hand, one can (with some clever linear algebra/matrix manipulation) write $\sum_{i=1}^{k}\xi_{i}^{2}$ as $\sum_{i=1}^{k-1}\eta_{i}^{2}$ where $\eta_{i}$ are   {\em independent} $N(0,1)$ random variables. Thus we get $\chi_{k-1}^{2}$ distribution.

\section{Tests for independence}
Suppose we have a bivariate sample $(X_{1},Y_{1}),(X_{2},Y_{2}),\ldots ,(X_{n},Y_{n})$ i.i.d. from a joint density (or joint pmf) $f(x,y)$. The question is to decide whether $X_{i}$ is independent of $Y_{i}$.

\begin{example} There are many situations in which such a problem arises. For example, suppose a bunch of students are given two exams, one testing mathematical skills and another testing verbal skills. The underlying goal may be to investigate whether the human brain has distinct centers for verbal and quantitative thinking. 
\end{example}
\begin{example} As another example, say we want to investigate whether smoking causes lung cancer. In this case, for each person  in the sample, we take two measurements - $X$ (equals $1$ if smoker and $0$ if not) and $Y$ (equal $1$ if the person has lung cancer, $0$ if not). The resulting data may be summarized in a two-way table as follows.
$$
\begin{array}{c|cc|c}
 & X=0 & X=1 & \\
 \hline 
 Y=0 & n_{0,0} & n_{0,1} & n_{0\cdot}\\
Y=1 & n_{1,0} & n_{1,1} & n_{1\cdot} \\
\hline
 & n_{\cdot 0} & n_{\cdot 1} & n
\end{array}
$$
Here the total sample is of $n$ persons and $n_{i,j}$ denote the numbers in each of the four boxes. The numbers $n_{0\cdot}$ etc denote row or column sums. The statistical problem is to check if smoking ($X$) and incidence of lung cancer ($Y$) are positively correlated.
\end{example}

\para{Testing independence in bivariate normal} We shall not discuss this problem in detail but instead quickly give some indicators and move on. Here we have $(X_{i},Y_{i})$ i.i.d bivariate normal random variables with $\mathbf{E}[X]=\mu_{1}$, $\mathbf{E}[Y]=\mu_{2}$, $\mbox{Var}(X)={\sigma}_{1}^{2}$, $\mbox{Var}(Y)={\sigma}_{2}^{2}$ and $\mbox{Corr}(X,Y)=\rho$. The testing problem is $H_{0}: \; \rho=0$ versus $H_{1}: \; \rho\not=0$. (Remember that if $(X,Y)$ is bivariate normal, then $X$ and $Y$ are independent if and only if $X$ and $Y$ are uncorrelated.

The natural statistic to consider is the sample correlation coefficient ({\em Pearson's $r$ statistic})
$$
r_{n}:=\frac{s_{X,Y}}{s_{X}.s_{Y}}
$$
where $s_{X}^{2},s_{Y}^{2}$ are the sample variances of $X$ and $Y$ and $s_{X,Y}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})(Y_{i}-\overline{Y})$ is the sample covariance. It is clear that the test should reject null hypothesis if $r_{n}$ is away from $0$. To decide the threshold we need the distribution of $r_{n}$ under the null hypothesis. 

\para{Fisher} Under the null hypothesis, $r_{n}^{2}$ has $\mbox{Beta}(\frac{1}{2}, \frac{n-2}{2})$ distribution.

\medskip

Using this result, we can draw the threshold for rejection using the Beta distribution (of course the explicit threshold can only be computed numerically). If the assumption of normality of the data is not satisfied, then this test is invalid. However, for large $n$ as usual we can obtain an asymptotically level-$\alpha$ test.

\para{Testing for independence in contingency tables}
Here the measurements $X$ and $Y$ take values in $\{x_{1},\ldots ,x_{k}\}$ and $\{y_{1},\ldots ,y_{\ell}\}$, respectively. These $x_{i},y_{j}$ are categories, not numerical values (such as ``smoking'' and ``non-smoking''). Let the total number of samples be $n$ and let $N_{i,j}$ be the number of samples with values $(x_{i},y_{j})$. Let $N_{i\cdot}=\sum_{j}N_{i,j}$ and let $N_{\cdot j}=\sum_{i}N_{i,j}$.

We want to test 
\begin{align*}
H_{0}&: \; X \mbox{ and } Y \mbox{ are independent} \\
H_{1}&: \; X \mbox{ and } Y \mbox{ are not independent}.
\end{align*}

Let $\mu(i,j)=\mathbf{P}\{X=x_{i},Y=y_{j}\}$ be the joint pmf of $(X,Y)$ and let $p(i)$, $q(j)$ be the marginal pmfs of $X$ and $Y$ respectively. From the sample, our estimates for these probabilities would be $\hat{\mu}(i,j)=N_{i,j}/n$ and $\hat{p}(i)=N_{i\cdot}/n$ and $\hat{q}(j)=N_{\cdot j}/n$ (which are consistent in the sense that $\sum_{j}\hat{\mu}(i,j)=\hat{p}(i)$ etc).

Under the null hypothesis we must have $\mu(i,j)=p(i)q(j)$. We test if these equalities hold (approximately) for the estimates. That is, define
$$
T=\sum_{i=1}^{k}\sum_{j=1}^{\ell}\frac{(N_{i,j}-n\hat{p}(i)\hat{q}(j))^{2}}{n\hat{p}(i)\hat{q}(j)}.
$$
Note that this is in the usual form of a $\chi^{2}$ statistic (sum of $(\mbox{observed}-\mbox{expected})^{2}/\mbox{expected}$). 

The number of terms is $k\ell$. We lose one d.f. as usual but in addition we estimate $(k-1)$ parameters $p(i)$ (the last one $p(k)$ can be got from the others) and $(\ell-1)$ parameters $q(j)$. Consequently, the total degress of freedom is $k\ell-1-(k-1)-(\ell-1)=(k-1)(\ell-1)$. 

Hence, we reject the null hypothesis if $T>\chi_{(k-1)(\ell-1)}^{2}(\alpha)$ to get (an approximately) level $\alpha$ test.



\section{Regression and Linear regression}
Let $(X_{i},Y_{i})$ be i.i.d random variables. For example, we could pick people at random from a population and measure their height ($X$) and weight ($Y$). One question of interest is to predict the value of $Y$ from the value of $X$. This may be useful if $Y$ is difficult to measure directly. For instance, $X$ could be the height of a person and $Y$ could be the xxx

In other words, we assume that there is an underlying relationship $Y=f(X)$ for an unknown function $f$ which we want to find. From a random sample $(X_{1},Y_{1}),\ldots ,(X_{n},Y_{n})$  we try to guess the function $f$.

If we allow all possible functions, it is easy to find one that fits all the data points, i.e., there exists a function $f:\mathbb{R}\rightarrow \mathbb{R}$ (in fact we may take $f$ to be a polynomials of degree $n$) such that $f(X_{i})=Y_{i}$ for each $i\le n$ (this is true only if we assume that all $X_{i}$ are distinct which happens if $X$ has a continuous distribution). This is not a good predictor, because the next data point $(U,V)$ will fall way off the curve. We have found a function that ``predicts'' well all the data we have, but not for a future observation! 

Instead, we fix a class of functions, for example the collection of all linear functions $y=mx+c$ where $m,c\in \mathbb{R}$ and within this class, find the best fitting function. 

\begin{remark} One may wonder if linearity is too restrictive. To some extent, but perhaps not as much as it sounds at first.
\begin{enumerate}\setlength\itemsep{6pt}
\item Firstly, many relationships are linear in a reasonable range of the $X$ variable (for example, resistance of a materiaal versus temperature). 
\item Secondly, we may sometimes transform the variables so that the relationship becomes linear. For example, if $Y=ae^{bX}$, then $\log(Y)=a'+b'X$ where $a'=\log(a)$ and $b'=\log(b)$ and hence in terms of the new variables $X$ and $\log(Y)$, we have a linear relationship. 
\item Lastly, as a slight extension of linear regression,  one can study {\em multiple linear regression}, where one   has several independent variables $X^{(1)},\ldots, X^{(p)}$ and try to fit a linear  function $Y=\beta_{1}X^{(1)}+\ldots +\beta_{p}X^{(p)}$. Once that is done, it increases the scope of curve fitting even more. For example, if we have two variable $X,Y$, then we can take $X^{(1)}=1$, $X^{(2)}=X$, $X^{(3)}=X^{2}$. Then, linear regression of $Y$ against $X^{(1)},X^{(2)},X^{(3)}$ is tantamount to fitting a quadratic polynomial curve for $X,Y$. 
\end{enumerate}
In short, multiple linear regression along with non-linear transformations of the individual variables, the class of functions $f$ is greatly extended.
 \end{remark}

\para{Finding the best linear fit} We need a criterion for deciding the ``best''. A basic one is the {\em method of least squares} which recommends finding $\alpha,\beta$ such that the error sum of squares $R^{2}:=\sum_{k=1}^{n}(Y_{k}-\alpha-\beta X_{k})^{2}$ is minimized.

For fixed $X_{i},Y_{i}$ this is a simple problem in calculus. We get
$$
\hat{\beta}=\frac{\sum_{k=1}^{n}(X_{k}-\overline{X}_{n})(Y_{k}-\overline{Y}_{n})}{\sum_{k=1}^{n}(X_{k}-\overline{X}_{n})^{2}}=\frac{s_{X,Y}}{s_{X}^{2}},  \qquad \hat{\alpha}=\overline{Y}_{n}-\hat{\beta}\overline{X}_{n}
$$
where $s_{X,Y}$ is the sample covariance of $X,Y$ and $s_{X}$ is the sample variance of $X$.

We leave the derivation of the least squares estimators by calculus to you. Instead we present another approach. 

For a given choice of $\beta$, we know that the choice of $\alpha$ which minimizes $R^{2}$ is the sample mean of $Y_{i}-\beta X_{i}$ which is $\overline{Y}-\beta \overline{X}$. Thus, we only need to find $\hat{\beta}$ that minimizes 
$$\sum_{k=1}^{n}\left((Y_{k}-\overline{Y})-\beta(X_{k}-\overline{X})\right)^{2}$$
and then we simply set $\hat{\alpha}=\overline{Y}-\beta\overline{X}$. Let\footnote{We are dividing by $X_{k}-\overline{X}$. What if it is zero for some $k$? But note that in the expression $\sum \left((Y_{k}-\overline{Y})-\beta(X_{k}-\overline{X})\right)^{2}$, all such terms do not involve $\beta$ and hence can be safely left out of the summation. We leave the details for you to work out (the expressions at the end should involve all $X_{k},Y_{k}$). } $Z_{k}=\frac{Y_{k}-\overline{Y}}{X_{k}-\overline{X}}$ and $w_{k}=(X_{k}-\overline{X})^{2}/s_{X}^{2}$. Then,
$$
\sum_{k=1}^{n}\left((Y_{k}-\overline{Y})-\beta(X_{k}-\overline{X})\right)^{2}=s_{X}^{2}\sum_{k=1}^{n}w_{k}\left(Z_{k}-\beta \right)^{2}.
$$
Since $w_{k}$ are non-negative numbers that add to $1$, we can intepret it as a probability mass function and hence we see that the minimizing $\beta$ is given by the expectation with respect to this mass function. In other words, 
$$
\hat{\beta}=\sum_{k=1}^{n}w_{k}Z_{k} = \frac{s_{X,Y}}{s_{X}^{2}}.
$$
Another way to write it is $\hat{\beta}=\frac{s_{Y}}{s_{X}}r_{X,Y}$ where $r_{X,Y}$ is the sample correlation coefficient.

\para{A motivation for the least squares criterion} Suppose we make more detailed model assumptions as follows. Let $X$ be a control variable (i.e., not random but we can tune it to any value, like temperature) and assume that $Y_{i}=\alpha+\beta X_{i}+\epsilon_{i}$ where $\epsilon_{i}$ are i.i.d. $N(0,{\sigma}^{2})$ ``errors''. Then, the data is essential $Y_{i}$ that are independent $N(\alpha+\beta X_{i},{\sigma}^{2})$ random variables. Now we can extimate $\alpha,\beta$ by the maximum likelihood method.

\begin{example} [Hubble's 1929 experiment on the recession velocity of nebulae and their distance to earth] Hubble collected the following data that I took from \href{http://lib.stat.cmu.edu/DASL/Datafiles/Hubble.html}{this website}. Here $X$ is the number of megaparsecs from the nebula to earth and $Y$ is the observed recession velocity in $10^{3}$km/s.
\begin{center}
\begin{tabular}{||r|c|c|c|c|c|c|c|c|c|c|c|c|c||}
\hline
$X$  & 0.032 & 0.034 & 0.214 & 0.263 & 0.275 & 0.275 & 0.45 & 0.5 & 0.5 & 0.63 & 0.8 &  2 \\
\hline
$Y$  & 0.17 & 0.29 & -0.13 & -0.07 & -0.185 & -0.22 & 0.2 & 0.29 & 0.27 & 0.2 & 0.3 & 1.09\\
\hline
\hline
$X$ & 0.9 & 0.9 & 0.9 & 0.9 & 1 & 1.1 & 1.1 & 1.4 & 1.7 & 2 & 2 & 2 \\
\hline
$Y$ & -0.03 & 0.65 & 0.15 & 0.5 & 0.92 & 0.45 & 0.5 & 0.5 & 0.96 & 0.5 & 0.85 & 0.8   \\
\hline
\hline
\end{tabular}
\end{center}
We fit two straight lines to this data.
\begin{enumerate}\setlength\itemsep{6pt}
\item Fit the line $Y=\alpha+\beta X$. The least squares estimators (as derived earlier) turn out to be $\hat{\alpha}=-0.04078$ and $\hat{\beta}=0.45416$.  If $Z_{i}=\alpha+\beta X_{i}$ are the predicted values of $Y_{i}$s, then one can see that the {\em residual sum of squares} is $\sum_{i}(Y_{i}-Z_{i})^{2}=1.1934$.
\item Fit the line $Y=bX$. In this case we get $\hat{b}$ by minimizing $\sum_{i}(Y_{i}-bX_{i})^{2}$. This is slightly different from before, but the same methods (calculus or the alternate argument we gave) work to give 
$$
\hat{b}=\frac{\sum_{i=1}^{n}Y_{i}X_{i}}{\sum_{i=1}^{n}X_{i}^{2}}= 0.42394.
$$
The residual sum of squares $\sum_{i=1}^{n}(Y_{i}-bX_{i})^{2}$ turns out to be $1.2064$.
\end{enumerate}
The residual sum of squares is smaller in the first, thus one may naively think that it is a better fit. However, note that the reduction is due to an extra parameter. Purely statistically, introducing extra parametrs will always reduce the residual sum of squares for obvious reasons. But the question is whether the extra parameter is worth the reduction. More precisely, if we fit the data too closely, then the next data point to be discovered (which may be nebula that is $10$ megaparsecs away)  may fall way off the curve.

More importantly, in this example, physics tells us  that the line must pass through zero (that is, there is no recession velocity when two objects are very close). Therefore it is the second line that we consider, not the first. This gives the Hubble constant to be $423$ km./s./megaparsec (the currently accepted values appear to be about $70$, with data going up to distances of hundreds of megaparsecs...see  \href{https://www.cfa.harvard.edu/~dfabricant/huchra/hubble.plot.dat}{this data}!).
\end{example}

\begin{example} I have taken this example from the \href{http://ces.iisc.ernet.in/hpg/nvjoshi/statspunedatabook/databook.html}{ wonderful compilation of data sets} by A. P. Gore, S. A. Paranjpe, M. B. Kulkarni. In this example, $Y$ denotes the number of frogs of age $X$ (in some delimited population). 
\begin{center}
\begin{tabular}{||r|c|c|c|c|c|c|c|c|c|c|c|c|c||}
\hline
$X$  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8  \\
\hline
$Y$  & 9093 & 35 & 30 & 28 & 12 & 8 & 5 & 2\\
\hline
\hline
\end{tabular}
\end{center}
A prediction about life-times says that the survival probability $P(t)$ (which is the chance that an individual survives up to age $t$ or more) decays as $P(t)=Ae^{-bt}$ for some constants $A$ and $b$. We would like to check this agains the given data.

What we need are individuals that survive beyond age $t$. Taking $Z$ to be the cumulative sums of $Y$, this gives us
\begin{center}
\begin{tabular}{||r|c|c|c|c|c|c|c|c|c|c|c|c|c||}
\hline
$X$  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8  \\
\hline
$Z$  & 9213 &  120 & 85 & 55 & 27 & 15 & 7 & 2\\
\hline
$P=Z/n$ & 1.0000  &  0.0130 &   0.0092  &  0.0060  &  0.0029  &  0.0016  &  0.0008  &  0.0002 \\
\hline
$W=\log P$ &  0  & -4.3409  & -4.6857 &  -5.1210  & -5.8325  & -6.4203 &  -7.1825 &  -8.4352 \\
\hline
\hline
\end{tabular}
\end{center}
We compute that $\overline{X}=4.5$, $\overline{W}=-5.25$, $\mbox{std}(X)=2.45$, $\mbox{std}(W)= 2.52$ and $\mbox{corr}(X,W)=0.92$. Hence, in the linear regression $W=a+bX$, we see that $\hat{b}=0.94$ and $\hat{a}=-9.49$. The residual sum of squares is $7.0$. 
\end{example}

\parag{How good is the fit?} For the same data $(X_{1},Y_{1}),\ldots ,(X_{n},Y_{n})$, suppose we have two candidates \; (a) \; $Y=f(X)$ and \; (b) \; $Y=g(X)$. How to decide which is better? Or how to say if a fit is good at all? 

By the least-squares criterion, the answer is  the one with smaller residual sum of squares $SS:=\sum_{k=1}^{n}(Y_{k}-f(X_{k}))^{2}$. Usually one presents a closely related quantity $R^{2}=1-\frac{SS}{SS_{0}}$ (where $\mbox{SS}_{0}=\sum_{k=1}^{n}(Y_{k}-\overline{Y})^{2}=(n-1)s_{Y}^{2}$). Since $SS_{0}$ is (a multiple of) the total variance in $Y$, $R^{2}$ measures how much of it is ``explained'' by a particular fit. Note that $0\le R^{2}\le 1$. And higher (i.e., closer to $1$) the $R^{2}$ is, the better the fit.

Thus, the first naive answer to the above question is to compute $R^{2}$ in the two situations (fitting by $f$ and fitting by $g$) and see which is higher. But a more nuanced approach is preferable. Consider the same data and three situations.
\begin{enumerate}\setlength\itemsep{6pt}
\item Fit a constant function. This means, choose  $\alpha$  to minimize $\sum_{k=1}^{n}(Y_{k}-\alpha)^{2}$. The solution is $\hat{\alpha}=\overline{Y}$ and the residual sum of squares is $\mbox{SS}_{0}$ itself. Then, $R_{0}^{2}=0$. 
\item Fit a linear function. Then $\alpha,\beta$ are chosen as discussed earlier and the residual sum of squares is $\mbox{SS}_{1}=\sum_{k=1}^{n}(Y_{k}-\hat{\alpha}-\hat{\beta}X_{k})^{2}$. Then, $R_{1}^{2}=1-\frac{SS_{1}}{SS_{0}}$. 
\item Fit a quadratic function. The the residual sum of squares is $\mbox{SS}_{2}=\sum_{k=1}^{n}(Y_{k}-\hat{\alpha}-\hat{\beta}X_{k}-\hat{\gamma}X_{k}^{2})^{2}$ where  $\hat{\alpha},\hat{\beta},\hat{\gamma}$ are chosen so as to minimize $\sum_{k=1}^{n}(Y_{k}-\alpha-\beta X_{k}-\gamma X_{k}^{2})^{2}$. Then $R_{2}^{2}=1-\frac{SS_{2}}{SS_{0}}$.
\end{enumerate}
Obviously we will have $R_{2}^{2}\ge R_{1}^{2}\ge R_{0}^{2}$ (since linear functions include constants and quadratic functions include linear ones). Does that mean that the third is better? If that were the conclusion, then we can continue to introduce more parameters as that will always reduce the residual sum of squares! But that comes at the cost of making the model more complicated (and having too many parameters means that it will fit the current data well, but not future data!). When to stop adding more parameters?

Qualitatively,  a new parameter is desirable  if it leads to a  {\em significant increase} of the $R^{2}$. The question is, how big an increase is significant. For this, one introduces the notion of {\em adjusted} $R^{2}$, which is defined as follows:

If the model has $p$ parameters, then define $\overline{SS}=SS/(n-1-p)$. In particular, $\overline{SS}_{0}=\frac{SS_{0}}{n-1}=s_{Y}^{2}$.  Then define the adjusted $R^{2}$ as $\overline{R}^{2}=1-\frac{\overline{SS}}{\overline{SS}_{0}}$. 

In particular, $\overline{R}_{0}^{2}=R_{0}^{2}$ as before. But $R_{1}^{2}=1-\frac{SS_{1}/(n-2)}{SS_{0}/(n-1)}$. Note that $\overline{R}^{2}$ does not necessarily increase upon adding an extra parameter. If we want a polynomial fit, then a rule of thumb is to keep adding more powers as long as $\overline{R}^{2}$ continues to increase and stop the moment it decreases.
\begin{example} To illustrate the point let us look at a simulated data set. I generated $25$ i.i.d $N(0,1)$ variables $X_{i}$ and then generated $25$ i.i.d. $N(0,1/4)$ variables $\epsilon_{i}$. And set $Y_{i}=2X_{i}+\epsilon_{i}$. The  data set obtained was as follows.
\begin{center}
\begin{tabular}{||r|c|c|c|c|c|c|c|c|c|c|c|c|c||}
\hline
$X$  & -0.87&0.07&-1.22&-1.12&-0.01&1.53&-0.77&0.37&-0.23&1.11&-1.09&0.03&0.55 \\
\hline
$Y$  & -2.43&-0.56&-2.19&-2.32&-0.12&3.77&-1.4&0.84&0.34&1.83&-1.83&0.48&0.98 \\
\hline
\hline
$X$ & 1.1&1.54&0.08&-1.5&-0.75&-1.07&2.35&-0.62&0.74&-0.2&0.88&-0.77 &\\
\hline
$Y$ & 2.3&2.5&-0.41&-2.94&-1.13&-0.84&4.36&-1.14&1.45&-1.36&1.55&-2.43  &  \\
\hline
\hline
\end{tabular}
\end{center}
To this data set we fit two models (A) $Y=\beta X$ and (B) $Y=a+bX$. The results are as follows.
\begin{align*}
\mbox{SS}_{0}=96.20, &\;\;  R_{0}^{2}=0 \\
\mbox{SS}_{1}=6.8651, &\;\;  R_{1}^{2}=0.9286, \hspace{2mm} \overline{R}_{1}^{2}=0.9255 \\
\mbox{SS}_{2}= 6.8212, &\;\; R_{2}^{2}=0.9291, \hspace{2mm} \overline{R}_{2}^{2}=0.9227.
\end{align*}
Note that the adjusted $R^{2}$ decreases (slightly) for the the second model. Thus, if we go by that, then the model with one parameter is chosen (correctly, as we generated from that model!). You can try various simulations yourself. Also note the high value of $R_{1}^{2}$ (and $R_{2}^{2}$) which indicates that it is not a bad fit at all.
\end{example}

%\begin{center}
%\begin{tabular}{||r|c|c|c|c|c|c|c|c|c|c|c|c|c||}
%\hline
%Observed $Y$ &  0  & -4.3409  & -4.6857 &  -5.1210  & -5.8325  & -6.4203 &  -7.1825 &  -8.4352 \\
%\hline
%Predicted $Y$
%\hline
%\hline
%\end{tabular}
%\end{center}

%\begin{thebibliography}{99}

%\bibitem{freedmanpisanipurves}
%\bibitem{freedmanpisanipurves} {\sc Freedma},
% Statistics.

%\bibitem{hkpv} {\sc Hough, J. B., Krishnapur, M., Peres, Y. and Vir\'{a}g, B. 
%{Zeros of Gaussian analytic functions and determinantal point processes}, American Mathematical Society, (2009)}.
%\end{thebibliography}


\end{document}






\appendix
%    Include appendix "chapters" here.
%\include{}
\chapter{Plan of lectures}
\begin{tabular}{r|l}
02/08 & Intro. Probability space defns. \\
03/08 & Examples of discrete prob spaces\\
06/08 & Infinite sums \\
09/08 & Basic rules of probability; Inclusion-exclusion \\
10/08 & Bonferroni's inequalities. Combinatorial examples \\
13/08 & -- \\
16/08 & --These three days, have them go over lots of combinatorial problems \\
17/08 & -- \\
20/08 & Probability distributions. Binomial, Poisson, Geometric, Hypergeometric \\
23/08 & Continuous CDFs and densities \\
24/08 & Normal, Exponential and Gamma, Uniform and Beta, Cauchy \\
27/08 & Padding \\
30/08 & Expectation, variance, covariance \\
31/08 & Inequalities - Cauchy-Schwarz, Jensen's, Markov, Chebyshev\\
03/09 & Joint distributions. Change of variable formula. \\
06/09 & Independence. Conditional probability.  \\
07/09 & Examples. \\
10/09 & \\
13/09 & \\
14/09 & \\
17/09 & \\
20/09 & \\
21/09 & \\
\end{tabular}
 
 
 \bigskip
 
\begin{tabular}{c|c|l}
1 & 02& Intro. Prob spaces. Examples.\\
2 & 05& Infinite sums. Basic rules. Inclusion-exclusion\\
3 & 12& [Lost week]\\
4 & 19& Distributions with examples. CDF. Uncountable prob spaces. Examples of pdf.\\
5 & 26& Examples further. Simulation. Joint distributions. Independence.\\
6 & 02& Conditioning. Bayes' rule. \\
7 & 09& Expectation, variance, covariance. Inequalities. \\
8 & 16& WLLN. \\
9 & 23& Normal and Poisson convergence of Binomial.\\
10 & 30& Distribution of the sum. Whatever else.\\
\hline
11 & 07& Basic problems in statistics. Summarizing data. \\
12 & 14& Estimation problems. \\   
13 & 21& Hypothesis testing problems. \\
14 & 28& Linear regression and least squares method. \\
15 & 04& Kolmogorov-Smirnov and Chi-squared tests. \\   
16 & 11& Testing for independence. Contingency tables. \\
17 & 18& \\
18 & 25& \\   
?? & ??& Random walks. P\'{o}lya's urn scheme. Branching processes. \\
\end{tabular}

\para{Must include} Coupon collector problem. Banach's matchbox problem. Boltzmann-Gibbs, Fermi-Dirac and Bose-Einstein statistics. Sampling error in polls. P\'{o}lya's urn scheme. Ballot problem. 


%\backmatter
%    Bibliographies can be prepared with BibTeX using amsplain,
%    amsalpha, or (for "historical" overviews) natbib style.
\bibliographystyle{amsplain}
%\bibliography{Essentials/GAF_book}

%    See note above about multiple indexes.
\printindex

\section{Lecture by lecture plan}
\begin{tabular}{r||c|c||c}
DATE   & TARGET & ACTUAL & COMMENTS \\
\hline
02/Aug & Introductory lecture & & \\
\hline
05/Aug & Probability space definition & & \\
07/Aug & Examples of probability spaces & & \\
09/Aug & ----& & \\
\hline
12/Aug & Balls in bins, Nonexamples& & \\
14/Aug & Countability, Infinite sums & Only countability & \\
16/Aug & Rules of probability&  Infinite sums &\\
\hline
19/Aug & Inclusion exclusion& Countable probability spaces & \\
21/Aug & Bonferroni's inequalities& Rules of probability & \\
23/Aug & $\approx\approx\approx\approx$(Independence?)& Inclusion-exclusion & \\
\hline
26/Aug & Random variables, mean, pmf, cdf& & \\
28/Aug & Binomial, Geometric, Poisson & & \\
30/Aug & Simulation & & \\
\hline
02/Sep & Conceptual difficulties of continuous distributions & & \\
04/Sep & Continuous distributions& & \\
06/Sep & Normal, exponential, Uniform& & \\
\hline
09/Sep & Simulation & & \\
11/Sep & Joint distributions, Independence& & \\
13/Sep & Conditioning & & \\
\hline
16/Sep & Conditioning& & \\
18/Sep & Change of variable& & \\
20/Sep & Change of variable& & \\
\hline
23/Sep & Mean, variance, covariance& & \\
25/Sep & Cauchy-Shwarz, Markov, Chebyshev& & \\
27/Sep & Weak law of large numbers& & \\
\hline
30/Sep & Monte Carlo integration& & \\
02/Oct & Central limit theorem& & \\
04/Oct & --End of probability--& & \\
\hline
\hline
07/Sep & Statistics - introduction & & \\
09/Oct & Estimation & & \\
11/Oct & Estimation & & \\
\hline
14/Sep & Confidence intervals & & \\
16/Oct & Confidence intervals & & \\
18/Oct & Wrap up estimation & & \\
\hline
21/Oct & Testing & & \\
23/Oct & Testing & & \\
25/Oct & Testing & & \\
\hline
28/Oct & Testing & & \\
30/Oct & Testing & & \\
01/Nov & Regression & & \\
\hline
04/Nov & Regression & & \\
06/Nov & $\approx\approx\approx\approx$ & & \\
08/Nov & $\approx\approx\approx\approx$ & & \\
\hline
\end{tabular}
\begin{remark} Probably losing one week for midterm. But there must be two more weeks at the end. Assuming loss of a couple of classes to holidays, there may be just enough time. But schedule must be adhered to.
\end{remark}


\section{Various pieces}
There are many pieces that should be inserted in exercises if they cannot be covered in lectures or tutorials.
\begin{itemize}\setlength\itemsep{3pt}
\item Stirling's formula
\item Poisson limit of Binomial
\item Banach's matchbox problem
\item Coupon collector problem
\item Polya's urn scheme (definition)
\item Random walk in one and two dimensions
\item Gambler's ruin problem
\item Ballot problem
\item Catalan numbers
\item Gamma function
\item Beta function
\item Branching process
\item Integration of $e^{-x^{2}}$
\item Multidimensional normal integral (at least bivariate)
\item Hardy-Weinberg law
\item Fisher's explanation of sex-ratios
\item Mendel's actual data, falsification?
\item Comparing literary styles, with example
\item Sample surveys - actual examples?
\item 
\end{itemize}


\end{document}




%\appendix
%    Include appendix "chapters" here.
%\include{}

%\backmatter
%    Bibliographies can be prepared with BibTeX using amsplain,
%    amsalpha, or (for "historical" overviews) natbib style.
\bibliographystyle{amsplain}
%\bibliography{Essentials/GAF_book}

%    See note above about multiple indexes.
%\printindex

\end{document}



