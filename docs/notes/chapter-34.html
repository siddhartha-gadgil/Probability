
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">
<link rel="stylesheet" href="../css/extras.css">



  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    
<div class="container-fluid">
<div class="banner">
<h1 class="text-center bg-primary">Chapter 34 : Hypothesis testing - first examples</h1>
</div>
</div>

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <span class="navbar-brand navbar-right">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav">
          <li><a href="index.html">Table of Contents</a></li>
            
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 1 to 14 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
<li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
  </ul>
</li>
    

<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 15 to 28 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
<li><a href="chapter-28.html">28. Introduction</a> </li>
  </ul>
</li>
    

<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 29 to 40 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
  </ul>
</li>
    


        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">







 <p class="text-justify">

Earlier in the course we discussed the problem of how to test whether a ''psychic'' can make predictions better than a random guesser. This is a prototype of what are called <em> testing problems</em>. We start with this simple example and introduce various general terms and notions in the context of this problem.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-173"><div class="panel panel-danger question"> <div class="panel-heading">Question 173 </div><div class="panel-body"> A ''psychic'' claims to guess the order of cards in a deck. We shuffle a deck of cards, ask her to guess and count the number of correct guesses, say $X$.
 </p>
           
 <p class="text-justify">
<p></p>
One hypotheses (we call it the <em> null hypothesis</em> and denote it by $H_{0}$) is that the psychic is guessing randomly. The <em> alternate hypothesis</em> (denoted $H_{1}$) is that his/her guesses are better than random guessing (in itself this does not imply existence of psychic powers. It could be that he/she has managed to see some of the cards etc.). Can we decide between the two hypotheses based on $X$?
</div></div></div>
 </p>
<p class="text-justify">
          
 What we need is a rule for deciding which hypothesis is true. A rule for deciding between the hypotheses is called a <em> test</em>. For example, the following are examples of rules (the only condition is that the rule must depend only on the data at hand).
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-174"><div class="panel panel-info example"> <div class="panel-heading">Example 174 </div><div class="panel-body"> We present three possible rules.
<ol>
<li> If $X$ is an even number declare that $H_{1}$ is true. Else declare that $H_{1}$ is false.
</li>
<li> If $X\ge 5$, then accept $H_{1}$, else reject $H_{1}$.
</li>
<li> If $X\ge 8$, then accept $H_{1}$, else reject $H_{1}$.
</ol>
The first rule does not make much sense as the parity (evenness or oddness) has little to do with either hypothesis. On the other hand, the other two rules make some sense. They rely on the fact that if $H_{1}$ is true then we expect $X$ to be larger than if $H_{0}$ is true. But the question still remains, should we draw the line at $5$ or at $8$ or somewhere else?
</div></div></div>
In testing problems  there is only one objective, to avoid the following two possible types of mistakes.
$$\begin{align*}
\mbox{Type-I error:} &   H_{0} \mbox{ is true but our rule concludes }H_{1}. \\
\mbox{Type-II error:} &   H_{1} \mbox{ is true but our rule concludes }H_{0}.
\end{align*}$$
The probability of type-I error is called the <em> significance level</em> of the test and usually denote by $\alpha$. That is, $\alpha=\mathbf{P}_{H_{0} }\{\mbox{the test accepts }H_{1}\}$ where we write $\mathbf{P}_{H_{0} }$ to mean that the probability is calculated under the assumption that $H_{0}$ is true. Similarly one define the <em> power</em> of the test as $\beta=\mathbf{P}_{H_{1} }\{\mbox{the test accepts }H_{1}\}$. Note that $\beta$ is the probability of not making type-II error, and hence we would like it to be close to $1$. Given two tests with the same level of significance, the one with higher power is better. Ideally we would like both to be small, but that is not always achievable.
 </p>
<p class="text-justify">
          
 We fix the desired level of significance, usually $\alpha=0.05$ or $0.1$ and only consider tests whose probability of type-I error is at most $\alpha$. It may seem surprising that we take $\alpha$ to be so small. Indeed the  two hypotheses are not treated equally. Usually $H_{0}$ is the default option, representing traditional belief and $H_{1}$ is a claim that must prove itself. As such, the burden of proof is on $H_{1}$.
 </p>
           
 <p class="text-justify">
To use analogy with law, when a person is convicted, there are two hypotheses, one that he is guilty and the other that he is not guilty. According to the maxim ''innocent till proved guilty'', one is not required to prove his/her innocence. On the other hand guilt must be proved. Thus the null hypothesis is ''not guilty'' and the alternative hypothesis is ''guilty''.
 </p>
<p class="text-justify">
          
 In our example of card-guessing, assuming random guessing, we have calculated the distribution of $X$ long ago. Let $p_{k}=\mathbf{P}\{X=k\}$ for $k=0,1,\ldots ,52$.  Now consider a test of the form ''Accept $H_{1}$ if $X\ge k_{0}$ and reject otherwise''. Its level of significance is
$$
\mathbf{P}_{H_{0} }\{\mbox{accept }H_{1}\} = \mathbf{P}_{H_{0} }\{X\ge k_{0}\} = \sum_{i=k_{0} }^{52}p_{i}.
$$
For $k_{0}=0$, the right side is $1$ while for $k_{0}=52$ it is $1/52!$ which is tiny. As we increase $k_{0}$ there is a first time where it becomes less than or equal to $\alpha$. We take that $k_{0}$ to be the threshold for cut-off.
 </p>
           
 <p class="text-justify">
In the same example of card-guessing, let $\alpha=0.01$. Let us also assume that Poisson approximation holds. This means that $p_{j}\approx e^{-1}/j! $ for each $j$. Then, we are looking for the smallest $k_{0}$ such that $\sum_{j=k_{0} }^{\infty}e^{-1}/j! \le 0.01$. For $k_{0}=4$, this sum is about $0.019$ while for $k_{0}=5$ this sum is $0.004$. Hence, we take $k_{0}=5$. In other words, accept $H_{1}$ if $X\ge 5$ and reject if $X < 5$. If we took $\alpha=0.0001$ we would get $k_{0}=7$ and so on.
 </p>
<p class="text-justify">
          
 <strong>Strength of evidence :</strong> Rather than merely say that we accepted $H_{1}$ or rejected it would be better to say how strong the evidence is in favour of the alternative hypothesis. This is captured by the <em> $p$-value</em>, a central concept of decision making. It is defined as <em> the probability that data drawn from the null hypothesis would show closer agreement with the alternative hypothesis than the data we have at hand</em> (read it five times!).
 </p>
           
 <p class="text-justify">
Before we compute it in our example, let us return to the analogy with law. Suppose  a man is convicted for murder. Recall that $H_{0}$ is that he is not guilty and $H_{1}$ is that he is guilty. Suppose his fingerprints were found in the house of the murdered person. Does it prove his guilt? It is some evidence in favour of it, but not necessarily strong. For example, if the convict was a friend of the murdered person, then he might be innocent but have left his fingerprints on his visits to his friend. However if the convict is a total stranger, then one wonders why, if he was innocent, his finger prints were found there. The evidence is stronger for guilt. If bloodstains are found on his shirt, the evidence would be even stronger! In saying this, we are  asking ourselves questions like ''if he was innocent, how likely is it that his shirt is blood-stained?''. That is $p$-value. Smaller the $p$-value, stronger the evidence for the alternate hypothesis.
 </p>
<p class="text-justify">
          
 Now we return to our example. Suppose the observed value is $X_{\mbox{obs} }=4$. Then the $p$-value is $\mathbf{P}\{X\ge 4\}=p_{4}+\ldots +p_{52}\approx 0.019$. If the observed value was $X_{\mbox{obs} }=6$, then the $p$-value would be $p_{6}+\ldots +p_{52}\approx 0.00059$. Note that the computation of $p$-value does not depend on the level of significance. It just depends on the given hypotheses and the chosen test.
 </p>
           

<div class="pull-right"><a href="chapter-35.html" class="btn btn-primary">Chapter 35. Testing for the mean of a normal population</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="../js/bootstrap.min.js"`></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    