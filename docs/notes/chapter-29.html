
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">

<style type="text/css">
   body { padding-top: 60px; }
   .section {padding-top: 60px;}
   #arxiv {
     border-style: solid;
     border-width: 1px;
   }
</style>


  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <span class="navbar-brand navbar-right">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav">
          <li><a href="index.html">Table of Contents</a></li>
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probabibility (part 1) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Probability (part 2) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
            </ul>
          </li>
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">
<h1 class="text-center bg-info">Chapter 29 : Estimation problems</h1>
<p>&nbsp;</p>











 <p class="text-justify">

Consider the following examples.
<ol>
<li> A coin has an unknown probability $p$ of turning up head. We wish to determine the value of $p$. For this, we toss the coin $100$ times and observe the outcomes. How to give a guess for the value of $p$ based on the data?
</li>
<li> A factory manufacture light bulbs whose lifetimes may be assumed to be exponential random variables with a mean life-time $\mu$. We take a sample of $50$ bulbs at random and measure their life-times $X_{1},\ldots ,X_{50}$. Based on this data, how can we present a reasonable guess for $\mu$? We may want to do this so that the specifications can be printed on the product when sold.
</li>
<li> Can we guess the average height $\mu$ of all people in India by taking a random sample of $100$ people and measuring their heights?
</ol>
In such questions, there is an unknown parameter $\mu$ (there could be more than one unknown parameter too) whose value we are trying to guess based on the data. The data consists of i.i.d. random variables from a family of distributions. We assume that the family of distributions is known and the only unknown is (are) the value of the parameter(s).
 Rather than present the ideas in abstract let us see a few examples.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-155"><div class="panel panel-info example"> <div class="panel-heading">Example 155 </div><div class="panel-body">Let $X_{1},\ldots ,X_{n}$ be i.i.d. random variables with Exponential density $f_{\mu}(x)=\frac{1}{\mu}e^{-x/\mu}$ (fro $x > 0$) where the value of $\mu > 0$ is unknown. How to  <em> estimate</em> it using the data $X=(X_{1},\ldots ,X_{n})$?
 </p>
           
 <p class="text-justify">
This is the framework in which we would study the second example above, namely the lie-time distribution of light bulbs. Observe that we have parameterized the exponential family of distributions differently from usual. We could equivalently have considered $g_{\lambda}(x)=\lambda e^{-\lambda x}$ but the interest is then in estimating $1/\lambda$ (which is the expected value) rather than $\lambda$. Here are two methods.
 </p>
<p class="text-justify">
          
 <strong>Method of moments :</strong> We observe that $\mu=\mathbf{E}_{\mu}[X_{1}]$, the mean of the distribution (also called <em> population mean</em>). Hence it seems reasonable to take the sample mean $\bar{X}_{n}$ as an estimate. On second thought, we realize that $\mathbf{E}_{\mu}[X_{1}^{2}]=2\mu^{2}$ and hence $\mu=\sqrt{\frac{1}{2}\mathbf{E}_{\mu}[X_{1}^{2}]}$. Therefore it also seems reasonable to take the corresponding sample quantity, $T_{n}:=\sqrt{\frac{1}{2n}(X_{1}^{2}+\ldots +X_{n}^{2})}$ as an estimate for $\mu$. One can go further and write $\mu$ in various ways as $\mu=\sqrt{\mbox{Var}_{\mu}(X_{1})}$, $\mu=\sqrt[3]{\frac{1}{6}\mathbf{E}_{\mu}[X_{1}^{3}]}$ etc. Each such expression motivates an estimate, just by substituting sample moments for population moments.
 </p>
           
 <p class="text-justify">
This is called estimating by the <em> method of moments</em> because we are equating the sample moments to population moments to obtain the estimate.
 </p>
<p class="text-justify">
          
 We can also use other features of the distribution, such as quantiles (we may call this  the ''method of quantiles''). In other words, obtain estimates by equating the sample quantiles to population quantiles. For example, the median of $X_{1}$ is $\mu\log 2$, hence a reasonable estimate for $\mu$ is $M_{n}/\log 2$, where $M_{n}$ is a sample median. Alternately, the $25\%$ quantile of $\mbox{Exponential}(1/\mu)$ distribution is $\mu\log(4/3)$ and hence another estimate for $\mu$ is $Q_{n}/\log(4/3)$ where $Q_{n}$ is a $25\%$ sample quantile.
 </p>
           
 <p class="text-justify">
<strong>Maximum likelihood method :</strong> The joint density of $X_{1},\ldots ,X_{n}$ is $$g_{\mu}(x_{1},\ldots ,x_{n})=\mu^{-n}e^{-\mu(x_{1}+\ldots +x_{n})} \qquad \mbox{ if all }x_{i} > 0$$ (since $X_{i}$ are independent, the joint density is a product). We evaluate the joint density at the observed data values. This is called the likelihood function. In other words, define,
$$
L_{X}(\mu) := \mu^{-n}e^{-\frac{1}{\mu}\sum_{i=1}^{n}X_{i} }.
$$
Two points: This is the joint density of $X_{1},\ldots ,X_{n}$, evaluated at the observed data. Further, we like to think of it as a function of $\mu$ with $X:=(X_{1},\ldots ,X_{n})$ being fixed.
 </p>
<p class="text-justify">
          
 When $\mu$ is the actual value, then $L_{X}(\mu)$ is the ''likelihood'' of seeing the data that we have actually observed. The <em> maximum likelihood estimate</em> is that value of $\mu$ that maximizes the likelihood function. In our case, by differentiating and setting equal to zero we get,
$$
0 =\frac{d}{d\mu}L_{X}(\mu) = -n\mu^{-n-1}e^{-\frac{1}{\mu}\sum_{i=1}^{n}X_{i} }+\mu^{-n}\left(\frac{1}{\mu^{2} }\sum_{i=1}^{n}X_{i}\right)e^{-\frac{1}{\mu}\sum_{i=1}^{n}X_{i} }
$$
which is satisfied when $\mu=\frac{1}{n}\sum_{i=1}^{n}X_{i}=\bar{X}_{n}$. To distinguish this from the true value of $\mu$ which is unknown, it is customary to put a hat on the leter $\mu$. We write $\hat{\mu}_{MLE}=\bar{X}_{n}$. We should really verify whether $L(\mu)$ is maximized or minimized (or neither) at this point, but we leave it to you to do the checking (eg., by looking at the second derivative).
</div></div></div>
 </p>
           
 <p class="text-justify">
Let us see the same methods at work in two more examples.
<p>&nbsp;</p>
                <div id="theorem-156"><div class="panel panel-info example"> <div class="panel-heading">Example 156 </div><div class="panel-body"> Let $X_{1},\ldots ,X_{n}$ be i.i.d. Ber($p$) random variables where the value of $p$ is unknown. How to  <em> estimate</em> it using the data $X=(X_{1},\ldots ,X_{n})$?
 </p>
<p class="text-justify">
          
 <strong>Method of moments :</strong> We observe that $p=\mathbf{E}_{p}[X_{1}]$, the mean of the distribution (also called <em> population mean</em>). Hence, a method of moments estimator would be   the sample mean $\bar{X}_{n}$. In this case, $\mathbf{E}_{p}[X_{1}^{2}]=p$ again but we don't get any new estimate because $X_{k}^{2}=X_{k}$ (as $X_{k}$ is $0$ or $1$)
 </p>
           
 <p class="text-justify">
<strong>Maximum likelihood method :</strong> Now we have a probability mass function instead of density. The joint pmf of of $X_{1},\ldots ,X_{n}$ is $f_{p}(x_{1},\ldots ,x_{n}=p^{\sum_{i=1}^{n}x_{i} }(1-p)^{n-\sum_{i=1}^{n}x_{i} }$ when each $x_{i}$ is $0$ or $1$. The likelihood function is
$$
L_{X}(p) := p^{\sum_{i=1}^{n}x_{i} }(1-p)^{n-\sum_{i=1}^{n}x_{i} } = p^{n\bar{X}_{n} }(1-p)^{n(1-\bar{X}_{n})}.
$$
We need to find the value of $p$ that maximizes $L_{X}(p)$. Here is a trick that almost always simplifies calculations (try it in the previous example too!). Instead of maximizing $L_{X}(p)$, maximize $\ell_{X}(p)=\log L_{X}(p)$ (called the <em> log-likelihood function</em>). Since ''$\log$'' is an increasing function, the maximizer will remain the same. In our case,
$$
\ell_{X}(p)=\bar{X}_{n}\log p + n(1-\bar{X}_{n})\log (1-p).
$$
 Differentiating and setting equal to $0$, we get $\hat{p}_{MLE}=\bar{X}_{n}$. Again the sample mean is the maximum likelihood estimate.
</div></div></div>
 </p>
<p class="text-justify">
          
 A last example.
<p>&nbsp;</p>
                <div id="theorem-157"><div class="panel panel-info example"> <div class="panel-heading">Example 157 </div><div class="panel-body"> Consider the two-parameter Laplace-density $f_{\theta,\alpha}(x)=\frac{1}{2\alpha}e^{-\frac{|x-\theta|}{\alpha} }$ for all $x\in \mathbb{R}$.  Check that $f_{\theta,\alpha}$ is indeed a density for all $\theta\in \mathbb{R}$ and $\alpha > 0$.
 </p>
           
 <p class="text-justify">
Now suppose we have data $X_{1},\ldots ,X_{n}$ i.i.d. from $f_{\theta,\alpha}$ where we do not know the values of $\theta$ and $\alpha$. How to estimate the parameters?
 </p>
<p class="text-justify">
          
 <strong>Method of moments :</strong> We compute
$$\begin{align*}
\mathbf{E}_{\theta,\alpha}[X_{1}]&=\frac{1}{2\alpha}\int\limits_{-\infty}^{+\infty}te^{-\frac{|t-\theta|}{\alpha} }dt = \frac{1}{2}\int\limits_{-\infty}^{+\infty}(\alpha s+\theta) e^{-|s|}ds = \theta.\\
\mathbf{E}_{\theta,\alpha}[X_{1}^{2}]&=\frac{1}{2\alpha}\int\limits_{-\infty}^{+\infty}t^{2}e^{-\frac{|t-\theta|}{\alpha} }dt = \frac{1}{2}\int\limits_{-\infty}^{+\infty}(\alpha s+\theta)^{2} e^{-|s|}ds = 2\alpha^{2}+\theta^{2}.
\end{align*}$$
Thus the variance is $\mbox{Var}_{\theta,\alpha}(X_{1})=2\alpha^{2}$. Based on this, we can take the method of moments estimate to be $\hat{\theta}_{n}=\bar{X}_{n}$ (sample mean) and $\hat{\alpha}_{n}=\frac{1}{\sqrt{2} }s_{n}$ where $s_{n}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}$. At the moment the ideas of defining sample variance as $s_{n}^{2}$ may look strange and it might be more natural to take $V_{n}:=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}$ as an estimate for the population variance. As we shall see later, $s_{n}^{2}$ has some desirable properties that $V_{n}$ lacks. Whenever we say sample variance, we mean $s_{n}^{2}$, unless stated otherwise.
 </p>
           
 <p class="text-justify">
<strong>Maximum likelihood method :</strong> The likelihood function of the data is
$$
L_{X}(\theta,\alpha)=\prod\limits_{k=1}^{n}\frac{1}{2\alpha}\exp\left\{-\frac{|X_{k}-\theta|}{\alpha}\right\}= 2^{-n}\alpha^{-n}\exp\left\{-\sum_{k=1}^{n}\frac{|X_{k}-\theta|}{\alpha}\right\}.
$$
The log-likelihood function is $$\ell_{X}(\theta,\alpha)=\log L(\theta,\alpha)=-n\log 2 - n\log \alpha -\frac{1}{\alpha}\sum_{k=1}^{n}|X_{k}-\theta|.$$ We know that<sup> <a data-toggle="collapse" href="#footnote-1" aria-expanded="false" aria-controls="footnote-1">
                                1
                                 </a> </sup><span class="collapse small" id="footnote-1">If you do not know here is an argument. Let $x_{1} < x_{2} < \ldots  < x_{n}$ be $n$ distinct real numbers and let $a\in \mathbb{R}$. Rewrite $\sum_{k=1}^{n}|x_{k}-a|$ as $(|x_{1}-a|+|x_{n}-a|)+(|x_{2}-a|+|x_{n-1}-a|)+\ldots$. By triangle inequality, we see that $$|x_{1}-a|+|x_{n}-a|\ge x_{n}-x_{1}, \;\;\; |x_{2}-a|+|x_{n-1}-a|\ge x_{n-1}-x_{2}, \;\;\; |x_{3}-a|+|x_{n-2}-a|\ge x_{n-2}-x_{3}\ldots.
$$
Further the first inequality is an equality if and only if $x_{1}\le a\le x_{n}$, the second inequality is an equality if and only if $x_{2}\le a\le x_{n-1}$ etc. In particular, if $a$ is a median, then all these inequalities become equalities and shows that a median minimizes the given sum.
</span>  for fixed $X_{1},\ldots ,X_{n}$, the value of $\sum_{k=1}^{n}|X_{k}-\theta|$ is minimized when $\theta=M_{n}$, the median of $X_{1},\ldots ,X_{n}$ (strictly speaking the median may have several choices, all of them are equally good). Thus we fix $\hat{\theta}=M_{n}$ and then we maximize $\ell(\hat{\theta},\alpha)$ over $\alpha$ by differentiating. We get $\hat{\alpha}=\frac{1}{n}\sum_{k=1}^{n}|X_{k}-\theta|$ (the sample mean-absolute deviation about the median). Thus the MLE of $(\theta,\alpha)$ is $(\hat{\theta},\hat{\alpha})$.
</div></div></div>
 </p>
<p class="text-justify">
          
 In homeworks and tutorials you will see several other estimation problems which we list in the exercise below.
<p>&nbsp;</p>
                <div id="theorem-158"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 158 </div><div class="panel-body"> Find an estimate for the unknown parameters by the method of moments and the maximum likelihood method.
 <ol>
<li> $X_{1},\ldots, X_{n}$ are i.i.d. $N(\mu,1)$. Estimate $\mu$. How do your estimates change if the distribution is $N(\mu,2)$?
</li>
<li> $X_{1},\ldots, X_{n}$ are i.i.d. $N(0,{\sigma}^{2})$. Estimate ${\sigma}^{2}$. How do your estimates change if the distribution is $N(7,{\sigma}^{2})$?
</li>
<li> $X_{1},\ldots, X_{n}$ are i.i.d. $N(\mu,{\sigma}^{2})$. Estimate $\mu$ and ${\sigma}^{2}$.
</ol>
[<strong> Note:</strong> The first case is when ${\sigma}^{2}$ is known and $\mu$ is unknown. Then the known value of ${\sigma}^{2}$ may be used to estimate $\mu$. In the second case it is similar, now $\mu$ is known and ${\sigma}^{2}$ is not known. In the third case, both are unknown].
</div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-159"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 159 </div><div class="panel-body"> $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Geo}(p)$ Estimate $\mu=1/p$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-160"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 160 </div><div class="panel-body"> $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Pois}(\lambda)$ Estimate $\lambda$.
</div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-161"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 161 </div><div class="panel-body"> $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Beta}(a,b)$ Estimate $a,b$.
</div></div></div>
 </p>
<p class="text-justify">
          
 The following exercise is approachable by the same methods but requires you to think a little.
<p>&nbsp;</p>
                <div id="theorem-162"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 162 </div><div class="panel-body"> $X_{1},\ldots, X_{n}$ are i.i.d. $\mbox{Uniform}[a,b]$ Estimate $a,b$.
</div></div></div>
 </p>
           

<div class="pull-right"><a href="chapter-30.html" class="btn btn-primary">Chapter 30. Properties of estimates</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="../js/bootstrap.min.js"`></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    