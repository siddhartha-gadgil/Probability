
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">
<link rel="stylesheet" href="../css/extras.css">



  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    
<div class="container-fluid">
<div class="banner">
<h1 class="text-center bg-primary">Chapter 16 : Examples of continuous distributions</h1>
</div>
</div>

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <span class="navbar-brand navbar-right">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav">
          <li><a href="index.html">Table of Contents</a></li>
            
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 1 to 14 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
<li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
  </ul>
</li>
    

<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 15 to 28 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
<li><a href="chapter-28.html">28. Introduction</a> </li>
  </ul>
</li>
    

<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 29 to 40 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
  </ul>
</li>
    


        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">









 <p class="text-justify">

Cumulative distributions will also be referred to as simply distribution functions or distributions. We start by giving two large classes of CDFs. There are CDFs that do not belong to either of these classes, but for practical purposes they may be ignored (for now).
<ol>
<li> (CDFs with pmf). Let $f$ be a pmf, i.e., let $t_{1},t_{2},\ldots$ be a countable subset of reals and let $f(t_{i})$ be non-negative numbers such that $\sum_{i}f(t_{i})=1$. Then,  define $F:\mathbb{R}\rightarrow \mathbb{R}$ by
$$
 F(t) := \sum_{i: t_{i}\le t}f(t_{i}).
$$
Then, $F$ is a CDF. Indeed, we have seen that it is the CDF of a discrete random variable. A special feature of this CDF is that it increases only in jumps (in more precise language, if $F$ is continuous on an interval $[s,t]$, then $F(s)=F(t)$).
</li>
<li> (CDFs with pdf). Let $f:\mathbb{R}\rightarrow\mathbb{R}_{+}$ be a  function (convenient to assume that it is a piece-wise continuous function) such that $\int_{-\infty}^{+\infty}f(u)du=1$. Such a function is called a <em> probability density function</em> or pdf for short.  Then,  define $F:\mathbb{R}\rightarrow \mathbb{R}$ by
\[\begin{aligned}
F(t) :=\int_{-\infty}^{t}f(u) du.
\end{aligned}\]
Again, $F$ is a CDF. Indeed, it is clear that $F$ has the increasing property (if $t > s$, then $F(t)-F(s)=\int_{s}^{t}f(u)du$ which is non-negative because $f(u)$ is non-negative for all $u$), and its limits at $\pm \infty$ are as they should be (why?). As for right-continuity, $F$ is in-fact continuous. Actually $F$ is differentiable except at points where $f$ is discontinuous and $F'(t)=f(t)$.
</ol>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-92"><div class="panel panel-success remark"> <div class="panel-heading">Remark 92 </div><div class="panel-body"> We understand the pmf. For example if $X$ has pmf $f$, then $f(t_{i})$ is just the probability that $X$ takes the value $t_{i}$. How to interpret the pdf? If $X$ has pdf $f$, then as we already remarked, the CDF is continuous and hence $\mathbf{P}\{X=t\}=0$. Therefore $f(t)$ cannot be interpreted as $\mathbf{P}\{X=t\}$ (in fact, pdf can take values greater than $1$, so it cannot be a probability!).
 </p>
           
 <p class="text-justify">
To interpret $f(a)$, take a small positive number $\delta$ and look at
$$
F(a+\delta)-F(a)  = \int\limits_{a}^{a+\delta}f(u) du \approx \delta f(a).
$$
 In other words, $f(a)$ measures the chance of the random variable taking values near $a$. Higher the pdf, greater the chance of taking values near that point.
</div></div></div>
 </p>
<p class="text-justify">
          
 Among distributions with pmf, we have seen the Binomial, Poisson, Geometric and Hypergeometric families of distributions. Now we give many important examples of distributions (CDFs) with densities.
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-93"><div class="panel panel-info example"> <div class="panel-heading">Example 93 </div><div class="panel-body"> <strong>Uniform distribution on the interval $[a,b]$ :</strong>, denoted Unif($[a,b]$) where $a < b$ is the distribution with density and distribution given by
$$
\mbox{PDF:}  f(t) = \begin{cases}\frac{1}{b-a} & \mbox{if } t\in(a,b) \ 0 & \mbox{otherwise} \end{cases}\qquad
\mbox{CDF:}  F(t) = \begin{cases}0 & \mbox{if } t\le a \ \frac{t-a}{b-a} & \mbox{if }t\in (a,b) \ 1 & \mbox{if }t\ge b.\end{cases}
$$
 </p>
<p class="text-justify">
          
 </div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-94"><div class="panel panel-info example"> <div class="panel-heading">Example 94 </div><div class="panel-body"> <strong>Exponential distribution with parameter $\lambda$ :</strong>, denoted Exp($\lambda$) where $\lambda > 0$ is the distribution with density and distribution given by
$$
\mbox{PDF:}  f(t) = \begin{cases}\lambda e^{-\lambda t}& \mbox{if } t > 0 \ 0 & \mbox{otherwise} \end{cases}\qquad
\mbox{CDF:}  F(t) = \begin{cases}0 & \mbox{if } t\le 0 \ 1-e^{-\lambda t} & \mbox{if }t > 0.\end{cases}
$$
</div></div></div>
<p>&nbsp;</p>
                <div id="theorem-95"><div class="panel panel-info example"> <div class="panel-heading">Example 95 </div><div class="panel-body"> <strong>Normal distribution with parameters $\mu,{\sigma}^{2}$ :</strong>, denoted N($\mu,{\sigma}^{2}$) where $\mu\in \mathbb{R}$ and ${\sigma}^{2} > 0$ is the distribution with density and distribution given by
$$
\mbox{PDF:}  \varphi_{\mu,{\sigma}^{2} }(t) = \frac{1}{\sqrt{2\pi} }e^{-\frac{1}{2{\sigma}^{2} }(t-\mu)^{2} }\qquad
\mbox{CDF:}  \Phi_{\mu,{\sigma}^{2} }(t) = \int\limits_{-\infty}^{t}\varphi_{\mu,{\sigma}^{2} }(u)du.
$$
There is no closed form expression for the CDF. It is standard notation to write $\varphi$ and $\Phi$ to denote the normal density and CDF when $\mu=0$ and ${\sigma}^{2}=1$. N($0,1$) is called the standard normal distribution. By a change of variable one can check that $\Phi_{\mu,{\sigma}^{2} }(t)=\Phi(\frac{t-\mu}{{\sigma}})$.
 </p>
<p class="text-justify">
          
 We said that the normal CDF has no simple expression, but is it even clear that it is a CDF?! In other words, is the proposed density a true pdf? Clearly $\varphi(t)=\frac{1}{\sqrt{2\pi} }e^{-t^{2}/2}$ is non-negative. We need to check that its integral is $1$.
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-96"><div class="panel panel-primary lemma"> <div class="panel-heading">Lemma 96 </div><div class="panel-body"> Fix $\mu\in \mathbb{R}$ and ${\sigma} > 0$ and let $\varphi(t)=\frac{1}{\sqrt{2\pi} }e^{-\frac{1}{2{\sigma}^{2} }(t-\mu)^{2} }$. Then, $\int\limits_{-\infty}^{\infty}\varphi(t) dt =1$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <div class="proof"> It suffices to check the case $\mu=0$ and ${\sigma}^{2}=1$ (why?).   To find its integral is quite non-trivial. Let $I=\int_{-\infty}^{\infty} \varphi(t)dt$. We introduce the two-variable function $h(t,s):=\varphi(t)\varphi(s)=(2\pi)^{-1}e^{-(t^{2}+s^{2})/2}$. On the one hand,
$$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h(t,s)dtds = \left(\int_{-\infty}^{+\infty}\varphi(t)dt\right) \left(\int_{-\infty}^{+\infty}\varphi(s)ds\right)=I^{2}.
$$
On the other hand, using polar co-ordinates $t=r\cos\theta$, $s=r\sin \theta$, we see that
$$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h(t,s)dtds =\int_{0}^{\infty}\int_{0}^{2\pi}(2\pi)^{-1}e^{-r^{2}/2}rd\theta dr = \int_{0}^{\infty}re^{-r^{2}/2}dr =1
$$
since $\frac{d}{dr}e^{-r^{2}/2}=-re^{-r^{2}/2}$. Thus $I^{2}=1$ and hence $I=1$.
</div>
</div></div></div>
<p>&nbsp;</p>
                <div id="theorem-97"><div class="panel panel-info example"> <div class="panel-heading">Example 97 </div><div class="panel-body"> <strong>Gamma distribution with shape parameter $\nu$ and scaler parameter $\lambda$ :</strong>, where $\nu > 0$ and $\lambda > 0$, denoted Gamma($\nu,\lambda$) is the distribution with density and distribution given by -
$$
\mbox{PDF:}  f(t) = \begin{cases}\frac{1}{\Gamma(\nu)}\lambda^{\nu} t^{\nu-1}e^{-\lambda t}& \mbox{if } t > 0 \ 0 & \mbox{otherwise} \end{cases}\qquad
\mbox{CDF:}  F(t) = \begin{cases}0 & \mbox{if } t\le 0 \ \int_{0}^{t}f(u)du & \mbox{if }t > 0.\end{cases}
$$
Here $\Gamma(\nu):=\int_{0}^{\infty}t^{\nu-1}e^{-t}dt$. Firstly, $f$ is a density, that is, that it integrates to $1$. To see this, make the change of variable $\lambda t=u$ to see that
$$
\int_{0}^{\infty}\lambda^{\nu}e^{-\lambda t}t^{\nu-1}dt = \int_{0}^{\infty}e^{-u}u^{\nu-1}d\nu = \Gamma(\nu).
$$
Thus, $\int_{0}^{\infty} f(t)dt=1$.
 </p>
           
 <p class="text-justify">
When $\nu=1$, we get back the exponential distribution. Thus, the Gamma family subsumes the exponential distributions.   For positive integer values of $\nu$, one can actually write an expression for the CDF of Gamma($\nu,\lambda$)  as (this is a homework problem)
$$
F_{\nu,\lambda}(t)=1-e^{-\lambda t}\sum\limits_{k=0}^{\nu-1}\frac{(\lambda t)^{k} }{k!}.
$$
Once the expression is given, it is easy to check it by induction (and integration by parts). A curious observation is that the right hand side is exactly $\mathbf{P}(N\ge \nu)$ where $N\sim \mbox{Pois}(\lambda t)$. This is in fact indicating a deep connection between Poisson distribution and the Gamma distributions. The function $\Gamma(\nu)$, also known as Euler's Gamma function, is an interesting and important  one and occurs all over mathematics.
<sup> <a data-toggle="collapse" href="#footnote-1" aria-expanded="false" aria-controls="footnote-1">
                                1
                                 </a> </sup><span class="collapse small" id="footnote-1"><strong> The Gamma function:</strong> The function $\Gamma:(0,\infty)\rightarrow \mathbb{R}$ defined by $\Gamma(\nu)=\int_{0}^{\infty}e^{-t}t^{\nu-1}dt$ is a very important function that often occurs in mathematics and physics. There is no simpler expression for it, although one can find it explicitly for special values of $\nu$. One of its most important properties is that $\Gamma(\nu+1)=\nu\Gamma(\nu)$. To see this, consider
$$
\Gamma(\nu+1)=\int_{0}^{\infty}e^{-t}t^{\nu}dt = -e^{-t}t^{\nu}\left.\vphantom{\hbox{\Large (}}\right|_{0}^{\infty}+\nu\int_{0}^{\infty}e^{-t}t^{\nu-1}dt = \nu \Gamma(\nu).
$$
Starting with  $\Gamma(1)=1$ (direct computation) and using the above relationship repeatedly one sees that $\Gamma(\nu)=(\nu-1)!$ for positive integer values of $\nu$. Thus, the Gamma function interpolates the factorial function (which is defined only for positive integers).  Can we compute it for any other $\nu$? The answer is yes, but only for special values of $\nu$. For example,
\[\begin{aligned}
\Gamma(1/2)= \int_{0}^{\infty}x^{-1/2}e^{-x}dx = \sqrt{2}\int_{0}^{\infty}e^{-y^{2}/2}dy
\end{aligned}\]
by substituting $x=y^{2}/2$. The last integral was computed above in the context of the normal distribution and equal to $\sqrt{\pi/2}$. Hence we get $\Gamma(1/2)=\sqrt{\pi}$. From this, using again the relation $\Gamma(\nu+1)=\nu\Gamma(\nu)$, we can compute $\Gamma(3/2)=\frac{1}{2}\sqrt{\pi}$, $\Gamma(5/2)=\frac{3}{4}\sqrt{\pi}$, etc. Yet another useful fact about the Gamma function is its asymptotics as $\nu\rightarrow\infty$.
 </p>
<p class="text-justify">
          
 <strong> Stirling's approximation:</strong> $\frac{\Gamma(\nu+1)}{\nu^{\nu+\frac{1}{2} }e^{-\nu}\sqrt{2\pi} }\rightarrow 1$ as $\nu\rightarrow \infty$.
 </p>
           
 <p class="text-justify">
<strong>A small digression :</strong> It was Euler's  idea to observe that $n!=\int_{0}^{\infty}x^{n}e^{-x}dx$ and that on the right side $n$ could be replaced by  any real number greater than $-1$. But this was his second approach to defining the Gamma function. His first approach was as follows. Fix a positive integer $n$. Then for any $\ell\ge 1$ (also a positive integer), we may write
\[\begin{aligned}
n!=\frac{(n+\ell)!}{(n+1)(n+2)\ldots (n+\ell)} = \frac{\ell!(\ell+1)\ldots (\ell+n)}{(n+1)\ldots (n+\ell)} = \frac{\ell!  \ell^{n} }{(n+1)\ldots (n+\ell)}\cdot\frac{(\ell+1)\ldots (\ell+n)}{\ell^{n} }
\end{aligned}\]
The second factor approaches $1$ as $\ell\rightarrow \infty$. Hence,
\[\begin{aligned}
n!=\lim_{\ell\rightarrow \infty}\frac{\ell!  \ell^{n} }{(n+1)\ldots (n+\ell)}.
\end{aligned}\]
Euler then showed (by a rather simple argument that we skip) that the limit on the right exists if we replace $n$ by any complex number other than $\{-1,-2,-3,\ldots \}$ (negative integers are a problem as they make the denominator zero). Thus, he extended the factorial function to all complex numbers except negative integers! It is a fun exercise to check that this agrees with the definition by the integral given earlier. In other words, for $\nu > -1$, we have
\[\begin{aligned}
\lim_{\ell\rightarrow \infty}\frac{\ell!  \ell^{\nu} }{(\nu+1)\ldots (\nu+\ell)}=\int_{0}^{\infty}x^{\nu}e^{-x}dx.
\end{aligned}\]
</span> 
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-98"><div class="panel panel-info example"> <div class="panel-heading">Example 98 </div><div class="panel-body"> <strong>Beta distributions :</strong> Let $\alpha,\beta > 0$. The Beta distribution with parameters $\alpha,\beta$, denoted Beta($\alpha,\beta$), is the distribution with density and distribution given by -
\[\begin{aligned}
\mbox{PDF:}  f(t) = \begin{cases}\frac{1}{B(\alpha,\beta)}t^{\alpha-1}(1-t)^{\beta-1}& \mbox{if } t\in(0,1) \ 0 & \mbox{otherwise} \end{cases}\qquad
\mbox{CDF:}  F(t) = \begin{cases}0 & \mbox{if } t\le 0 \ \int_{0}^{t}f(u)du & \mbox{if }t\in(0,1) \\
0 &\mbox{if }t\ge 1.\end{cases}
\end{aligned}\]
Here $B(\alpha,\beta):=\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt$. Again, for special values of $\alpha,\beta$ (eg., positive integers), one can find the value of $B(\alpha,\beta)$, but in general there is no simple expression. However, it can be expressed in terms of the Gamma function!
<p>&nbsp;</p>
                <div id="theorem-99"><div class="panel panel-primary proposition"> <div class="panel-heading">Proposition 99 </div><div class="panel-body"> For any $\alpha,\beta > 0$, we have $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$.
</div></div></div>
<div class="proof"> For $\beta=1$ we see that $B(\alpha,1)=\int_{0}^{1}t^{\alpha-1}=\frac{1}{\alpha}$ which is also equal to $\frac{\Gamma(\alpha)\Gamma(1)}{\Gamma(\alpha+1)}$ as required. Similarly (or by the symmetry relation $B(\alpha,\beta)=B(\beta,\alpha)$), we see that $B(1,\beta)$ also has the desired expression.
 </p>
           
 <p class="text-justify">
Now for any other <em> positive integer</em> value of $\alpha$ and real $\beta > 0$ we can integrate by parts and get
$$\begin{align*}
B(\alpha,\beta)&=\int_{0}^{1}t^{\alpha-1}(1-t)^{\beta-1}dt \\
&= -\frac{1}{\beta}t^{\alpha-1}(1-t)^{\beta}\left.\vphantom{\hbox{\Large (}}\right|_{0}^{1} + \frac{\alpha-1}{\beta}\int_{0}^{1}t^{\alpha-2}(1-t)^{\beta}dt \\
&= \frac{\alpha-1}{\beta}B(\alpha-1,\beta+1).
\end{align*}$$
Note that the first term vanishes because $\alpha >  1$ and $\beta > 0$. When $\alpha$ is an integer, we repeat this for $\alpha$ times and get
$$
B(\alpha,\beta)=\frac{(\alpha-1)(\alpha-2)\ldots 1}{\beta(\beta+1)\ldots (\beta+\alpha-2)}B(1,\beta+\alpha-1).
$$
But we already checked that $B(1,\beta+\alpha-1)=\frac{\Gamma(1)\Gamma(\alpha+\beta-1)}{\Gamma(\alpha+\beta)}$ from which we get
$$
B(\alpha,\beta) = \frac{(\alpha-1)(\alpha-2)\ldots 1}{\beta(\beta+1)\ldots (\beta+\alpha-2)}\frac{\Gamma(1)\Gamma(\alpha+\beta-1)}{\Gamma(\alpha+\beta)} =\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
$$
by the recursion property of the Gamma function. Thus we have proved the proposition when $\alpha$ is a positive integer. By symmetry the same is true when $\beta$ is a positive integer (and $\alpha$ can take any value). We do not bother to prove the proposition for general $\alpha,\beta > 0$ here.
</div>
</div></div></div>
<p>&nbsp;</p>
                <div id="theorem-100"><div class="panel panel-info example"> <div class="panel-heading">Example 100 </div><div class="panel-body"> <strong>The standard Cauchy distribution :</strong>  is the distribution with density and distribution given by
$$
\mbox{PDF:}  f(t) = \frac{1}{\pi(1+t^{2})}\qquad
\mbox{CDF:}  F(t) = \frac{1}{2}+\frac{1}{\pi}\tan^{-1}t.
$$
One can also make a parametric family of Cauchy distributions with parameters $\lambda > 0$ and $a\in \mathbb{R}$ denoted Cauchy($a,\lambda$) and having density and CDF
$$
f(t)=\frac{\lambda}{\pi(\lambda^{2}+(t-a)^{2})}\qquad F(t)=\frac{1}{2}+\frac{1}{\pi}\tan^{-1}\left(\frac{t-a}{\lambda}\right).
$$
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-101"><div class="panel panel-success remark"> <div class="panel-heading">Remark 101 </div><div class="panel-body"> Does every CDF come from a pdf? Not necessarily. For example any CDF that is not continuous (for example, CDFs of discrete distributions such as Binomial, Poisson, Geometric etc.). In fact even continuous CDFs may not have densities (there is a good example manufactured out of the $1/3$-Cantor set, but that would take us out of the topic now). However, suppose $F$ is a <em> continuous</em> CDF and suppose $F$ is differentiable except at finitely many points and that the derivative is a continuous function. Then $f(t):=F'(t)$ defines a pdf which by the fundamental theorm of Calculus satisfies $F(t)=\int_{-\infty}^{t}f(u)du$.
</div></div></div>
 </p>
           

<div class="pull-right"><a href="chapter-17.html" class="btn btn-primary">Chapter 17. Simulation</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="../js/bootstrap.min.js"`></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    