
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">
<link rel="stylesheet" href="../css/extras.css">



  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    
<div class="container-fluid">
<div class="banner">
<h1 class="text-center bg-primary">Chapter 38 : Chi-squared test for goodness of fit</h1>
</div>
</div>

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <span class="navbar-brand navbar-right">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav">
          <li><a href="index.html">Table of Contents</a></li>
            
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 1 to 14 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
<li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
  </ul>
</li>
    

<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 15 to 28 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
<li><a href="chapter-28.html">28. Introduction</a> </li>
  </ul>
</li>
    

<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Chapters 29 to 40 <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
  </ul>
</li>
    


        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">










 <p class="text-justify">

At various times we have made statements such as ''heights follow normal distribution'', ''lifetimes of bulbs follow exponential distribution'' etc. Where do such claims come from? Over years of analysing data, of course. This leads to an interesting question. Can we test whether  lifetimes of bulbs do follow exponential distribution?
 </p>
<p class="text-justify">
          
 We start with a simple example of testing whether a die is fair.  The hypotheses are  $H_{0}:$ the die is fair, versus $H_{1}:$ the die is unfair<sup> <a data-toggle="collapse" href="#footnote-1" aria-expanded="false" aria-controls="footnote-1">
                                1
                                 </a> </sup><span class="collapse small" id="footnote-1">You may feel that the null and alternative hypotheses are reversed. Is not independence a special property that should prove itself. Yes and no. Here we are imagining a situation where we have some reason to think that the die is fair. For example perhaps the die looks symmetric.</span> .
 </p>
           
 <p class="text-justify">
We throw the die $n$ times and record the observations $X_{1},\ldots ,X_{n}$. For $j\le 6$, let $O_{j}$ be the number of times we observe the face $j$ turn up. In symbols $O_{j}=\sum_{i=1}^{n}{\mathbf 1}_{X_{i}=j}$. Let $E_{j}=\mathbf{E}[O_{j}]=\frac{n}{6}$ be the expected number of times we see the face $j$ (under the null hypothesis). Common sense says that if $H_{0}$ is true then $O_{j}$ and $E_{j}$ must be rather close for each $j$. How to measure the closeness? Karl Pearson introduced the test statistic
$$
 T:=\sum_{j=1}^{6}\frac{(O_{j}-E_{j})^{2} }{E_{j} }.
$$
If the desired level of significance is $\alpha$, then the Pearson $\chi^{2}$-test says ''Reject $H_{0}$ if $T\ge \chi^{2}_{5}(\alpha)$''. The number of degrees of freedom is $5$ here. In general, it is one less than the number of bins (i.e., how many terms you are summing to get $T$).
 </p>
<p class="text-justify">
          
 <strong>Some practical points :</strong> The $\chi^{2}$ test is really an asymptotic statement. For large $n$, the level of significance is approximately $1-\alpha$. There is no assurance for small $n$. Further, in performing the test, it is recommended that each bin must have at least $5$ observations (i.e., $O_{j}\ge 5$). Otherwise we club together bins with fewer entries. The number $5$ is a rule of thumb, the more the better.
 </p>
           
 <p class="text-justify">
<strong>Fitting the Poisson distribution :</strong> We consider the famous data collected by Rutherford, Chadwick and Ellis on the number of radioactive disintegrations. For details see the book of Feller's book (section VI.7) or \hrefhttp://galton.uchicago.edu/&nbsp;lalley/Courses/312/PoissonProcesses.pdfthis website.
 </p>
<p class="text-justify">
          
 The data consists of $X_{1},\ldots ,X_{2608}$ (where $X_{k}$ is the number of particles detected by the counter in the $k^{\mbox{th} }$ time interval. The hypotheses are
 $$
 H_{0}: \; F \mbox{ is a Poisson distribution}. \qquad H_{1}: \; F \mbox{ is not Poisson}.
 $$
The physical theories predict that the distribution ought to be Poisson and hence we have taken it as the null hypothesis<sup> <a data-toggle="collapse" href="#footnote-2" aria-expanded="false" aria-controls="footnote-2">
                                2
                                 </a> </sup><span class="collapse small" id="footnote-2">When a new theory is proposed, it should prove itself and is put in the alterntive hypotheis, but here we take it as null.</span> 
 </p>
           
 <p class="text-justify">
We define $O_{j}$ as the number of time intervals in which we see exactly $j$ particles. Thus $O_{j}=\sum_{i=1}^{2608}{\mathbf 1}_{X_{i}=j}$. How do we find the expected numbers? If the null hypothesis had said that $F$ has Poisson(1) distribution, we could use that to find the expected numbers. But $H_{0}$ only says Poisson($\lambda$) for an unspecified $\lambda$? This brings in a new feature.
 </p>
<p class="text-justify">
          
 First estimate $\lambda$, for example $\hat{\lambda}=\bar{X}_{n}$ is an MLE as well as method of moments estimate. Then we use this to calculate Poisson probabilities and the expected numbers. In other words, $E_{j}=e^{-\hat{\lambda} }\frac{\hat{\lambda}^{j} }{j!}$. For the given data we find that $\hat{\lambda}=3.87$. The table is as follows.
 </p>
           
 <p class="text-justify">
\[
\begin{array}{||r|c|c|c|c|c|c|c|c|c|c|c||}
\hline
j  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & \ge 10 \\
\hline
O_{j}  & 57 & 203 & 383 & 525 & 532 & 408 & 273 & 139 & 45 & 27 & 16 \\
\hline
E_{j} & 54.4 & 210.5 & 407.4 & 525.4 & 508.4 & 393.5 & 253.8 & 140.3 & 67.9 & 29.2 & 17.1 \\
\hline
\end{array}
\]
 </p>
<p class="text-justify">
          
 Two remarks: The original data would have consisted of several more bins for $j=11,12\ldots$. These have been clubbed together to perform the $\chi^{2}$ test (instead of a minimum of $5$ per bin, they may have ensured that there are at least $10$ per bin). Also, the estimate $\hat{\lambda}=3.87$ was obtained before clubbing these bins. Indeed, if the data is merely presented as the above table, there will be some ambiguity in how to find $\hat{\lambda}$ as one of the bins says ''$\ge 10$''.
 </p>
           
 <p class="text-justify">
Then we compute
$$
 T=\sum_{j=0}^{10}\frac{(O_{j}-E_{j})^{2} }{E_{j} } = 14.7.
$$
Where should we look up in the $\chi^{2}$ table? Earlier we said that the degrees of freedom is one less than the number of bins. Here we give the more general rule.
$$
\mbox{Degrees of freedom of the }\chi^{2} = \mbox{ No. of bins }-1-\mbox{No. of parameters estimated from data}.
$$
In our case we estimated one parameter, $\lambda$ hence the d.f. of the $\chi^{2}$ is $11-1-1=9$. Looking at $\chi_{9}^{2}$ table one can see that the $p$-value is $0.10$. This is the probability that a $\chi_{9}^{2}$ random variable is greater than $14.7$. (Caution: Elsewhere I see that the $p$-value for this experiment is reported as $0.17$, please check my calculations!). This means that at $5\%$ level, we would not reject the null hypothesis. If the $p$-value was $0.17$, we would not reject the null hypothesis even at $10\%$ level.
 </p>
<p class="text-justify">
          
 <strong>Fitting a continuous distribution :</strong> Chi-squared test can be used to test goodness of fit for continuous distributions too. We need some modifications. We must make bins of appropriate size, like $[a,a+h],[a+h,a+2h],\ldots ,[a+h(k-1),a+hk]$ for a suitable $h$ and $k$. Then we find the expected numbers in each bin using the null hypothesis (first estimating some parameters if necessary) and then proceed to compute $T$ in the same way as before. Then check against the $\chi^{2}$ table with the appropriate degrees of freedom. We omit details.
 </p>
           
 <p class="text-justify">
<strong>The probability theorem behind the $\chi^{2}$-test for goodness of fit :</strong> Let $(W_{1},\ldots ,W_{k})$ have multinomial distribution with parameters $n,m,(p_{1},\ldots ,p_{k})$. (In other words, place $n$ balls at random into $m$ bins, but each ball goes into the $i^{\mbox{th} }$ bin with probability $p_{i}$ and distinct balls are assigned independently of each other). The following proposition is the mathematics behind Pearson's test.
 </p>
<p class="text-justify">
          
 <strong>Proposition [Pearson] :</strong> Fix $k,p_{1},\ldots,p_{k}$. Let $T_{n}=\sum_{i=1}^{k}\frac{(W_{i}-np_{i})^{2} }{np_{i} }$. Then  $T_{n}$ converges to a $\chi_{k-1}^{2}$ distribution in the sense that $\mathbf{P}\{T_{n}\le x\}\rightarrow \int_{0}^{x}f_{k-1}(u)du$ where $f_{k-1}$ is the density of $\chi_{k-1}^{2}$ distribution.
 </p>
           
 <p class="text-justify">
<p></p>
How does this help? Suppose $X_{1},\ldots ,X_{n}$ are i.i.d. random variables taking $k$ values (does not matter what the values are, say $t_{1},t_{2},\ldots ,t_{k}$) with probabilities $p_{1},\ldots ,p_{k}$. Then, let $W_{i}$ be the number of $X_{i}$s whose value is $t_{i}$. Clearly, $(W_{1},\ldots ,W_{k})$ has a multinomial distribution.  Therefore, for large $n$, the random variable $T_{n}$ defined above (which is in fact the $\chi^{2}$-statistic of Pearson) has approximately $\chi_{k-1}^{2}$ distribution. This explains the test.
 </p>
<p class="text-justify">
          
 <strong>Sketch of proof of the proposition :</strong> Start with the case $k=2$. Then, $W_{1}\sim \mbox{Bin}(n,p_{1})$ and $W_{2}=r-W_{1}$. Thus, $T_{n}=\frac{(W_{1}-np_{1})^{2} }{np_{1}p_{2} }$ (recall that $p_{1}+p_{2}=1$ and check this!). We know that $(W_{1}-np_{1})/\sqrt{np_{1}q_{1} }$ is approximately a $N(0,1)$ random variable, where $q_{i}=1-p_{i}$). Its square has (approximately$\chi_{1}^{2}$ distribution. Thus the proposition is proved for $k=2$.
 </p>
           
 <p class="text-justify">
When $k > 2$, what happens is that the random variables $\xi_{i}:=(W_{i}-np_{i})/\sqrt{np_{i}q_{i} }$ are approximately $N(0,1)$, but not independent. In fact the correlation between $\xi_{i}$ and $\xi_{j}$ is  close to $-\sqrt{p_{i}p_{j}/q_{i}q_{j} }$. The sum of squares of $\xi_{i}$s  gives the $\chi^{2}$ statistic. On the other hand, one can (with some clever linear algebra/matrix manipulation) write $\sum_{i=1}^{k}\xi_{i}^{2}$ as $\sum_{i=1}^{k-1}\eta_{i}^{2}$ where $\eta_{i}$ are   <em> independent</em> $N(0,1)$ random variables. Thus we get $\chi_{k-1}^{2}$ distribution.
 </p>
<p class="text-justify">
          
 
 </p>
           

<div class="pull-right"><a href="chapter-39.html" class="btn btn-primary">Chapter 39. Tests for independence</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="../js/bootstrap.min.js"`></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    