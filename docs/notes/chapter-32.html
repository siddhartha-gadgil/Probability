
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">
<link rel="stylesheet" href="../css/extras.css">



  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    
<div class="container-fluid">
<div class="banner">
<h1 class="text-center bg-primary">Chapter 32 : Confidence interval for the mean</h1>
</div>
</div>

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <span class="navbar-brand navbar-right">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav">
          <li><a href="index.html">Table of Contents</a></li>
            
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probability (part 1) <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
  </ul>
</li>
    
            
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probability (part 2) <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
  </ul>
</li>
    
            
<li class="dropdown">
<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
  <ul class="dropdown-menu">
     <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
  </ul>
</li>
    
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">



 <p class="text-justify">

Now suppose $X_{1},\ldots ,X_{n}$ are i.i.d. random variables from some distribution with mean $\mu$ and variance ${\sigma}^{2}$, both unknown. How can we construct a confidence interval for $\mu$?
 </p>
<p class="text-justify">
          
 In case of normal distribution, recall that the $(1-\alpha)$-CI that we gave was
$$
\left[\bar{X}_{n}-\frac{s_{n} }{\sqrt{n} }t_{n-1}\left(\frac{\alpha}{2}\right),\bar{X}_{n}+\frac{s_{n} }{\sqrt{n} }t_{n-1}\left(\frac{\alpha}{2}\right)\right] or \left[\bar{X}_{n}-\frac{s_{n} }{\sqrt{n} }z_{\alpha/2},\bar{X}_{n}+\frac{s_{n} }{\sqrt{n} }z_{\alpha/2}\right]
$$
 Is this a valid confidence interval in general? The answer is ''No'' for both. If $X_{i}$ are from some general distribution then the distributions of $\sqrt{n}(\bar{X}_{n}-\mu)/s_{n}$ and $\sqrt{n}(\bar{X}_{n}-\mu)/{\sigma}$ are very complicated to find. Even if $X_{i}$ come from binomial or exponential family, these distributions will depend on the parameters in a complex way (in particular, the distributions are not free from the parameters, which is important in constructing confidence intervals).
 </p>
           
 <p class="text-justify">
But suppose $n$ is large. Then the sample variance is close to population variance and hence $s_{n}\approx {\sigma}
$. Further, by CLT, we know that $\sqrt{n}(\bar{X}_{n}-\mu)/{\sigma}$ has approximately    $N(0,1)$ distribution. Hence, we see that
$$
\mathbf{P}\left\{-z_{\alpha/2}\le  \frac{\sqrt{n}(\bar{X}_{n}-\mu)}{s_{n} } \le z_{\alpha/2}\right\} \approx \Phi(z_{\alpha/2})-\Phi(-z_{\alpha/2}) =1-\alpha.
$$
Consequently, we may say that
$$
\mathbf{P}\left\{ \bar{X}_{n}-\frac{s_{n} }{\sqrt{n} }z_{\alpha/2} \le \mu \le \bar{X}_{n}+\frac{s_{n} }{\sqrt{n} }z_{\alpha/2}\right\} \approx 1-\alpha.
$$
Thus, $\left[\bar{X}_{n}-\frac{s_{n} }{\sqrt{n} }z_{\alpha/2}, \bar{X}_{n}+\frac{s_{n} }{\sqrt{n} }z_{\alpha/2} \right]$ is an approximate $(1-\alpha)$-confidence interval. Further, when $n$ is large, the difference between $V_{n}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}$ and $V_{n}:=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}$ is small (indeed, $s_{n}^{2}=(n/(n-1))V_{n}$). Hence it is also okay to use $\left[\bar{X}_{n}-\frac{\sqrt{V_{n} }}{\sqrt{n} }z_{\alpha/2}, \bar{X}_{n}+\frac{\sqrt{V_{n} }}{\sqrt{n} }z_{\alpha/2} \right]$ as an approximate $(1-\alpha)$-confidence interval.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-172"><div class="panel panel-info example"> <div class="panel-heading">Example 172 </div><div class="panel-body"> Let $X_{1},\ldots ,X_{n}$ be i.i.d. $\mbox{Ber}(p)$. Consider the problem of finding a confidence interval for $p$. Since each $X_{i}$ is $0$ or $1$, observe that
$$
\hat{s}_{n}^{2}= \frac{1}{n}\sum_{i=1}^{n}X_{i}^{2} - \bar{X}_{n}^{2} = \bar{X}_{n}-(\bar{X}_{n})^{2} = \bar{X}_{n}(1-\bar{X}_{n}).
$$
</div></div></div>
Hence, an approximate $(1-\alpha)$-CI for $p$ is given by
$$
\left[\bar{X}_{n}-z_{\alpha/2}\sqrt{\frac{\bar{X}_{n}(1-\bar{X}_{n})}{n} }, \bar{X}_{n}+z_{\alpha/2}\sqrt{\frac{\bar{X}_{n}(1-\bar{X}_{n})}{n} }\right].
$$
 </p>
           

<div class="pull-right"><a href="chapter-33.html" class="btn btn-primary">Chapter 33. Actual confidence by simulation</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="../js/bootstrap.min.js"`></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    