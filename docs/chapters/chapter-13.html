
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">

<style type="text/css">
   body { padding-top: 60px; }
   .section {padding-top: 60px;}
   #arxiv {
     border-style: solid;
     border-width: 1px;
   }
</style>


  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <span class="navbar-brand">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav navbar-right">
          <li><a href="index.html">Table of Contents</a></li>
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probabibility (part 1) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Probability (part 2) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
            </ul>
          </li>
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">
<h1 class="text-center bg-info">Chapter 13 : Discrete probability distributions</h1>
<p>&nbsp;</p>











 <p class="text-justify">

Let $(\Omega,p)$ be a probability space and $X:\Omega\rightarrow \mathbb{R}$ be a random variable. We define two objects associated to $X$.
 </p>
<p class="text-justify">
          
 <strong>Probability mass function (pmf)</strong>. The range of $X$ is a countable subset of $\mathbb{R}$, denote it by $\mbox{Range}(X)=\{t_{1},t_{2},\ldots\}$. Then, define $f_{X}:\mathbb{R}\rightarrow [0,1]$ as the function
$$
f_{X}(t)=\begin{cases}
\mathbf{P}\{\omega {\; : \;} X(\omega)=t\} & \mbox{ if }t\in \mbox{Range}(X). \\
0 & \mbox{ if }t\not\in \mbox{Range}(X).
\end{cases}
$$
One obvious property is that $\sum_{t\in \mathbb{R}}f_{X}(t)=1$. Conversely, any non-negative function $f$ that is non-zero on a countable set $S$ and such that $\sum_{t\in \mathbb{R}} f(t)=1$ is a pmf of some random variable.
 </p>
           
 <p class="text-justify">
<strong>Cumulative distribution function (CDF)</strong>. Define $F_{X}:\mathbb{R}\rightarrow [0,1]$ by
$$F_{X}(t)=\mathbf{P}\{\omega{\; : \;} X(\omega)\le t\}.$$
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-74"><div class="panel panel-info example"> <div class="panel-heading">Example 74 </div><div class="panel-body"> Let $\Omega=\{(i,j){\; : \;} 1\le i,j\le 6\}$ with $p_{(i,j)}=\frac{1}{36}$ for all $(i,j)\in \Omega$. Let $X:\Omega \rightarrow \mathbb{R}$ be the random variable defined by $X(i,j)=i+j$. Then, $\mbox{Range}(X)=\{2,3,\ldots ,12\}$. The pmf and CDF of $X$ are given by
$$
f_{X}(k) = \begin{cases}
1/36 & \mbox{ if }k=2. \\
2/36 & \mbox{ if }k=3. \\
3/36 & \mbox{ if }k=4. \\
4/36 & \mbox{ if }k=5. \\
5/36 & \mbox{ if }k=6. \\
6/36 & \mbox{ if }k=7. \\
5/36 & \mbox{ if }k=8. \\
4/36 & \mbox{ if }k=9. \\
3/36 & \mbox{ if }k=10. \\
2/36 & \mbox{ if }k=11. \\
1/36 & \mbox{ if }k=12. \\
\end{cases}
\qquad
F_{X}(t) = \begin{cases}
0 & \mbox{ if }t < 2. \\
1/36 & \mbox{ if }t\in[2,3). \\
3/36 & \mbox{ if }t\in[3,4). \\
6/36 & \mbox{ if }t\in[4,5). \\
10/36 & \mbox{ if }t\in[5,6). \\
15/36 & \mbox{ if }t\in[6,7). \\
21/36 & \mbox{ if }t\in[7,8). \\
26/36 & \mbox{ if }t\in[8,9). \\
30/36 & \mbox{ if }t\in[9,10). \\
33/36 & \mbox{ if }t\in[10,11). \\
35/36 & \mbox{ if }t\in[11,12). \\
1 & \mbox{ if }t\ge 12. \\
\end{cases}
$$
</div></div></div>
 </p>
           
 <p class="text-justify">
A picture of the pmf and CDF for a Binomial distribution are shown in Figure ref{fig:pdfandcdfofbinomial}.
 </p>
<p class="text-justify">
          
 <strong>Basic properties of a CDF :</strong> The following observations are easy to make.
<ol>
<li> $F$ is an increasing function on $\mathbb{R}$.
</li>
<li> $\lim\limits_{t\rightarrow +\infty}F(t)=1$ and $\lim\limits_{t\rightarrow -\infty}F(t)=0$.
</li>
<li> $F$ is right continuous, that is, $\lim\limits_{h\searrow 0}F(t+h)=F(t)$ for all $t\in \mathbb{R}$.
</li>
<li> $F$ increases only in jumps. This means that if $F$ has no jump discontinuities (an increasing function has no other kind of discontinuity anyway) in an interval $[a,b]$, then $F(a)=F(b)$.
</ol>
Since $F(t)$ is the probability of a certain event, these statements can be proved using the basic rules of probability that we saw earlier.
 </p>
           
 <p class="text-justify">
<div class="proof"> Let $t < s$. Define two events, $A=\{\omega {\; : \;} X(\omega)\le t\}$ and $B=\{\omega{\; : \;} X(\omega)\le s\}$. Clearly $A\subseteq B$ and hence $F(t)=\mathbf{P}(A)\le \mathbf{P}(B)=F(s)$. This proves the first property.
 </p>
<p class="text-justify">
          
 To prove the second property, let $A_{n}=\{\omega {\; : \;} X(\omega)\le n\}$ for $n\ge 1$. Then, $A_{n}$ are increasing in $n$ and $\bigcup_{n=1}^{\infty}A_{n}=\Omega$. Hence, $F(n)=\mathbf{P}(A_{n})\rightarrow \mathbf{P}(\Omega)=1$ as $n\rightarrow \infty$. Since $F$ is increasing, it follows that $\lim_{t\rightarrow +\infty}F(t)=1$. Similarly one can prove that $\lim_{t\rightarrow -\infty}F(t)=0$.
 </p>
           
 <p class="text-justify">
Right continuity of $F$ is also proved the same way, by considering the events $B_{n}=\{\omega {\; : \;} X(\omega)\le t+\frac{1}{n}\}$. We omit details.
</div>
<p>&nbsp;</p>
                <div id="theorem-75"><div class="panel panel-success remark"> <div class="panel-heading">Remark 75 </div><div class="panel-body">
It is easy to see that one can recover the pmf from the CDF and vice versa. For example, given the pmf $f$, we can write the CDF as $F(t)=\sum_{u:u\le t}f(u)$. Conversely, given the CDF, by looking at the locations of the jumps and the sizes of the jumps, we can recover the pmf.
</div></div></div>
 </p>
<p class="text-justify">
          
 The point is that probabilistic questions about $X$ can be answered by knowing its CDF $F_{X}$. Therefore, in a sense, the probability space becomes irrelevant. For example, the expected value of a random variable can be computed using its CDF only. Hence, we shall often make statements like ``$X$ is a random variable with pmf $f$'' or ``$X$ is a random variable with CDF $F$'', without bothering to indicate the probability space.
 </p>
           
 <p class="text-justify">
Some distributions (i.e., CDF or the associated pmf) occur frequently enough to merit a name.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-76"><div class="panel panel-info example"> <div class="panel-heading">Example 76 </div><div class="panel-body"> Let $f$ and $F$ be the pmf, CDF pair
$$
f(t)=\begin{cases}p & \mbox{ if }t=1, \\ q & \mbox{ if }t=0, \end{cases} \qquad F_{X}(t)=\begin{cases} 1 &\mbox{ if } t\ge 1, \\ q & \mbox{ if }t\in [0,1), \\ 0 & \mbox{ if }t <  0. \end{cases}
$$
A random variable $X$ having this pmf (or equivalently the CDF) is said to have <em>Bernoulli distribution</em> with parameter $p$ and  write $X\sim \mbox{Ber}(p)$. For example, if $\Omega=\{1,2,\ldots ,10\}$ with $p_{i}=1/10$, and $X(\omega)={\mathbf 1}_{\omega\le 3}$, then $X\sim \mbox{Ber}(0.3)$. Any random variable taking only the values $0$ and $1$, has Bernoulli distribution.
</div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-77"><div class="panel panel-info example"> <div class="panel-heading">Example 77 </div><div class="panel-body">  Fix $n\ge 1$ and $p\in [0,1]$. The pmf defined by $f(k)=\binom{n}{k}p^{k}q^{n-k}$ for $0\le k\le n$ is called the <em>Binomial  distribution</em> with parameters $n$ and $p$ and is denoted Bin($n,p$). The CDF is as usual defined by $F(t)=\sum_{u:\mathbf{u}\le t}f(u)$, but it does not have any particularly nice expression.
 </p>
<p class="text-justify">
          
 For example, if $\Omega=\{0,1\}^{n}$ with $p_{\underline{\omega}}=p^{\sum_{i}\omega_{i} }q^{n-\sum_{i}\omega_{i} }$, and $X(\underline{\omega})=\omega_{1}+\ldots +\omega_{n}$, then $X\sim \mbox{Bin}(n,p)$. In words, the number of heads in $n$ tosses of a $p$-coin has $\mbox{Bin}(n,p)$ distribution.
</div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-78"><div class="panel panel-info example"> <div class="panel-heading">Example 78 </div><div class="panel-body"> Fix $p\in (0,1]$ and let $f(k)=q^{k-1}p$ for $k\in \mathbb{N}_{+}$. This is called the <em>Geometric  distribution</em> with parameter $p$ and is denoted Geo($p$). The CDF is
$$
F(t) = \begin{cases}
0 & \mbox{ if } t < 1, \\
1-q^{k} & \mbox{ if } k\le t < k+1, \mbox{ for some }k\ge 1.
\end{cases}
$$
For example, the number of tosses of a $p$-coin till the first head turns up, is a random variable with $\mbox{Geo}(p)$ distribution.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-79"><div class="panel panel-info example"> <div class="panel-heading">Example 79 </div><div class="panel-body"> Fix $\lambda > 0$ and define the pmf $f(k)=e^{-\lambda}\frac{\lambda^{k} }{k!}$. This is called the <em>Poisson  distribution</em> with parameter $\lambda$ and is denoted Pois($\lambda$).
 </p>
           
 <p class="text-justify">
In the problem of a psychic (randomly) guessing the cards in a deck, we have seen that the number of matches (correct guesses) had an <em>approximately</em> Pois($1$) distribution.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-80"><div class="panel panel-info example"> <div class="panel-heading">Example 80 </div><div class="panel-body"> Fix positive integers $b,w$ and $m\le b+w$. Define the pmf  $f(k)=\frac{\binom{b}{k}\binom{w}{m-k} }{\binom{b+w}{m} }$ where the binomial coefficient $\binom{x}{y}$ is interpreted to be zero if $y > x$ (thus $f(k) > 0$ only for $\max\{m-w,0\}\le k\le b$). This is called the <em>Hypergeometric distribution</em> with parameters $b,w,m$ and we shall denote it by  Hypergeo($b,w,m$).
 </p>
           
 <p class="text-justify">
Consider a population with $b$ men and $w$ women. The number of men in a random sample (without replacement) of size $m$, is a random variable with the Hypergeo($b,w,m$) distribution.
</div></div></div>
 </p>
<p class="text-justify">
          
 <strong>Computing expectations from the pmf</strong> Let $X$ be a random variable on $(\Omega,p)$ with pmf $f$. Then we claim that
$$
\mathbf{E}[X] = \sum_{t\in \mathbb{R}}tf(t).
$$
Indeed, let $\mbox{Range}(X)=\{x_{1},x_{2},\ldots\}$. Let $A_{k}=\{\omega{\; : \;} X(\omega)=x_{k}\}$. By definition of pmf we have $\mathbf{P}(A_{k})=f(x_{k})$. Further, $A_{k}$ are pairwise disjoint and exhaustive. Hence
$$
\mathbf{E}[X] = \sum_{\omega\in \Omega}X(\omega)p_{\omega} = \sum_{k}\sum_{\omega\in A_{k} }X(\omega)p_{\omega} = \sum_{k}x_{k}\mathbf{P}(A_{k})=\sum_{k}x_{k}f(x_{k}).
$$
Similarly, $\mathbf{E}[X^{2}]=\sum_{k}x_{k}^{2}f(x_{k})$. More generally, if $h:\mathbb{R}\rightarrow \mathbb{R}$ is any function, then the random variable $h(X)$ has expectation $\mathbf{E}[h(X)]=\sum_{k}h(x_{k})f(x_{k})$. Although this sounds trivial, there is a very useful point here. To calculate $\mathbf{E}[X^{2}]$ we do not have to compute the pmf of $X^{2}$ first, which can be done but would be more complicated. Instead, in the above formulas, $\mathbf{E}[h(X)]$ has been computed directly in terms of the pmf of $X$.
<p>&nbsp;</p>
                <div id="theorem-81"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 81 </div><div class="panel-body"> Find $\mathbf{E}[X]$ and $\mathbf{E}[X^{2}]$ in each case.
<ol>
<li> $X\sim \mbox{Bin}(n,p)$.
</li>
<li> $X\sim \mbox{Geo}(p)$.
</li>
<li> $X\sim \mbox{Pois}(\lambda)$.
</li>
<li> $X\sim \mbox{Hypergeo}(b,w,m)$.
</ol>
</div></div></div>
 </p>
           

<div class="pull-right"><a href="chapter-14.html" class="btn btn-primary">Chapter 14. General probability distributions</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    