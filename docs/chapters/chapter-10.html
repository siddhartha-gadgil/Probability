
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">

<style type="text/css">
   body { padding-top: 60px; }
   .section {padding-top: 60px;}
   #arxiv {
     border-style: solid;
     border-width: 1px;
   }
</style>


  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <span class="navbar-brand">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav navbar-right">
          <li><a href="index.html">Table of Contents</a></li>
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probabibility (part 1) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Probability (part 2) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
            </ul>
          </li>
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">
<h1 class="text-center bg-info">Chapter 10 : Conditional probability and independence</h1>
<p>&nbsp;</p>









 <p class="text-justify">

<p>&nbsp;</p>
                <div id="theorem-63"><div class="panel panel-primary definition"> <div class="panel-heading">Definition 63 </div><div class="panel-body"> Let $A,B$ be two events in the same probability space.
<ol>
<li> If $\mathbf{P}(B)\not=0$, we define the <em>conditional probability of $A$ given $B$</em> as $$\mathbf{P}\left(A\left.\vphantom{\hbox{\Large (}}\right| B\right):=\frac{\mathbf{P}(A\cap B)}{\mathbf{P}(B)}.$$
</li>
<li> We say that $A$ and $B$ are <em>independent</em> if $\mathbf{P}(A\cap B)=\mathbf{P}(A)\mathbf{P}(B)$. If $\mathbf{P}(B)\not=0$, then $A$ and $B$ are independent if and only if $\mathbf{P}(A\ \pmb{\big|} \  B)=\mathbf{P}(A)$ (and similarly with the roles of $A$ and $B$ reversed). If $\mathbf{P}(B)=0$, then $A$ and $B$ are necessarily independent since $\mathbf{P}(A\cap B)$ must also be $0$.
</ol>
</div></div></div>
What do these notions mean intuitively? In real life, we keep updating probabilities based on information that we get. For example, when playing cards, the chance that a randomly chosen card is an ace is $1/13$, but having drawn a card, the probability for the next card may not be the same - if the first card was seen to be an ace, then the chance of the second being an ace falls to $3/51$. This updated probability is called a conditional probability. Independence of two events $A$ and $B$ means that knowing whether or not $A$ occured does not change the chance of occurrence of $B$. In other words, the conditional probability of $A$ given $B$ is the same as the unconditional (original) probability of $A$.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-64"><div class="panel panel-info example"> <div class="panel-heading">Example 64 </div><div class="panel-body"> Let $\Omega=\{(i,j){\; : \;} 1\le i,j\le 6\}$ with $p_{(i,j)}=\frac{1}{36}$. This is the probability space corresponding to a throw of two fair dice. Let $A=\{(i,j){\; : \;} i\mbox{ is odd}\}$ and $B=\{(i,j){\; : \;} j \mbox{ is }1\mbox{ or }6\}$ and $C=\{(i,j){\; : \;} i+j=4\}$. Then $A\cap B=\{(i,j){\; : \;} i=1,3,\mbox{ or }5, \mbox{ and }j=1\mbox{ or }6\}$. Then, it is easy to see that
$$
\mathbf{P}(A\cap B)=\frac{6}{36}=\frac{1}{6}, \;\; \mathbf{P}(A)=\frac{18}{36}=\frac{1}{2}, \;\; \mathbf{P}(B)=\frac{12}{36}=\frac{1}{3}.
$$
In this case, $\mathbf{P}(A\cap B)=\mathbf{P}(A)\mathbf{P}(B)$ and hence $A$ and $B$ are independent. On the other hand,
$$
\mathbf{P}(A\cap C)=\mathbf{P}\{(1,3),(2,2)\}=\frac{1}{18}, \;\; \mathbf{P}(C)=\mathbf{P}\{(1,3),(2,2),(3,1)\}=\frac{1}{12}.
$$
Thus, $\mathbf{P}(A\cap C)\not=\mathbf{P}(A)\mathbf{P}(C)$ and hence $A$ and $C$ are not independent.
 </p>
           
 <p class="text-justify">
This agrees with the intuitive understanding of independence, since $A$ is an event that depends only on the first toss and $B$ is an event that depends only on the second toss. Therefore, $A$ and $B$ ought to be independent. However, $C$ depends on both tosses, and hence cannot be expected to be independent of $A$. Indeed, it is easy to see that $\mathbf{P}(C \ \pmb{\big|} \  A)=\frac{1}{9}$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-65"><div class="panel panel-info example"> <div class="panel-heading">Example 65 </div><div class="panel-body"> Let $\Omega=S_{52}$ with $p_{\pi}=\frac{1}{52!}$. Define the events
$$
A=\{\pi{\; : \;} \pi_{1}\in \{10,20,30,40\}\}, \qquad A=\{\pi{\; : \;} \pi_{2}\in \{10,20,30,40\}\}.
$$
Then both $\mathbf{P}(A)=\mathbf{P}(B)=\frac{1}{13}$. However, $\mathbf{P}(B\ \pmb{\big|} \  A)=\frac{3}{51}$. One can also see that $\mathbf{P}(B \ \pmb{\big|} \  A^{c})=\frac{4}{51}$.
 </p>
           
 <p class="text-justify">
In words, $A$ (respectively $B$) could be the event that the first (respectively second) card is an ace. Then $\mathbf{P}(B)=4/52$ to start with. When we see the first card, we update the probability. If the first card was not an ace, we update it to $\mathbf{P}(B\ \pmb{\big|} \  A^{c})$ and if the first card was an ace, we update it to  $\mathbf{P}(B\ \pmb{\big|} \  A)$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <strong>Caution :</strong> Independence should not be confused with disjointness! If $A$ and $B$ are disjoint, $\mathbf{P}(A\cap B)=0$ and hence $A$ and $B$ can be independent if and only if one of $\mathbf{P}(A)$ or $\mathbf{P}(B)$ equals $0$. Intuitively, if $A$ and $B$ are disjoint, then knowing that $A$ occurred gives us a lot of information about $B$ (that it did not occur!), so independence is not to be expected.
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-66"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 66 </div><div class="panel-body"> If $A$ and $B$ are independent, show that the following pairs of events are also independent.
<ol>
<li> $A$ and $B^{c}$.
</li>
<li> $A^{c}$ and $B$.
</li>
<li> $A^{c}$ and $B^{c}$.
</ol>
</div></div></div>
 </p>
<p class="text-justify">
          
 <strong>Total probability rule and Bayes' rule :</strong> Let $A_{1},\ldots ,A_{n}$ be pairwise disjoint and mutually exhaustive events in a probability space. Assume $\mathbf{P}(A_{i}) > 0$ for all $i$. This means that $A_{i}\cap A_{j}=\emptyset$ for any $i\not= j$ and $A_{1}\cup A_{2}\cup \ldots \cup A_{n}=\Omega$. We also refer to such a collection of events as a partition of the sample space.
 </p>
           
 <p class="text-justify">
Let $B$ be any other event.
<ol>
<li> (Total probability rule). $\mathbf{P}(B)=\mathbf{P}(A_{1})\mathbf{P}(B\ \pmb{\big|} \  A_{1})+\ldots +\mathbf{P}(A_{n})\mathbf{P}(B\ \pmb{\big|} \  A_{n})$.
</li>
<li> (Bayes' rule).  Assume that $\mathbf{P}(B) > 0$. Then, for each $k=1,2,\ldots ,n$, we have
$$\mathbf{P}(A_{k}\ \pmb{\big|} \  B)=\frac{\mathbf{P}(A_{k})\mathbf{P}(B\ \pmb{\big|} \  A_{k})}{\mathbf{P}(A_{1})\mathbf{P}(B\ \pmb{\big|} \  A_{1})+\ldots +\mathbf{P}(A_{n})\mathbf{P}(B\ \pmb{\big|} \  A_{n})}.$$
</ol>
<div class="proof"> The proof is merely by following the definition.
<ol>
<li> The right hand side is equal to
$$
\mathbf{P}(A_{1})\frac{\mathbf{P}(B\cap A_{1})}{\mathbf{P}(A_{1})}+\ldots +\mathbf{P}(A_{n})\frac{\mathbf{P}(B\cap A_{n})}{\mathbf{P}(A_{n})}=\mathbf{P}(B\cap A_{1})+\ldots +\mathbf{P}(B\cap A_{n})
$$
which is equal to $\mathbf{P}(B)$ since $A_{i}$ are pairwise disjoint and exhaustive.
</li>
<li> Without loss of generality take $k=1$. Note that $\mathbf{P}(A_{1}\cap B)=\mathbf{P}(A_{1})\mathbf{P}(B\cap A_{1})$. Hence
$$\begin{align*}
\mathbf{P}(A_{1}\ \pmb{\big|} \  B) &= \frac{\mathbf{P}(A_{1}\cap B)}{\mathbf{P}(B)} \\
 &= \frac{\mathbf{P}(A_{1})\mathbf{P}(B\ \pmb{\big|} \  A_{1})}{\mathbf{P}(A_{1})\mathbf{P}(B\ \pmb{\big|} \  A_{1})+\ldots +\mathbf{P}(A_{n})\mathbf{P}(B\ \pmb{\big|} \  A_{n})}
\end{align*}$$
where we used the total probability rule to get the denominator. \qedhere
</ol>
</div>
<p>&nbsp;</p>
                <div id="theorem-67"><div class="panel panel-warning exercise"> <div class="panel-heading">Exercise 67 </div><div class="panel-body"> Suppose $A_{i}$ are events such that $\mathbf{P}(A_{1}\cap \ldots \cap A_{n}) > 0$. Then show that $$\mathbf{P}(A_{1}\cap \ldots \cap A_{n})=\mathbf{P}(A_{1})\mathbf{P}(A_{2}\ \pmb{\big|} \  A_{1})\mathbf{P}(A_{3}\ \pmb{\big|} \  A_{1}\cap A_{2})\ldots \mathbf{P}(A_{n}\ \pmb{\big|} \  A_{1}\cap \ldots \cap A_{n-1}).$$
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-68"><div class="panel panel-info example"> <div class="panel-heading">Example 68 </div><div class="panel-body"> Consider a rare disease $X$ that affects one in a million people. A medical test is used to test for the presence of the disease. The test is 99\% accurate in the sense that if a person has no disease, the chance that the test shows positive is 1\% and if the person has disease, the chance that the test shows negative is also 1\%.
 </p>
           
 <p class="text-justify">
Suppose a person is tested for the disease and the test result is positive. What is the chance that the person has the disease $X$?
 </p>
<p class="text-justify">
          
 Let $A$ be the event that the person has the disease $X$. Let $B$ be the event that the test shows positive. The given data may be summarized as follows.
<ol>
<li> $\mathbf{P}(A)=10^{-6}$. Of course $\mathbf{P}(A^{c})=1-10^{-6}$.
</li>
<li> $\mathbf{P}(B \ \pmb{\big|} \  A)=0.99$ and $\mathbf{P}(B\ \pmb{\big|} \  A^{c})=0.01$.
</ol>
What we want to find is $\mathbf{P}(A\ \pmb{\big|} \  B)$. By Bayes' rule (the relevant partition is $A_{1}=A$ and $A_{2}=A^{c}$),
$$
\mathbf{P}(A\ \pmb{\big|} \  B) = \frac{\mathbf{P}(B\ \pmb{\big|} \  A)\mathbf{P}(A)}{\mathbf{P}(B\ \pmb{\big|} \  A)\mathbf{P}(A)+\mathbf{P}(B\ \pmb{\big|} \  A^{c})\mathbf{P}(A^{c})} = \frac{0.99 \times 10^{-6} }{0.99\times 10^{-6}+0.01\times (1-10^{-6})} = 0.000099.
$$
The test is quite an accurate one, but the person tested positive has a really low chance of actually having the disease! Of course, one should observe that the chance of having disease is now approximately $10^{-4}$ which is considerably higher than $10^{-6}$.
 </p>
           
 <p class="text-justify">
A calculation-free understanding of this surprising looking phenomenon can be achieved as follows: Let everyone in the population undergo the test. If there are $10^{9}$ people in the population, then there are only $10^{3}$ people with the disease. The number of true positives is approximately $10^{3}\times 0.99\approx 10^{3}$ while the number of false positives is $(10^{9}-10^{3})\times 0.01\approx 10^{7}$. In other words, among all positives, the false positives are way more numerous than true positives.
</div></div></div>
The surprise here comes from not taking into account the relative sizes of the sub-populations with and without the disease. Here is another manifestation of exactly the same fallacious reasoning.
 </p>
<p class="text-justify">
          
 <strong>Question :</strong> A person $X$ is introverted,  very systematic in thinking and somewhat absent-minded. You are told that he is a doctor or a mathematician. What would be your guess - doctor or mathematician?
 </p>
           
 <p class="text-justify">
As we saw in class, most people answer ``mathematician''. Even accepting the stereotype that a mathematician is more likely to have all these qualities than a doctor, this answer ignores the fact that there are perhaps a hundred times more doctors in the world than mathematicians! In fact, the situation is identical to the one in the example above, and the mistake is in confusing $\mathbf{P}(A\big| B)$ and $\mathbf{P}(B\big| A)$.
 </p>
<p class="text-justify">
          
 
 </p>
           

<div class="pull-right"><a href="chapter-11.html" class="btn btn-primary">Chapter 11. Independence of three or more events</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script type="text/javascript" src="../js/jquery-2.1.4.min.js"></script>
 <script type="text/javascript" src='../js/bootstrap.min.js'></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    