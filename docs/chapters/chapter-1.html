
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">

<style type="text/css">
   body { padding-top: 60px; }
   .section {padding-top: 60px;}
   #arxiv {
     border-style: solid;
     border-width: 1px;
   }
</style>


  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <span class="navbar-brand">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav navbar-right">
          <li><a href="index.html">Table of Contents</a></li>
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probabibility (part 1) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Probability (part 2) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
            </ul>
          </li>
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">
<h1 class="text-center bg-info">Chapter 1 : What is statistics and what is probability?</h1>
<p>&nbsp;</p>








 <p class="text-justify">

Sometimes statistics is described as <em> the art or science of decision making in the face of uncertainty</em>.  Here are some examples to illustrate what it means.
<p>&nbsp;</p>
                <div id="theorem-1"><div class="panel panel-info example"> <div class="panel-heading">Example 1 </div><div class="panel-body"> Recall the apocryphal story of two women who go to King Solomon with a child, each claiming that it is her own daughter. The solution according to the story uses human psychology and is not relevant to recall here. But is this a reasonable question that the king can decide?
 </p>
 <p>
 Daughters resemble mothers to varying degrees, and one cannot be absolutely sure of guessing correctly.
 On the other hand, by comparing various features of the child with those of the two women, there is certainly a decent chance to guess correctly.
 </p>
           
 <p class="text-justify">
If we could always get the right answer, or if we could never get it right, the question would not have been interesting. However, here we have uncertainty, but there is a decent chance of getting the right answer. That makes it interesting - for example, we can have a debate between <em> eyeists</em>  and <em> nosists</em> as to whether it is better to compare the eyes or the noses in arriving at a decision.
</div></div></div>
 </p>
 <p>
 <p>&nbsp;</p>
                <div id="theorem-2"><div class="panel panel-info example"> <div class="panel-heading">Example 2 </div><div class="panel-body"> The IISc cricket team meets the Basavanagudi cricket club for a match. Unfortunately, the Basavanagudi team forgot to bring a coin to toss. The IISc captain helpfully offers his coin, but can he be trusted? What if he spent the previous night doctoring the coin so that it falls on one side with probability $3/4$ (or some other number)?
 </p>
           
 <p class="text-justify">
Instead of cricket, they could spend their time on the more interesting question of checking if the coin is <em> fair</em> or <em> biased</em>. Here is one way. If the coin is fair, in a large number of tosses, common sense suggests that we should get about equal number of heads and tails. So they  toss the coin 100 times. If the number of heads is exactly 50, perhaps they will agree that it is fair. If the number of heads is 90, perhaps they will agree that it is biased. What if the number of heads is 60? Or 35? Where and on what basis to draw the line between fair and biased? Again we are faced with the question of making decision in the face of uncertainty.
</div></div></div>
 </p>
 <p>
 <p>&nbsp;</p>
                <div id="theorem-3"><div class="panel panel-info example"> <div class="panel-heading">Example 3 </div><div class="panel-body"> A psychic claims to have divine visions unavailable to most of us. You are assigned the task of testing her claims. You take a standard deck of cards, shuffle it well and keep it face down on the table. The psychic writes down the list of cards in some order - whatever her vision tells her about how the deck is ordered. Then you count the number of correct guesses. If the number is 1 or 2, perhaps you can dismiss her claims. If it is 45, perhaps you ought to be take her seriously. Again, where to draw the line?
 </p>
           
 <p class="text-justify">
The logic is this. Roughly one may say that <em> surprise</em> is just the name for our reaction to an event that we <em> \'{a</em> priori} thought to have low chance of occurring. Thus, we approach the experiment with the belief that the psychic is just guessing at random, and if the results are such that under that random-guess-hypothesis they have very small probability, then we are willing to be surprised, that is willing to discard our preconception and accept that she is a psychic.
 </p>
 <p>
 How low a probability is surprising? In the context of psychics, let us say, $1/10000$. Once we fix that, we must find a number $m\le 52$ such that by pure guessing, the probability to get more than $m$ correct guesses is less that $1/10000$. Then we tell the psychic that if she gets more than $m$ correct guesses, we accept her claim, and otherwise, reject her claim.  This raises the simple (and you can do it yourself)
<p>&nbsp;</p>
                <div id="theorem-4"><div class="panel panel-danger question"> <div class="panel-heading">Question 4 </div><div class="panel-body"> For a deck of $52$ cards, find the number $m$ such that
$$
\mathbf{P}(\mbox{by random guessing we get more than }m\mbox{ correct guesses}) < \frac{1}{10000}.
$$
</div></div></div>
</div></div></div>
 </p>
           
 <p class="text-justify">
<strong>Summary:</strong> There are many situations in real life where one is required to make decisions under uncertainty. A general template for the answer could be to fix a small number that we allow as the probability of error, and deduce thresholds based on it. This brings us to the question of computing probabilities in various situations.
 </p>
 <p>
 <strong>Probability:</strong> Probability theory is a branch of pure mathematics, and forms the theoretical basis of statistics. In itself, probability theory has some basic objects and their relations (like real numbers, addition etc for analysis) and it makes no pretense of saying anything about the real world. Axioms are given and theorems are then deduced about these objects, just as in any other part of mathematics.
 </p>
           
 <p class="text-justify">
But a very important aspect of probability is that it is <em> applicable</em>. In other words, there are many real-world situations in which it is reasonable to take a model in probability and it turns out to reasonably replicate features of  the real-world situation.
 </p>
 <p>
 In the example above, to compute the probability one must make the assumption that the deck of cards was completely shuffled. In other words, all possible 52! orders of the 52 cards are assumed to be equally likely. Whether this assumption is reasonable or not depends on how well the card was shuffled, whether the psychic was able to get a peek at the cards, whether some insider is informing the psychic of the cards etc. All these are non-mathematical questions, and must be decided on other basis.
 </p>
           
 <p class="text-justify">
<strong>However...:</strong> Probability and statistics are very relevant in many situations that do not involve any uncertainty on the face of it. Here are some examples.
<p>&nbsp;</p>
                <div id="theorem-5"><div class="panel panel-info example"> <div class="panel-heading">Example 5 </div><div class="panel-body"> <em> Compression of data</em>. Large files in a computer can be compressed to a .zip format and uncompressed when necessary. How is it possible to compress data like this? To give a very simple analogy, consider a long English word like <em> invertebrate</em>. If we take a novel and replace every occurrence of this word with ``zqz'', then  it is certainly possible to recover the original novel (since ``zqz'' does not occur anywhere else). But the reduction in size by replacing the 12-letter word by the 3-letter word is not much, since the word <em> invertebrate</em> does not occur often. Instead, if we replace the 4-letter word ``then'' by ``zqz'', then the total reduction obtained may be much higher, as the word ``then'' occurs quite often.
 </p>
 <p>
 This suggests the following optimal way to represent words in English. The 26 most frequent words will be represented by single letters. The next $26\times 26$ most frequent words will be represented by two letter words, the next $26\times 26\times 26$ most frequent words by three-letter words, etc. Assuming there are no errors in transcription, this is a good way to reduce the size of any text document! Now, this involves knowing what the frequencies of occurrences of various words in actual texts are. Such statistics of usage of words are therefore clearly relevant (and they could be different for biology textbooks as compared to 19th century novels).
</div></div></div>
<p>&nbsp;</p>
                <div id="theorem-6"><div class="panel panel-info example"> <div class="panel-heading">Example 6 </div><div class="panel-body"> Search algorithms such as Google, use many randomized procedures. This cannot be explained right now, but let us give a simple reason to say why introducing randomness is a good idea in many situations. In the game of <em> rock-paper-scissors</em>, two people simultaneously shout one of the three words, rock, paper or scissors. The rule is that scissors beats paper, paper beats rock and rock beats scissors (if they both call the same word, they must repeat). In a game like this, although there is complete symmetry in the three items, it would be silly to have a fixed strategy. In other words, if you decide to always say rock, thinking that it doesn't matter which you choose, then your opponent can use that knowledge to always choose paper and thus win! In many games where the opponent gets to know your strategy (but not your move), the best strategy would involve  randomly choosing your move.
</div></div></div>
 </p>
           

<div class="pull-right"><a href="chapter-2.html" class="btn btn-primary">Chapter 2. Discrete probability spaces</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script type="text/javascript" src="../js/jquery-2.1.4.min.js"></script>
 <script type="text/javascript" src='../js/bootstrap.min.js'></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    