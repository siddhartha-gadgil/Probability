
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">

<style type="text/css">
   body { padding-top: 60px; }
   .section {padding-top: 60px;}
   #arxiv {
     border-style: solid;
     border-width: 1px;
   }
</style>


  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <span class="navbar-brand">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav navbar-right">
          <li><a href="index.html">Table of Contents</a></li>
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probabibility (part 1) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Probability (part 2) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
            </ul>
          </li>
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">
<h1 class="text-center bg-info">Chapter 35 : Testing for the mean of a normal population</h1>
<p>&nbsp;</p>






 <p class="text-justify">

 Let $X_{1},\ldots ,X_{n}$ be i.i.d. $N(\mu,{\sigma}^{2})$. We shall consider the following  hypothesis testing problems.
 </p>
<p class="text-justify">
          
 <ol>
<li> One sided test for the mean. $H_{0}:\; \mu=\mu_{0}$ versus $H_{1}: \; \mu > \mu_{0}$.
</li>
<li> Two sided test for the mean.  $H_{0}:\; \mu=\mu_{0}$ versus $H_{1}: \; \mu\not=\mu_{0}$.
</ol>
 </p>
           
 <p class="text-justify">
This kind of problem arises in many situations in comparing the effect of a treatment as follows.
<p>&nbsp;</p>
                <div id="theorem-175"><div class="panel panel-info example"> <div class="panel-heading">Example 175 </div><div class="panel-body"> Consider a drug claimed to reduce blood pressure. How do we check if it actually does? We take a random sample of $n$ patients, measure their blood pressures $Y_{1},\ldots ,Y_{n}$. We administer the drug to each of them and again measure the blood pressures $Y_{1}',\ldots ,Y_{n}'$, respectively.   Then, the question is whether the mean blood pressure decreases upon giving the treatment. To this effect, we define $X_{i}=Y_{i}-Y_{i}'$ and wish to test the hypothesis that the mean of $X_{i}$s is strictly positive. If $X_{i}$ are indeed normally distributed, this is exactly the one-sided test above.
</div></div></div>
<p>&nbsp;</p>
                <div id="theorem-176"><div class="panel panel-info example"> <div class="panel-heading">Example 176 </div><div class="panel-body"> The same applies to test the efficacy of a fertilizer to increase yield, a proposed drug to decrease weight, a particular educational method to improve a skill, or a particular course such as the current <em> probability and statistics course</em> in increasing subject knowledge. To make a policy decision on such  matters, we can conduct an experiment as in the above example.
 </p>
<p class="text-justify">
          
 For example, a bunch of students are tested on probability and statistics and their scores are noted. Then they are subjected to the course for a semester. They are tested again after the course (for the same marks, and at the same level of difficulty) and the scores are again noted. Take differences of the scores before and after, and test whether the mean of these differences is positive (or negative, depending on how you take the difference). This is a  one-sided tests for the mean. Note that in these examples, we are taking the null hypothesis to be that there is no effect. In other words, the burden of proof is on the new drug or fertilizer or the instructor of the course.
</div></div></div>
 </p>
           
 <p class="text-justify">
<strong>The tes :</strong> Now we present the test. We shall use the statistic $\mathcal T:=\frac{\sqrt{n}(\bar{X}-\mu_{0})}{s}$ where $\bar{X}$ and $s$ are the sample mean and sample standard deviation.
<ol>
<li> In the one-sided test, we accept the alternative hypothesis if $\mathcal T > t_{n-1}(\alpha)$.
</li>
<li> In the two sided-test, accept the alternative hypothesis if $\mathcal T > t_{n-1}(\alpha/2)$ or $\mathcal T < -t_{n-1}(\alpha/2)$.
</ol>
 </p>
<p class="text-justify">
          
 <strong>The rationale behind the test :</strong> If $\bar{X}$ is much larger than $\mu_{0}$ then the greater is the evidence that the true mean $\mu$ is greater than $\mu_{0}$. However, the magnitude depends on the standard deviation and hence we divide by $s$ (if we knew ${\sigma}$ we would divide by that). Another way to see that this is reasonable is that $\mathcal T$ does not depend on the units in which you measure $X_{i}$s (whether $X_{i}$ are measured in meters or centimeters, the value of $\mathcal T$ does not change).
 </p>
           
 <p class="text-justify">
<strong>The significance level is $\alpha :</strong> The question is where to draw the threshold. We have seen before that <em> under the null hypothesis</em>  $\mathcal T$ has a $t_{n-1}$ distribution. Recall that this is because, if the null hypothesis is true, then  $\frac{\sqrt{n}(\bar{X}-\mu_{0})}{{\sigma}}\sim N(0,1)$, $(n-1)s^{2}/{\sigma}^{2} \sim \chi^{2}_{n-1}$ and the two are independent.  Thus, the given tests have significance level $\alpha$ for the two problems.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-177"><div class="panel panel-success remark"> <div class="panel-heading">Remark 177 </div><div class="panel-body"> Earlier we considered the problem of constructing a $(1-\alpha)$-CI for $\mu$ when ${\sigma}^{2}$ is unknown. The two sided test abovecan be simply stated as follows: Accept the alternative at level $\alpha$ if the corresponding $(1-\alpha)$-CI does not contain $\mu_{0}$. Conversely, if we had dealt with testing problems first, we could define a confidence interval as the set of all those $\mu_{0}$ for which the corresponding test rejects the alternative.
 </p>
           
 <p class="text-justify">
Thus, confidence intervals and testing are closely related. This is true in some greater generality. For example, we did not construct confidence interval for $\mu$, but you should do so and check that it is closely related to the one-sided tests above.
</div></div></div>
 </p>
<p class="text-justify">
          
 
 </p>
           

<div class="pull-right"><a href="chapter-36.html" class="btn btn-primary">Chapter 36. Testing for the difference between means of two normal populations</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script type="text/javascript" src="../js/jquery-2.1.4.min.js"></script>
 <script type="text/javascript" src='../js/bootstrap.min.js'></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    