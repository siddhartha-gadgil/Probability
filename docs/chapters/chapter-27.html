
<html>
<head>
<meta charset="utf-8">
   <meta http-equiv="X-UA-Compatible" content="IE=edge">
   <meta name="viewport" content="width=device-width, initial-scale=1">

<title> Probability Models and Stastics</title>
<link rel="icon" href="../IIScLogo.jpg">

<!-- Latest compiled and minified CSS  for Bootstrap -->
<link rel="stylesheet" href="../css/bootstrap.min.css">

<style type="text/css">
   body { padding-top: 60px; }
   .section {padding-top: 60px;}
   #arxiv {
     border-style: solid;
     border-width: 1px;
   }
</style>


  <!-- mathjax config similar to math.stackexchange -->



  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } },
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$'], ['\\[', '\\]' ]],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
  });
  </script>
  <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
</head>
<body>
    

<nav class="navbar navbar-default navbar-fixed-bottom">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <span class="navbar-brand">Probability and Statistics (notes by Manjunath Krishnapur)</span>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

        <ul class="nav navbar-nav navbar-right">
          <li><a href="index.html">Table of Contents</a></li>
          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Probabibility (part 1) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-1.html">1. What is statistics and what is probability?</a> </li>
<li><a href="chapter-2.html">2. Discrete probability spaces</a> </li>
<li><a href="chapter-3.html">3. Examples of discrete probability spaces</a> </li>
<li><a href="chapter-4.html">4. Countable and uncountable</a> </li>
<li><a href="chapter-5.html">5. On infinite sums</a> </li>
<li><a href="chapter-6.html">6. Basic rules of probability</a> </li>
<li><a href="chapter-7.html">7. Inclusion-exclusion formula</a> </li>
<li><a href="chapter-8.html">8. Bonferroni's inequalities</a> </li>
<li><a href="chapter-9.html">9. Independence - a first look</a> </li>
<li><a href="chapter-10.html">10. Conditional probability and independence</a> </li>
<li><a href="chapter-11.html">11. Independence of three or more events</a> </li>
<li><a href="chapter-12.html">12. Subtleties of conditional probability</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Probability (part 2) <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-13.html">13. Discrete probability distributions</a> </li>
<li><a href="chapter-14.html">14. General probability distributions</a> </li>
<li><a href="chapter-15.html">15. Uncountable probability spaces - conceptual difficulties</a> </li>
<li><a href="chapter-16.html">16. Examples of continuous distributions</a> </li>
<li><a href="chapter-17.html">17. Simulation</a> </li>
<li><a href="chapter-18.html">18. Joint distributions</a> </li>
<li><a href="chapter-19.html">19. Change of variable formula</a> </li>
<li><a href="chapter-20.html">20. Independence and conditioning of random variables</a> </li>
<li><a href="chapter-21.html">21. Mean and Variance</a> </li>
<li><a href="chapter-22.html">22. Makov's and Chebyshev's inequalities</a> </li>
<li><a href="chapter-23.html">23. Weak law of large numbers</a> </li>
<li><a href="chapter-24.html">24. Monte-Carlo integration</a> </li>
<li><a href="chapter-25.html">25. Central limit theorem</a> </li>
<li><a href="chapter-26.html">26. Poisson limit for rare events</a> </li>
<li><a href="chapter-27.html">27. Entropy, Gibbs distribution</a> </li>
            </ul>
          </li>

          <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> Statistics <span class="caret"></span></a>
            <ul class="dropdown-menu">
             <li><a href="chapter-28.html">28. Introduction</a> </li>
<li><a href="chapter-29.html">29. Estimation problems</a> </li>
<li><a href="chapter-30.html">30. Properties of estimates</a> </li>
<li><a href="chapter-31.html">31. Confidence intervals</a> </li>
<li><a href="chapter-32.html">32. Confidence interval for the mean</a> </li>
<li><a href="chapter-33.html">33. Actual confidence by simulation</a> </li>
<li><a href="chapter-34.html">34. Hypothesis testing - first examples</a> </li>
<li><a href="chapter-35.html">35. Testing for the mean of a normal population</a> </li>
<li><a href="chapter-36.html">36. Testing for the difference between means of two normal populations</a> </li>
<li><a href="chapter-37.html">37. Testing for the mean in absence of normality</a> </li>
<li><a href="chapter-38.html">38. Chi-squared test for goodness of fit</a> </li>
<li><a href="chapter-39.html">39. Tests for independence</a> </li>
<li><a href="chapter-40.html">40. Regression and Linear regression</a> </li>
            </ul>
          </li>
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>

  
<div class="container">
<h1 class="text-center bg-info">Chapter 27 : Entropy, Gibbs distribution</h1>
<p>&nbsp;</p>







 <p class="text-justify">

In this section we talk about entropy, a concept of fundamental importance in physics, mathematics and information theory.<sup> <a data-toggle="collapse" href="#footnote-1" aria-expanded="false" aria-controls="footnote-1">
                                1
                                 </a> </sup><span class="collapse small" id="footnote-1">This section was not covered in class and may be safely omitted</span> 
<p>&nbsp;</p>
                <div id="theorem-145"><div class="panel panel-primary definition"> <div class="panel-heading">Definition 145 </div><div class="panel-body"> Let $X$ be a random variable that takes values in ${\mathcal A}=\{a_{1},\ldots ,a_{k}\}$ such that $\mathbf{P}(X=a_{i})=p_{i}$. The entropy of $X$ is defined as
$$
H(X) := -\sum\limits_{i=1}^{k} p_{i}\log p_{i}.
$$
If $X$ is a real-valued random variable with density $f$, its entropy is defined
$$
H(X):= -\int f(t)\log f(t) dt.
$$
</div></div></div>
<p>&nbsp;</p>
                <div id="theorem-146"><div class="panel panel-info example"> <div class="panel-heading">Example 146 </div><div class="panel-body"> Let $X\sim\mbox{Ber}(p)$. Then $H(X)=p\log(1/p) +(1-p)\log(1/(1-p))$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-147"><div class="panel panel-info example"> <div class="panel-heading">Example 147 </div><div class="panel-body"> Let $X\sim \mbox{Geo}(p)$. Then $H(X)=-\sum\limits_{k=0}^{\infty}(\log p+k\log q) pq^{k} = -\log p -q^{2}\log q$.
</div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-148"><div class="panel panel-info example"> <div class="panel-heading">Example 148 </div><div class="panel-body"> Let $X\sim \mbox{Exp}(\lambda)$. Then $H(X)=\int_{0}^{\infty} (\log \lambda -t)\lambda e^{-\lambda t}dt=\log \lambda -\frac{1}{\lambda}$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-149"><div class="panel panel-info example"> <div class="panel-heading">Example 149 </div><div class="panel-body"> Let $X\sim N(\mu,{\sigma}^{2})$
</div></div></div>
 </p>
           
 <p class="text-justify">
Entropy is a measure of the randomness. For example, among the $\mbox{Ber}(p)$ distributions, the entropy is maximized at $p=1/2$ and minimized at $p=0\mbox{ or }1$. It quantifies the intuitive feeling that $\mbox{Ber}(1/2)$ is <em>more random</em> than $\mbox{Ber}(1/4)$.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-150"><div class="panel panel-primary lemma"> <div class="panel-heading">Lemma 150 </div><div class="panel-body">
<ol>
<li> If $|{\mathcal A}|=k$, then $0\le H(X)\le \log k$. $H(X)=0$ if and only if $X$ is degenerate and $H(X)=\log k$ if and only if $X\sim \mbox{Unif}({\mathcal A})$.
</li>
<li> Let $f:{\mathcal A}\rightarrow {\mathcal B}$ and let $Y=f(X)$. Then $H(Y)\le H(X)$.
</li>
<li> Let $X$ take values in ${\mathcal A}$ and $Y$ take values in ${\mathcal B}$ and let $Z=(X,Y)$. Then $H(Z)\le H(X)+H(Y)$ with equality if and only if $X$ and $Y$ are independent.
</ol>
</div></div></div>
 </p>
           
 <p class="text-justify">
<strong>Gibbs measures :</strong> Let ${\mathcal A}$ be a countable set and let ${\mathcal H}:{\mathcal A}\rightarrow \mathbb{R}$ be a given function. For any $E\in \mathbb{R}$, consider the set of ${\mathcal P}_{E}$ of all probability mass functions on $\Omega$ under which ${\mathcal H}$ has expected value $E$. In other words,
$$
{\mathcal P}_{E}:=\{{\bf p}=\left(p_{i}\right)_{i\in {\mathcal A}}{\; : \;} \sum_{i\in {\mathcal A}} p(i){\mathcal H}(i)=E\}.
$$
${\mathcal P}_{E}$ is non-empty if and only if ${\mathcal H}_{\min}\le E\le {\mathcal H}_{\max}$.
<p>&nbsp;</p>
                <div id="theorem-151"><div class="panel panel-primary lemma"> <div class="panel-heading">Lemma 151 </div><div class="panel-body"> Assume that ${\mathcal H}_{\min}\le E\le {\mathcal H}_{\max}$. Then, there is a unique pmf in ${\mathcal P}_{E}$ with maximal entropy and it is given by
$$
p_{\beta}(i)=\frac{1}{Z_{\beta} }e^{-\beta {\mathcal H}(i)}
$$
where $Z_{\beta}=\sum\limits_{i\in {\mathcal A}}e^{-\beta {\mathcal H}(i)}$ and the value of $\beta$ is chosen to satisfy
$\frac{1}{Z_{\beta} }\frac{\partial Z_{\beta} }{\partial \beta}=E$.
</div></div></div>
This minimizing pmf is called the <em>Boltzmann-Gibbs</em> distribution. An analogous theorem holds for densities.
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-152"><div class="panel panel-info example"> <div class="panel-heading">Example 152 </div><div class="panel-body"> Let ${\mathcal A}=\{1,2,\ldots,n\}$ and ${\mathcal H}(i)=1$ for all $i$. Let $E=1$ so that ${\mathcal P}_{E}$ is the same as all pmfs on ${\mathcal A}$. Clearly $p_{\beta}(i)=\frac{1}{n}$ for all $i\le n$. Indeed, we know that the maximal entropy is attained by the uniform distribution.
</div></div></div>
 </p>
           
 <p class="text-justify">
<p>&nbsp;</p>
                <div id="theorem-153"><div class="panel panel-info example"> <div class="panel-heading">Example 153 </div><div class="panel-body"> Let ${\mathcal A}=\{0,1,2,\ldots\}$ and let ${\mathcal H}(i)=i$ for all $i$. Fix any $E > 0$. The Boltzmann-Gibbs distribution is given by $p_{\beta}(i)=\frac{1}{Z_{\beta} }e^{-\beta i}$. This is just the Geometric distribution with parameter chosen to have mean $E$.
</div></div></div>
 </p>
<p class="text-justify">
          
 <p>&nbsp;</p>
                <div id="theorem-154"><div class="panel panel-info example"> <div class="panel-heading">Example 154 </div><div class="panel-body"> Let us blindly apply the lemma to densities.
<ol>
<li> ${\mathcal A}=\mathbb{R}_{+}$ and ${\mathcal H}(x)=\lambda x$
</li>
<li> ${\mathcal A}=\mathbb{R}$ and ${\mathcal H}(x)=x^{2}$.
</ol>
</div></div></div>
 </p>
           
 <p class="text-justify">
}
 </p>
<p class="text-justify">
          
 \vspace*{\fill}
\begin{center}
\Huge <strong>Statistics</strong>
\end{center}
\vspace*{\fill}
 </p>
           

<div class="pull-right"><a href="chapter-28.html" class="btn btn-primary">Chapter 28. Introduction</a></div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

</div>
<script src="../js/jquery-3.2.1.min.js"></script>
<script src="../js/popper.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<script type="text/javascript" src='../js/probability.js'></script>
<script type="text/javascript">
      Illustrations.main();
    </script>

  </body>
</html>
    